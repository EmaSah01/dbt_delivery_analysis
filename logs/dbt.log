[0m11:39:00.930236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AC6FC24980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AC706DCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AC70D17D90>]}


============================== 11:39:00.966587 | 62c1d1e8-b40c-4ef0-aca1-8a5916122bf2 ==============================
[0m11:39:00.966587 [info ] [MainThread]: Running with dbt=1.10.15
[0m11:39:00.967622 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt debug', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'logs'}
[0m11:39:00.989349 [info ] [MainThread]: dbt version: 1.10.15
[0m11:39:00.990021 [info ] [MainThread]: python version: 3.13.3
[0m11:39:00.990630 [info ] [MainThread]: python path: C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\venv\Scripts\python.exe
[0m11:39:00.991464 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m11:39:01.131578 [info ] [MainThread]: Using profiles dir at C:\Users\Korisnik\.dbt
[0m11:39:01.132669 [info ] [MainThread]: Using profiles.yml file at C:\Users\Korisnik\.dbt\profiles.yml
[0m11:39:01.133224 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\dbt_project.yml
[0m11:39:01.134550 [info ] [MainThread]: Configuration:
[0m11:39:01.135123 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m11:39:01.135651 [info ] [MainThread]:   dbt_project.yml file [[31mERROR invalid[0m]
[0m11:39:01.136107 [info ] [MainThread]: Required dependencies:
[0m11:39:01.136628 [debug] [MainThread]: Executing "git --help"
[0m11:39:03.703129 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:39:03.703774 [debug] [MainThread]: STDERR: "b''"
[0m11:39:03.704220 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:39:03.704788 [info ] [MainThread]: Connection test skipped since no profile was found
[0m11:39:03.705410 [info ] [MainThread]: [31m2 checks failed:[0m
[0m11:39:03.705908 [info ] [MainThread]: Could not load dbt_project.yml
Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "dbt_delivery_analytics", target "dev" invalid: 1234 is not of type 'string'


[0m11:39:03.706633 [info ] [MainThread]: Project loading failed for the following reason:
Runtime Error
  dbt_project.yml does not parse to a dictionary


[0m11:39:03.707974 [debug] [MainThread]: Command `dbt debug` failed at 11:39:03.707831 after 2.98 seconds
[0m11:39:03.708388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AC7231CE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AC722F7410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AC72222E00>]}
[0m11:39:03.708788 [debug] [MainThread]: Flushing usage events
[0m11:39:04.688852 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:40:45.613509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001964E8C4980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001964F36CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001964F9A7D90>]}


============================== 11:40:45.617854 | 6792b6b5-c5a5-4799-9309-15e2b3bbb402 ==============================
[0m11:40:45.617854 [info ] [MainThread]: Running with dbt=1.10.15
[0m11:40:45.618602 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m11:40:45.638747 [info ] [MainThread]: dbt version: 1.10.15
[0m11:40:45.639445 [info ] [MainThread]: python version: 3.13.3
[0m11:40:45.639916 [info ] [MainThread]: python path: C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\venv\Scripts\python.exe
[0m11:40:45.640406 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m11:40:45.728115 [info ] [MainThread]: Using profiles dir at C:\Users\Korisnik\.dbt
[0m11:40:45.728802 [info ] [MainThread]: Using profiles.yml file at C:\Users\Korisnik\.dbt\profiles.yml
[0m11:40:45.729522 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\dbt_project.yml
[0m11:40:45.730907 [info ] [MainThread]: adapter type: postgres
[0m11:40:45.731431 [info ] [MainThread]: adapter version: 1.9.1
[0m11:40:45.839369 [info ] [MainThread]: Configuration:
[0m11:40:45.840080 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:40:45.840628 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:40:45.841177 [info ] [MainThread]: Required dependencies:
[0m11:40:45.841691 [debug] [MainThread]: Executing "git --help"
[0m11:40:46.110288 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:40:46.110956 [debug] [MainThread]: STDERR: "b''"
[0m11:40:46.111375 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:40:46.111861 [info ] [MainThread]: Connection:
[0m11:40:46.112413 [info ] [MainThread]:   host: localhost
[0m11:40:46.112986 [info ] [MainThread]:   port: 5433
[0m11:40:46.113392 [info ] [MainThread]:   user: postgres
[0m11:40:46.113874 [info ] [MainThread]:   database: delivery_analytics
[0m11:40:46.114323 [info ] [MainThread]:   schema: analytics
[0m11:40:46.114746 [info ] [MainThread]:   connect_timeout: 10
[0m11:40:46.115297 [info ] [MainThread]:   role: None
[0m11:40:46.115786 [info ] [MainThread]:   search_path: None
[0m11:40:46.116251 [info ] [MainThread]:   keepalives_idle: 0
[0m11:40:46.116754 [info ] [MainThread]:   sslmode: None
[0m11:40:46.117246 [info ] [MainThread]:   sslcert: None
[0m11:40:46.117813 [info ] [MainThread]:   sslkey: None
[0m11:40:46.118360 [info ] [MainThread]:   sslrootcert: None
[0m11:40:46.118858 [info ] [MainThread]:   application_name: dbt
[0m11:40:46.119332 [info ] [MainThread]:   retries: 1
[0m11:40:46.120321 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m11:40:46.474860 [debug] [MainThread]: Acquiring new postgres connection 'debug'
[0m11:40:46.593480 [debug] [MainThread]: Using postgres connection "debug"
[0m11:40:46.593925 [debug] [MainThread]: On debug: select 1 as id
[0m11:40:46.594209 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:40:46.740773 [debug] [MainThread]: SQL status: SELECT 1 in 0.146 seconds
[0m11:40:46.741971 [debug] [MainThread]: On debug: Close
[0m11:40:46.742429 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:40:46.743087 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:40:46.744658 [debug] [MainThread]: Command `dbt debug` succeeded at 11:40:46.744510 after 1.37 seconds
[0m11:40:46.745099 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:40:46.745892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019650FC2B10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000196510996D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001964FAA7CE0>]}
[0m11:40:46.746531 [debug] [MainThread]: Flushing usage events
[0m11:40:47.689720 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:45:57.529102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002260FD34980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226107ECB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022610E27D90>]}


============================== 11:45:57.535392 | 304ef73a-6129-45b7-9ed8-d675138210ca ==============================
[0m11:45:57.535392 [info ] [MainThread]: Running with dbt=1.10.15
[0m11:45:57.536426 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run --select staging', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m11:45:57.887994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '304ef73a-6129-45b7-9ed8-d675138210ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022610198770>]}
[0m11:45:57.983603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '304ef73a-6129-45b7-9ed8-d675138210ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022610CCEE00>]}
[0m11:45:57.984754 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m11:45:58.500227 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m11:45:58.501608 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m11:45:58.502232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '304ef73a-6129-45b7-9ed8-d675138210ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022610F39850>]}
[0m11:46:00.223659 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.std_shipment_status' (models\staging\std_shipment_status.sql) depends on a source named 'raw.shipment_status' which was not found
[0m11:46:00.225936 [debug] [MainThread]: Command `dbt run` failed at 11:46:00.225764 after 2.91 seconds
[0m11:46:00.226577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226126597C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022612659310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022610E2FE70>]}
[0m11:46:00.227160 [debug] [MainThread]: Flushing usage events
[0m11:46:01.344658 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:46:07.344705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3B4940980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3B53ECB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3B5A37D90>]}


============================== 11:46:07.349543 | 456eaf34-f993-4609-91fe-da358f7a28a0 ==============================
[0m11:46:07.349543 [info ] [MainThread]: Running with dbt=1.10.15
[0m11:46:07.350464 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select marts.core', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m11:46:07.610603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '456eaf34-f993-4609-91fe-da358f7a28a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3B4DA8770>]}
[0m11:46:07.687838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '456eaf34-f993-4609-91fe-da358f7a28a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3B58DEE00>]}
[0m11:46:07.689286 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m11:46:08.119298 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m11:46:08.121016 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m11:46:08.121747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '456eaf34-f993-4609-91fe-da358f7a28a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3B6E69850>]}
[0m11:46:09.406988 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.std_shipment_status' (models\staging\std_shipment_status.sql) depends on a source named 'raw.shipment_status' which was not found
[0m11:46:09.408722 [debug] [MainThread]: Command `dbt run` failed at 11:46:09.408554 after 2.25 seconds
[0m11:46:09.409208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3B72397C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3B7239310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3B5A3FE70>]}
[0m11:46:09.409665 [debug] [MainThread]: Flushing usage events
[0m11:46:10.347604 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:49:25.797287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BD974A0980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BD97F4CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BD98597D90>]}


============================== 11:49:25.803045 | f8ab7a31-48fa-4ed9-9ecd-a305ec0996cc ==============================
[0m11:49:25.803045 [info ] [MainThread]: Running with dbt=1.10.15
[0m11:49:25.804440 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run --select staging', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m11:49:26.083893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f8ab7a31-48fa-4ed9-9ecd-a305ec0996cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BD97908770>]}
[0m11:49:26.167721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f8ab7a31-48fa-4ed9-9ecd-a305ec0996cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BD9843EE00>]}
[0m11:49:26.169330 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m11:49:26.619007 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m11:49:26.620390 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m11:49:26.621130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'f8ab7a31-48fa-4ed9-9ecd-a305ec0996cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BD99A49850>]}
[0m11:49:27.946544 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.std_shipment_status' (models\staging\std_shipment_status.sql) depends on a source named 'raw.shipment_status' which was not found
[0m11:49:27.948888 [debug] [MainThread]: Command `dbt run` failed at 11:49:27.948681 after 2.47 seconds
[0m11:49:27.949408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BD99E197C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BD99E19310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BD9859FE70>]}
[0m11:49:27.949979 [debug] [MainThread]: Flushing usage events
[0m11:49:29.053301 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:51:34.783279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADAF00980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADB9ACB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADBFEBD90>]}


============================== 11:51:34.788081 | 8e67c9ca-5b38-4a02-9e9c-d922793088d9 ==============================
[0m11:51:34.788081 [info ] [MainThread]: Running with dbt=1.10.15
[0m11:51:34.788980 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select staging', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m11:51:35.045404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8e67c9ca-5b38-4a02-9e9c-d922793088d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADB368770>]}
[0m11:51:35.117685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8e67c9ca-5b38-4a02-9e9c-d922793088d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADBE8EE00>]}
[0m11:51:35.119027 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m11:51:35.515165 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m11:51:35.517142 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m11:51:35.518348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8e67c9ca-5b38-4a02-9e9c-d922793088d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADD0C9850>]}
[0m11:51:36.653875 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.std_shipment_status' (models\staging\std_shipment_status.sql) depends on a source named 'raw.shipment_status' which was not found
[0m11:51:36.655459 [debug] [MainThread]: Command `dbt run` failed at 11:51:36.655315 after 2.13 seconds
[0m11:51:36.655868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADD8497C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADD849310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADBFEFE70>]}
[0m11:51:36.656269 [debug] [MainThread]: Flushing usage events
[0m11:51:37.699693 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:25:13.945713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002656AE80980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002656B92CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002656BF7BD90>]}


============================== 12:25:13.953709 | e27b91c9-2c9c-45d2-b30c-7c9cd64b1605 ==============================
[0m12:25:13.953709 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:25:13.955705 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select marts.core', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:25:14.319441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e27b91c9-2c9c-45d2-b30c-7c9cd64b1605', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002656B2E8770>]}
[0m12:25:14.404468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e27b91c9-2c9c-45d2-b30c-7c9cd64b1605', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002656BE1EE00>]}
[0m12:25:14.406300 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:25:14.944330 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:25:14.945724 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:25:14.946595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e27b91c9-2c9c-45d2-b30c-7c9cd64b1605', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002656D429850>]}
[0m12:25:16.241544 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.std_shipment_status' (models\staging\std_shipment_status.sql) depends on a source named 'raw.shipment_status' which was not found
[0m12:25:16.243595 [debug] [MainThread]: Command `dbt run` failed at 12:25:16.243399 after 2.62 seconds
[0m12:25:16.244133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002656D7F97C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002656D7F9310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002656BF7FE70>]}
[0m12:25:16.244609 [debug] [MainThread]: Flushing usage events
[0m12:25:17.368044 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:26:32.430866 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE3B2A4980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE3BD4CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE3C397D90>]}


============================== 12:26:32.438699 | 39d1507c-397f-4c93-b6a3-7efef8d16296 ==============================
[0m12:26:32.438699 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:26:32.440034 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt debug', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:26:32.478658 [info ] [MainThread]: dbt version: 1.10.15
[0m12:26:32.479901 [info ] [MainThread]: python version: 3.13.3
[0m12:26:32.480938 [info ] [MainThread]: python path: C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\venv\Scripts\python.exe
[0m12:26:32.481822 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m12:26:32.638699 [info ] [MainThread]: Using profiles dir at C:\Users\Korisnik\.dbt
[0m12:26:32.639633 [info ] [MainThread]: Using profiles.yml file at C:\Users\Korisnik\.dbt\profiles.yml
[0m12:26:32.640373 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\dbt_project.yml
[0m12:26:32.641965 [info ] [MainThread]: adapter type: postgres
[0m12:26:32.642520 [info ] [MainThread]: adapter version: 1.9.1
[0m12:26:32.843454 [info ] [MainThread]: Configuration:
[0m12:26:32.844470 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:26:32.845177 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:26:32.845858 [info ] [MainThread]: Required dependencies:
[0m12:26:32.846531 [debug] [MainThread]: Executing "git --help"
[0m12:26:33.525081 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:26:33.525803 [debug] [MainThread]: STDERR: "b''"
[0m12:26:33.526321 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:26:33.527039 [info ] [MainThread]: Connection:
[0m12:26:33.527826 [info ] [MainThread]:   host: localhost
[0m12:26:33.528500 [info ] [MainThread]:   port: 5433
[0m12:26:33.529141 [info ] [MainThread]:   user: postgres
[0m12:26:33.529813 [info ] [MainThread]:   database: delivery_analytics
[0m12:26:33.530485 [info ] [MainThread]:   schema: analytics
[0m12:26:33.531163 [info ] [MainThread]:   connect_timeout: 10
[0m12:26:33.532000 [info ] [MainThread]:   role: None
[0m12:26:33.532879 [info ] [MainThread]:   search_path: None
[0m12:26:33.533646 [info ] [MainThread]:   keepalives_idle: 0
[0m12:26:33.534247 [info ] [MainThread]:   sslmode: None
[0m12:26:33.534993 [info ] [MainThread]:   sslcert: None
[0m12:26:33.535836 [info ] [MainThread]:   sslkey: None
[0m12:26:33.536561 [info ] [MainThread]:   sslrootcert: None
[0m12:26:33.537406 [info ] [MainThread]:   application_name: dbt
[0m12:26:33.538007 [info ] [MainThread]:   retries: 1
[0m12:26:33.539275 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:26:34.131814 [debug] [MainThread]: Acquiring new postgres connection 'debug'
[0m12:26:34.299492 [debug] [MainThread]: Using postgres connection "debug"
[0m12:26:34.300085 [debug] [MainThread]: On debug: select 1 as id
[0m12:26:34.300453 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:26:34.463907 [debug] [MainThread]: SQL status: SELECT 1 in 0.163 seconds
[0m12:26:34.465223 [debug] [MainThread]: On debug: Close
[0m12:26:34.465919 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m12:26:34.466881 [info ] [MainThread]: [32mAll checks passed![0m
[0m12:26:34.468975 [debug] [MainThread]: Command `dbt debug` succeeded at 12:26:34.468786 after 2.26 seconds
[0m12:26:34.469692 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:26:34.470253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE3D9B2B10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE3DA896D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE3C4B7CE0>]}
[0m12:26:34.470909 [debug] [MainThread]: Flushing usage events
[0m12:26:35.630461 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:26:41.714321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194F7830980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194F82DCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194F8927D90>]}


============================== 12:26:41.719601 | 206981eb-d5a2-470f-9ca0-997803c9a5af ==============================
[0m12:26:41.719601 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:26:41.720723 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt source freshness', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:26:41.972655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '206981eb-d5a2-470f-9ca0-997803c9a5af', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194F7C98770>]}
[0m12:26:42.034752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '206981eb-d5a2-470f-9ca0-997803c9a5af', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194F87CEE00>]}
[0m12:26:42.036326 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:26:42.466553 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:26:42.467999 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:26:42.468762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '206981eb-d5a2-470f-9ca0-997803c9a5af', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194F9D79850>]}
[0m12:26:43.491770 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.std_shipment_status' (models\staging\std_shipment_status.sql) depends on a source named 'raw.shipment_status' which was not found
[0m12:26:43.493751 [debug] [MainThread]: Command `dbt source freshness` failed at 12:26:43.493526 after 1.95 seconds
[0m12:26:43.494553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194FA14E7B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194FA14C320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194F892CC90>]}
[0m12:26:43.495177 [debug] [MainThread]: Flushing usage events
[0m12:26:44.485942 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:28:40.348471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001808CC18980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001808D6BCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001808DD07D90>]}


============================== 12:28:40.354458 | f7484a1b-bd5e-4e88-9730-d75ccb13b435 ==============================
[0m12:28:40.354458 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:28:40.355792 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'use_colors': 'True', 'invocation_command': 'dbt clean', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:28:40.553399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f7484a1b-bd5e-4e88-9730-d75ccb13b435', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001808D078770>]}
[0m12:28:40.593118 [debug] [MainThread]: Command `dbt clean` succeeded at 12:28:40.592927 after 0.55 seconds
[0m12:28:40.593799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001808DBAFCE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001808DBAF680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001808F24E150>]}
[0m12:28:40.594448 [debug] [MainThread]: Flushing usage events
[0m12:28:41.781598 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:28:46.631466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E882474980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E882F2CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E88356BD90>]}


============================== 12:28:46.638381 | ac79a14b-6f4c-4e4b-9c3d-479766383284 ==============================
[0m12:28:46.638381 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:28:46.639401 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt deps', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:28:46.839355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ac79a14b-6f4c-4e4b-9c3d-479766383284', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8828D8770>]}
[0m12:28:46.878718 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m12:28:46.880622 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m12:28:46.882388 [debug] [MainThread]: Command `dbt deps` succeeded at 12:28:46.882239 after 0.44 seconds
[0m12:28:46.882783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E88340FF00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E88340F130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8833EBA50>]}
[0m12:28:46.883231 [debug] [MainThread]: Flushing usage events
[0m12:28:47.953216 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:28:53.811007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7394C4980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A739F6CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A73A5A7D90>]}


============================== 12:28:53.816491 | 66bf7b68-0b76-4869-ba0d-b62d167617bf ==============================
[0m12:28:53.816491 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:28:53.818059 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt debug', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:28:53.847771 [info ] [MainThread]: dbt version: 1.10.15
[0m12:28:53.848690 [info ] [MainThread]: python version: 3.13.3
[0m12:28:53.849715 [info ] [MainThread]: python path: C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\venv\Scripts\python.exe
[0m12:28:53.850531 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m12:28:53.964537 [info ] [MainThread]: Using profiles dir at C:\Users\Korisnik\.dbt
[0m12:28:53.965436 [info ] [MainThread]: Using profiles.yml file at C:\Users\Korisnik\.dbt\profiles.yml
[0m12:28:53.966010 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\dbt_project.yml
[0m12:28:53.967664 [info ] [MainThread]: adapter type: postgres
[0m12:28:53.968164 [info ] [MainThread]: adapter version: 1.9.1
[0m12:28:54.093236 [info ] [MainThread]: Configuration:
[0m12:28:54.094398 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:28:54.094956 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:28:54.095560 [info ] [MainThread]: Required dependencies:
[0m12:28:54.096116 [debug] [MainThread]: Executing "git --help"
[0m12:28:54.365707 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:28:54.366230 [debug] [MainThread]: STDERR: "b''"
[0m12:28:54.366633 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:28:54.367156 [info ] [MainThread]: Connection:
[0m12:28:54.367711 [info ] [MainThread]:   host: localhost
[0m12:28:54.368142 [info ] [MainThread]:   port: 5433
[0m12:28:54.368645 [info ] [MainThread]:   user: postgres
[0m12:28:54.369172 [info ] [MainThread]:   database: delivery_analytics
[0m12:28:54.369774 [info ] [MainThread]:   schema: analytics
[0m12:28:54.370271 [info ] [MainThread]:   connect_timeout: 10
[0m12:28:54.370782 [info ] [MainThread]:   role: None
[0m12:28:54.371384 [info ] [MainThread]:   search_path: None
[0m12:28:54.371931 [info ] [MainThread]:   keepalives_idle: 0
[0m12:28:54.372447 [info ] [MainThread]:   sslmode: None
[0m12:28:54.372983 [info ] [MainThread]:   sslcert: None
[0m12:28:54.373493 [info ] [MainThread]:   sslkey: None
[0m12:28:54.374037 [info ] [MainThread]:   sslrootcert: None
[0m12:28:54.374492 [info ] [MainThread]:   application_name: dbt
[0m12:28:54.374898 [info ] [MainThread]:   retries: 1
[0m12:28:54.375649 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:28:54.983980 [debug] [MainThread]: Acquiring new postgres connection 'debug'
[0m12:28:55.145396 [debug] [MainThread]: Using postgres connection "debug"
[0m12:28:55.145885 [debug] [MainThread]: On debug: select 1 as id
[0m12:28:55.146425 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:28:55.266956 [debug] [MainThread]: SQL status: SELECT 1 in 0.120 seconds
[0m12:28:55.268598 [debug] [MainThread]: On debug: Close
[0m12:28:55.269207 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m12:28:55.269968 [info ] [MainThread]: [32mAll checks passed![0m
[0m12:28:55.271345 [debug] [MainThread]: Command `dbt debug` succeeded at 12:28:55.271219 after 1.66 seconds
[0m12:28:55.271710 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:28:55.272221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A73BBC2B10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A73BC996D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A73B697CE0>]}
[0m12:28:55.272669 [debug] [MainThread]: Flushing usage events
[0m12:28:56.330253 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:29:01.173226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212A4014980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212A4ACCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212A5107D90>]}


============================== 12:29:01.179714 | 510d9f84-dd19-4e84-91b1-7fb59660ef8e ==============================
[0m12:29:01.179714 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:29:01.180889 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'indirect_selection': 'eager', 'invocation_command': 'dbt source freshness', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:29:01.507506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '510d9f84-dd19-4e84-91b1-7fb59660ef8e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212A4478770>]}
[0m12:29:01.596227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '510d9f84-dd19-4e84-91b1-7fb59660ef8e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212A4FAEE00>]}
[0m12:29:01.598300 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:29:02.119266 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:29:02.120522 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:29:02.121130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '510d9f84-dd19-4e84-91b1-7fb59660ef8e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212A61E9850>]}
[0m12:29:03.614967 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.std_shipment_status' (models\staging\std_shipment_status.sql) depends on a source named 'raw.shipment_status' which was not found
[0m12:29:03.616898 [debug] [MainThread]: Command `dbt source freshness` failed at 12:29:03.616730 after 2.63 seconds
[0m12:29:03.617573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212A694E7B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212A694C320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212A510CC90>]}
[0m12:29:03.618169 [debug] [MainThread]: Flushing usage events
[0m12:29:04.698742 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:29:43.436907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7CDBA4980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7CE65CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7CEC97D90>]}


============================== 12:29:43.443248 | c3b38175-532b-427a-b78a-048a8ca27261 ==============================
[0m12:29:43.443248 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:29:43.444218 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'use_colors': 'True', 'invocation_command': 'dbt source freshness', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:29:43.808394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c3b38175-532b-427a-b78a-048a8ca27261', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7CE008770>]}
[0m12:29:43.877786 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c3b38175-532b-427a-b78a-048a8ca27261', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7CEB3EE00>]}
[0m12:29:43.879580 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:29:44.509449 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:29:44.511124 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:29:44.511827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'c3b38175-532b-427a-b78a-048a8ca27261', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7CFD79850>]}
[0m12:29:45.904118 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.stg_couriers' (models\staging\stg_couriers.sql) depends on a source named 'raw.couriers' which was not found
[0m12:29:45.906140 [debug] [MainThread]: Command `dbt source freshness` failed at 12:29:45.905995 after 2.69 seconds
[0m12:29:45.906530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7D04EE7B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7D04EC320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7CEC9CC90>]}
[0m12:29:45.906914 [debug] [MainThread]: Flushing usage events
[0m12:29:47.057553 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:32:17.211920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191FCE14980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191FD8CCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191FDF07D90>]}


============================== 12:32:17.218235 | 04ab8250-ad3a-46c5-be2f-a1ec28fcb0b8 ==============================
[0m12:32:17.218235 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:32:17.219384 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt source freshness', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:32:17.578961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '04ab8250-ad3a-46c5-be2f-a1ec28fcb0b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191FD278770>]}
[0m12:32:17.684133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '04ab8250-ad3a-46c5-be2f-a1ec28fcb0b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191FDDAEE00>]}
[0m12:32:17.686041 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:32:18.498963 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:32:18.501007 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:32:18.501905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '04ab8250-ad3a-46c5-be2f-a1ec28fcb0b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191FF339850>]}
[0m12:32:19.815616 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.stg_couriers' (models\staging\stg_couriers.sql) depends on a source named 'raw.couriers' which was not found
[0m12:32:19.817689 [debug] [MainThread]: Command `dbt source freshness` failed at 12:32:19.817537 after 2.93 seconds
[0m12:32:19.818140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191FF70E7B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191FF70C320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191FDF0CC90>]}
[0m12:32:19.818705 [debug] [MainThread]: Flushing usage events
[0m12:32:20.802630 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:32:43.392183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CAFEC4980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CB096CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CB0FBBD90>]}


============================== 12:32:43.398902 | 3e4cdd9a-a56b-4ec4-bab7-7eed0616dd35 ==============================
[0m12:32:43.398902 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:32:43.400377 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select staging', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:32:43.692066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3e4cdd9a-a56b-4ec4-bab7-7eed0616dd35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CB0328770>]}
[0m12:32:43.776225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3e4cdd9a-a56b-4ec4-bab7-7eed0616dd35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CB0E5EE00>]}
[0m12:32:43.778111 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:32:44.312393 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:32:44.313928 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:32:44.314716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3e4cdd9a-a56b-4ec4-bab7-7eed0616dd35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CB10C9850>]}
[0m12:32:45.619926 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.stg_couriers' (models\staging\stg_couriers.sql) depends on a source named 'raw.couriers' which was not found
[0m12:32:45.621912 [debug] [MainThread]: Command `dbt run` failed at 12:32:45.621744 after 2.44 seconds
[0m12:32:45.622519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CB28197C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CB2819310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014CB0FBFE70>]}
[0m12:32:45.623017 [debug] [MainThread]: Flushing usage events
[0m12:32:46.676640 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:33:47.779163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A775980980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A77643CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A776A77D90>]}


============================== 12:33:47.785470 | bbc386af-26db-4dc5-ba5c-870671bda6f2 ==============================
[0m12:33:47.785470 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:33:47.787000 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select staging', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:33:48.109542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bbc386af-26db-4dc5-ba5c-870671bda6f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A775DE8770>]}
[0m12:33:48.202185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bbc386af-26db-4dc5-ba5c-870671bda6f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A77691EE00>]}
[0m12:33:48.215566 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:33:48.746932 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:33:48.748429 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:33:48.749206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'bbc386af-26db-4dc5-ba5c-870671bda6f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A777EA9850>]}
[0m12:33:49.953791 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.stg_couriers' (models\staging\stg_couriers.sql) depends on a source named 'raw.couriers' which was not found
[0m12:33:49.956007 [debug] [MainThread]: Command `dbt run` failed at 12:33:49.955811 after 2.42 seconds
[0m12:33:49.956552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A7782797C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A778279310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A776A7FE70>]}
[0m12:33:49.957027 [debug] [MainThread]: Flushing usage events
[0m12:33:51.023877 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:34:47.825257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D33FC4980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D34A7CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D350B7D90>]}


============================== 12:34:47.831265 | 06ae78a2-7e82-47f5-b076-4414b39d6fda ==============================
[0m12:34:47.831265 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:34:47.832571 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt run --select staging', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:34:48.153118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '06ae78a2-7e82-47f5-b076-4414b39d6fda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D34428770>]}
[0m12:34:48.244423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '06ae78a2-7e82-47f5-b076-4414b39d6fda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D34F5EE00>]}
[0m12:34:48.246539 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:34:48.788128 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:34:48.790174 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:34:48.790863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '06ae78a2-7e82-47f5-b076-4414b39d6fda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D364E9850>]}
[0m12:34:50.110402 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.stg_couriers' (models\staging\stg_couriers.sql) depends on a source named 'raw.couriers' which was not found
[0m12:34:50.112386 [debug] [MainThread]: Command `dbt run` failed at 12:34:50.112142 after 2.51 seconds
[0m12:34:50.112984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D368B97C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D368B9310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D350BFE70>]}
[0m12:34:50.113478 [debug] [MainThread]: Flushing usage events
[0m12:34:51.235923 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:41:15.057554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D30BF4980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D3169CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D31CE7D90>]}


============================== 12:41:15.063480 | 6a9fc7d7-8416-4b99-9924-fe9ee1e7a41c ==============================
[0m12:41:15.063480 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:41:15.064657 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'use_colors': 'True', 'invocation_command': 'dbt debug', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:41:15.104106 [info ] [MainThread]: dbt version: 1.10.15
[0m12:41:15.105184 [info ] [MainThread]: python version: 3.13.3
[0m12:41:15.105990 [info ] [MainThread]: python path: C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\venv\Scripts\python.exe
[0m12:41:15.106800 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m12:41:15.212164 [info ] [MainThread]: Using profiles dir at C:\Users\Korisnik\.dbt
[0m12:41:15.213274 [info ] [MainThread]: Using profiles.yml file at C:\Users\Korisnik\.dbt\profiles.yml
[0m12:41:15.214266 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\dbt_project.yml
[0m12:41:15.216017 [info ] [MainThread]: adapter type: postgres
[0m12:41:15.216840 [info ] [MainThread]: adapter version: 1.9.1
[0m12:41:15.360794 [info ] [MainThread]: Configuration:
[0m12:41:15.362047 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m12:41:15.362997 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m12:41:15.363828 [info ] [MainThread]: Required dependencies:
[0m12:41:15.364814 [debug] [MainThread]: Executing "git --help"
[0m12:41:15.641912 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m12:41:15.642926 [debug] [MainThread]: STDERR: "b''"
[0m12:41:15.643485 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m12:41:15.644309 [info ] [MainThread]: Connection:
[0m12:41:15.645184 [info ] [MainThread]:   host: localhost
[0m12:41:15.645957 [info ] [MainThread]:   port: 5433
[0m12:41:15.646725 [info ] [MainThread]:   user: postgres
[0m12:41:15.647547 [info ] [MainThread]:   database: delivery_analytics
[0m12:41:15.648341 [info ] [MainThread]:   schema: analytics
[0m12:41:15.649182 [info ] [MainThread]:   connect_timeout: 10
[0m12:41:15.649889 [info ] [MainThread]:   role: None
[0m12:41:15.650601 [info ] [MainThread]:   search_path: None
[0m12:41:15.651382 [info ] [MainThread]:   keepalives_idle: 0
[0m12:41:15.652059 [info ] [MainThread]:   sslmode: None
[0m12:41:15.652669 [info ] [MainThread]:   sslcert: None
[0m12:41:15.653355 [info ] [MainThread]:   sslkey: None
[0m12:41:15.653993 [info ] [MainThread]:   sslrootcert: None
[0m12:41:15.654650 [info ] [MainThread]:   application_name: dbt
[0m12:41:15.655305 [info ] [MainThread]:   retries: 1
[0m12:41:15.656534 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:41:16.212599 [debug] [MainThread]: Acquiring new postgres connection 'debug'
[0m12:41:16.383368 [debug] [MainThread]: Using postgres connection "debug"
[0m12:41:16.383874 [debug] [MainThread]: On debug: select 1 as id
[0m12:41:16.384257 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:41:16.536812 [debug] [MainThread]: SQL status: SELECT 1 in 0.152 seconds
[0m12:41:16.538618 [debug] [MainThread]: On debug: Close
[0m12:41:16.539137 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m12:41:16.539973 [info ] [MainThread]: [32mAll checks passed![0m
[0m12:41:16.542419 [debug] [MainThread]: Command `dbt debug` succeeded at 12:41:16.542263 after 1.80 seconds
[0m12:41:16.542847 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m12:41:16.543401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D33362B10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D3343D6D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D331C7CE0>]}
[0m12:41:16.544138 [debug] [MainThread]: Flushing usage events
[0m12:41:17.632073 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:41:23.461518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CCBD94980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CCC82CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CCCE77D90>]}


============================== 12:41:23.466407 | d59166be-e54d-4ee4-b87e-a055730a2fea ==============================
[0m12:41:23.466407 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:41:23.467302 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:41:23.700430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd59166be-e54d-4ee4-b87e-a055730a2fea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CCC1F8770>]}
[0m12:41:23.761943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd59166be-e54d-4ee4-b87e-a055730a2fea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CCCD1EE00>]}
[0m12:41:23.763525 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:41:24.170871 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:41:24.172146 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:41:24.172901 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd59166be-e54d-4ee4-b87e-a055730a2fea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CCE329850>]}
[0m12:41:25.185511 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.stg_couriers' (models\staging\stg_couriers.sql) depends on a source named 'raw.couriers' which was not found
[0m12:41:25.187357 [debug] [MainThread]: Command `dbt run` failed at 12:41:25.187217 after 1.88 seconds
[0m12:41:25.187875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CCE6F97C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CCE6F9310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CCCE7FE70>]}
[0m12:41:25.188377 [debug] [MainThread]: Flushing usage events
[0m12:41:26.177168 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:48:45.989927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E264880980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26532CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E265977D90>]}


============================== 13:48:45.996954 | eab4d17c-c281-4435-9620-9b2f3506958a ==============================
[0m13:48:45.996954 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:48:45.998505 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'use_colors': 'True', 'invocation_command': 'dbt run --select staging', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:48:46.334626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'eab4d17c-c281-4435-9620-9b2f3506958a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E264CE8770>]}
[0m13:48:46.427844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'eab4d17c-c281-4435-9620-9b2f3506958a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26581EE00>]}
[0m13:48:46.429863 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:48:46.918174 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:48:46.919439 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:48:46.920193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'eab4d17c-c281-4435-9620-9b2f3506958a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E266DA9850>]}
[0m13:48:47.974368 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.stg_couriers' (models\staging\stg_couriers.sql) depends on a source named 'raw.couriers' which was not found
[0m13:48:47.976137 [debug] [MainThread]: Command `dbt run` failed at 13:48:47.975995 after 2.34 seconds
[0m13:48:47.976755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E2671797C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E267179310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26597FE70>]}
[0m13:48:47.977263 [debug] [MainThread]: Flushing usage events
[0m13:48:49.115236 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:49:54.810474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE88AC4980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE8957CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE89BB7D90>]}


============================== 13:49:54.816816 | 82b47af0-44e4-4b74-9b8a-34ebcea07274 ==============================
[0m13:49:54.816816 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:49:54.818188 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt run --select staging', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:49:55.112730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '82b47af0-44e4-4b74-9b8a-34ebcea07274', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE88F28770>]}
[0m13:49:55.195988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '82b47af0-44e4-4b74-9b8a-34ebcea07274', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE89A5EE00>]}
[0m13:49:55.197738 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:49:55.716590 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:49:55.717940 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:49:55.718554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '82b47af0-44e4-4b74-9b8a-34ebcea07274', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE8B0C9850>]}
[0m13:49:56.720365 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.stg_couriers' (models\staging\stg_couriers.sql) depends on a source named 'raw.couriers' which was not found
[0m13:49:56.722062 [debug] [MainThread]: Command `dbt run` failed at 13:49:56.721897 after 2.13 seconds
[0m13:49:56.722526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE8B4997C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE8B499310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE89BBFE70>]}
[0m13:49:56.722981 [debug] [MainThread]: Flushing usage events
[0m13:49:57.676580 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:51:33.046019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFF1894980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFF233CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFF2987D90>]}


============================== 13:51:33.055917 | f56b7041-1bf1-4990-bcee-a42b428e736c ==============================
[0m13:51:33.055917 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:51:33.057332 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt source snapshot-freshness', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:51:33.361417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f56b7041-1bf1-4990-bcee-a42b428e736c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFF1CF8770>]}
[0m13:51:33.453758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f56b7041-1bf1-4990-bcee-a42b428e736c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFF282EE00>]}
[0m13:51:33.455809 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:51:34.018295 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:51:34.020003 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:51:34.020737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'f56b7041-1bf1-4990-bcee-a42b428e736c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFF3DE9850>]}
[0m13:51:35.339146 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.stg_couriers' (models\staging\stg_couriers.sql) depends on a source named 'raw.couriers' which was not found
[0m13:51:35.341299 [debug] [MainThread]: Command `dbt source snapshot-freshness` failed at 13:51:35.341119 after 2.64 seconds
[0m13:51:35.341813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFF41C27B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFF41C0320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFF298CC90>]}
[0m13:51:35.342289 [debug] [MainThread]: Flushing usage events
[0m13:51:36.452829 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:52:36.733388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A89B884980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A89C32CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A89C977D90>]}


============================== 13:52:36.739291 | 13bacde7-4be0-4c46-8239-2cef1b15bbf2 ==============================
[0m13:52:36.739291 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:52:36.740677 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt source snapshot-freshness', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:52:37.072612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '13bacde7-4be0-4c46-8239-2cef1b15bbf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A89BCE8770>]}
[0m13:52:37.165681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '13bacde7-4be0-4c46-8239-2cef1b15bbf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A89C81EE00>]}
[0m13:52:37.169662 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:52:37.860194 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:52:37.862299 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:52:37.863557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '13bacde7-4be0-4c46-8239-2cef1b15bbf2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A89CE89850>]}
[0m13:52:39.121764 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.stg_couriers' (models\staging\stg_couriers.sql) depends on a source named 'raw.couriers' which was not found
[0m13:52:39.124241 [debug] [MainThread]: Command `dbt source snapshot-freshness` failed at 13:52:39.124035 after 2.64 seconds
[0m13:52:39.124659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A89E22E7B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A89E22C320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A89C97CC90>]}
[0m13:52:39.125097 [debug] [MainThread]: Flushing usage events
[0m13:52:40.117248 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:54:16.446804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231CFC70980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231D072CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231D0D6BD90>]}


============================== 13:54:16.453166 | a52d5d50-ef11-43c7-845d-4cc643cbd88b ==============================
[0m13:54:16.453166 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:54:16.454622 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt ls --resource-type source', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:54:16.777828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a52d5d50-ef11-43c7-845d-4cc643cbd88b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231D00D8770>]}
[0m13:54:16.860476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a52d5d50-ef11-43c7-845d-4cc643cbd88b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231D0C0EF10>]}
[0m13:54:16.862484 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:54:17.407856 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:54:17.409571 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:54:17.410570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a52d5d50-ef11-43c7-845d-4cc643cbd88b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231D2199850>]}
[0m13:54:18.738653 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.stg_couriers' (models\staging\stg_couriers.sql) depends on a source named 'raw.couriers' which was not found
[0m13:54:18.740619 [debug] [MainThread]: Command `dbt ls` failed at 13:54:18.740448 after 2.59 seconds
[0m13:54:18.741137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231D256A6C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231D2568230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231D0D6FE70>]}
[0m13:54:18.741614 [debug] [MainThread]: Flushing usage events
[0m13:54:19.817483 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:57:04.046895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A335714980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3361ACB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3367F7D90>]}


============================== 13:57:04.051819 | 5b818468-9b15-4aac-b0a2-e216a692e801 ==============================
[0m13:57:04.051819 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:57:04.052819 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt clean', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:57:04.243329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5b818468-9b15-4aac-b0a2-e216a692e801', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A335B78770>]}
[0m13:57:04.276209 [debug] [MainThread]: Command `dbt clean` succeeded at 13:57:04.276056 after 0.47 seconds
[0m13:57:04.276813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A33669FCE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A33669F680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A337D0E150>]}
[0m13:57:04.277326 [debug] [MainThread]: Flushing usage events
[0m13:57:05.455994 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:57:11.222256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027FAA9C0980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027FAB46CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027FABABBD90>]}


============================== 13:57:11.227188 | 260b6779-cc26-4336-8a0f-1de9215a560f ==============================
[0m13:57:11.227188 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:57:11.228341 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt deps', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:57:11.437840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '260b6779-cc26-4336-8a0f-1de9215a560f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027FAAE28770>]}
[0m13:57:11.487684 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:57:11.492598 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:57:11.494860 [debug] [MainThread]: Command `dbt deps` succeeded at 13:57:11.494551 after 0.47 seconds
[0m13:57:11.495415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027FAB95FF00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027FAB95F130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027FAB93BA50>]}
[0m13:57:11.495952 [debug] [MainThread]: Flushing usage events
[0m13:57:12.584064 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:57:20.222982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013BDD564980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013BDE00CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013BDE65BD90>]}


============================== 13:57:20.228038 | 40f9d7f2-ef3c-4999-8037-11105efa282a ==============================
[0m13:57:20.228038 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:57:20.228873 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:57:20.260163 [info ] [MainThread]: dbt version: 1.10.15
[0m13:57:20.260902 [info ] [MainThread]: python version: 3.13.3
[0m13:57:20.261427 [info ] [MainThread]: python path: C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\venv\Scripts\python.exe
[0m13:57:20.261951 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m13:57:20.387320 [info ] [MainThread]: Using profiles dir at C:\Users\Korisnik\.dbt
[0m13:57:20.387928 [info ] [MainThread]: Using profiles.yml file at C:\Users\Korisnik\.dbt\profiles.yml
[0m13:57:20.388480 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\dbt_project.yml
[0m13:57:20.390485 [info ] [MainThread]: adapter type: postgres
[0m13:57:20.391142 [info ] [MainThread]: adapter version: 1.9.1
[0m13:57:20.526169 [info ] [MainThread]: Configuration:
[0m13:57:20.527030 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:57:20.527678 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:57:20.528215 [info ] [MainThread]: Required dependencies:
[0m13:57:20.528831 [debug] [MainThread]: Executing "git --help"
[0m13:57:21.285089 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:57:21.285940 [debug] [MainThread]: STDERR: "b''"
[0m13:57:21.286649 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:57:21.287509 [info ] [MainThread]: Connection:
[0m13:57:21.288160 [info ] [MainThread]:   host: localhost
[0m13:57:21.289052 [info ] [MainThread]:   port: 5433
[0m13:57:21.289582 [info ] [MainThread]:   user: postgres
[0m13:57:21.290293 [info ] [MainThread]:   database: delivery_analytics
[0m13:57:21.290867 [info ] [MainThread]:   schema: analytics
[0m13:57:21.291478 [info ] [MainThread]:   connect_timeout: 10
[0m13:57:21.292082 [info ] [MainThread]:   role: None
[0m13:57:21.292634 [info ] [MainThread]:   search_path: None
[0m13:57:21.293177 [info ] [MainThread]:   keepalives_idle: 0
[0m13:57:21.293725 [info ] [MainThread]:   sslmode: None
[0m13:57:21.294257 [info ] [MainThread]:   sslcert: None
[0m13:57:21.294888 [info ] [MainThread]:   sslkey: None
[0m13:57:21.295435 [info ] [MainThread]:   sslrootcert: None
[0m13:57:21.295925 [info ] [MainThread]:   application_name: dbt
[0m13:57:21.296612 [info ] [MainThread]:   retries: 1
[0m13:57:21.297703 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:57:21.904178 [debug] [MainThread]: Acquiring new postgres connection 'debug'
[0m13:57:22.097544 [debug] [MainThread]: Using postgres connection "debug"
[0m13:57:22.098152 [debug] [MainThread]: On debug: select 1 as id
[0m13:57:22.098640 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:57:22.211902 [debug] [MainThread]: SQL status: SELECT 1 in 0.113 seconds
[0m13:57:22.213001 [debug] [MainThread]: On debug: Close
[0m13:57:22.213468 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m13:57:22.214196 [info ] [MainThread]: [32mAll checks passed![0m
[0m13:57:22.215877 [debug] [MainThread]: Command `dbt debug` succeeded at 13:57:22.215687 after 2.19 seconds
[0m13:57:22.216323 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:57:22.216799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013BDFC32B10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013BDFD096D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013BDFA97CE0>]}
[0m13:57:22.217201 [debug] [MainThread]: Flushing usage events
[0m13:57:23.267990 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:57:28.672770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9CB890980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9CC33CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9CC987D90>]}


============================== 13:57:28.679211 | 058f2b60-19b5-41da-90bc-1f25f9c87593 ==============================
[0m13:57:28.679211 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:57:28.680257 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt ls --resource-type source', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:57:28.960676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '058f2b60-19b5-41da-90bc-1f25f9c87593', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9CBCF8770>]}
[0m13:57:29.039757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '058f2b60-19b5-41da-90bc-1f25f9c87593', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9CC82EF10>]}
[0m13:57:29.041232 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:57:29.575926 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:57:29.577064 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:57:29.577667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '058f2b60-19b5-41da-90bc-1f25f9c87593', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9CDE09850>]}
[0m13:57:31.059273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '058f2b60-19b5-41da-90bc-1f25f9c87593', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9CC9E3D40>]}
[0m13:57:31.138140 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:57:31.163498 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:57:31.203161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '058f2b60-19b5-41da-90bc-1f25f9c87593', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9CDF38E50>]}
[0m13:57:31.203679 [info ] [MainThread]: Found 11 models, 5 sources, 459 macros
[0m13:57:31.204341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '058f2b60-19b5-41da-90bc-1f25f9c87593', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9CDE91230>]}
[0m13:57:31.205643 [info ] [MainThread]: source:dbt_delivery_analytics.raw.courier_assignments
[0m13:57:31.206249 [info ] [MainThread]: source:dbt_delivery_analytics.raw.couriers
[0m13:57:31.206780 [info ] [MainThread]: source:dbt_delivery_analytics.raw.shipment_status
[0m13:57:31.207335 [info ] [MainThread]: source:dbt_delivery_analytics.raw.shipments
[0m13:57:31.207965 [info ] [MainThread]: source:dbt_delivery_analytics.raw.users
[0m13:57:31.209301 [debug] [MainThread]: Command `dbt ls` succeeded at 13:57:31.209178 after 2.72 seconds
[0m13:57:31.209683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9CC753590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9CE4628E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9CE462990>]}
[0m13:57:31.210031 [debug] [MainThread]: Flushing usage events
[0m13:57:32.283618 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:58:18.188041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C965688980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C96612CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C96677BD90>]}


============================== 13:58:18.192993 | a785f218-1083-4e43-92cc-02925959720b ==============================
[0m13:58:18.192993 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:58:18.193995 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:58:18.526805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a785f218-1083-4e43-92cc-02925959720b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C965AE8770>]}
[0m13:58:18.610808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a785f218-1083-4e43-92cc-02925959720b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C96661EE00>]}
[0m13:58:18.612517 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:58:19.061958 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:58:19.268221 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:58:19.268954 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:58:19.338857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a785f218-1083-4e43-92cc-02925959720b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9680A4150>]}
[0m13:58:19.449986 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:58:19.455687 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:58:19.500629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a785f218-1083-4e43-92cc-02925959720b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C96660FD40>]}
[0m13:58:19.501449 [info ] [MainThread]: Found 11 models, 5 sources, 459 macros
[0m13:58:19.502147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a785f218-1083-4e43-92cc-02925959720b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C966C77310>]}
[0m13:58:19.505503 [info ] [MainThread]: 
[0m13:58:19.506328 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:58:19.506986 [info ] [MainThread]: 
[0m13:58:19.507980 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m13:58:19.514989 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m13:58:19.706212 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m13:58:19.706814 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m13:58:19.707240 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:58:19.868681 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.161 seconds
[0m13:58:19.870961 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m13:58:19.872409 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_delivery_analytics, now create_delivery_analytics_analytics)
[0m13:58:19.873242 [debug] [ThreadPool]: Creating schema "database: "delivery_analytics"
schema: "analytics"
"
[0m13:58:19.880923 [debug] [ThreadPool]: Using postgres connection "create_delivery_analytics_analytics"
[0m13:58:19.881497 [debug] [ThreadPool]: On create_delivery_analytics_analytics: BEGIN
[0m13:58:19.881851 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:58:19.977932 [debug] [ThreadPool]: SQL status: BEGIN in 0.096 seconds
[0m13:58:19.978617 [debug] [ThreadPool]: Using postgres connection "create_delivery_analytics_analytics"
[0m13:58:19.979093 [debug] [ThreadPool]: On create_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "create_delivery_analytics_analytics"} */
create schema if not exists "analytics"
[0m13:58:19.980799 [debug] [ThreadPool]: SQL status: CREATE SCHEMA in 0.001 seconds
[0m13:58:19.982415 [debug] [ThreadPool]: On create_delivery_analytics_analytics: COMMIT
[0m13:58:19.982956 [debug] [ThreadPool]: Using postgres connection "create_delivery_analytics_analytics"
[0m13:58:19.983443 [debug] [ThreadPool]: On create_delivery_analytics_analytics: COMMIT
[0m13:58:19.985185 [debug] [ThreadPool]: SQL status: COMMIT in 0.001 seconds
[0m13:58:19.985636 [debug] [ThreadPool]: On create_delivery_analytics_analytics: Close
[0m13:58:19.989765 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m13:58:19.999439 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m13:58:20.000227 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m13:58:20.000785 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:58:20.086487 [debug] [ThreadPool]: SQL status: BEGIN in 0.086 seconds
[0m13:58:20.087327 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m13:58:20.087884 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m13:58:20.101234 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.013 seconds
[0m13:58:20.103145 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m13:58:20.104569 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m13:58:20.113412 [debug] [MainThread]: Using postgres connection "master"
[0m13:58:20.114081 [debug] [MainThread]: On master: BEGIN
[0m13:58:20.114577 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:58:20.190380 [debug] [MainThread]: SQL status: BEGIN in 0.076 seconds
[0m13:58:20.191037 [debug] [MainThread]: Using postgres connection "master"
[0m13:58:20.191642 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m13:58:20.203827 [debug] [MainThread]: SQL status: SELECT 0 in 0.011 seconds
[0m13:58:20.205955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a785f218-1083-4e43-92cc-02925959720b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9684C8600>]}
[0m13:58:20.206617 [debug] [MainThread]: On master: ROLLBACK
[0m13:58:20.207416 [debug] [MainThread]: Using postgres connection "master"
[0m13:58:20.207844 [debug] [MainThread]: On master: BEGIN
[0m13:58:20.208677 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m13:58:20.209077 [debug] [MainThread]: On master: COMMIT
[0m13:58:20.209420 [debug] [MainThread]: Using postgres connection "master"
[0m13:58:20.209744 [debug] [MainThread]: On master: COMMIT
[0m13:58:20.210260 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:58:20.210570 [debug] [MainThread]: On master: Close
[0m13:58:20.217908 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.stg_couriers
[0m13:58:20.218644 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.stg_courier_assigments
[0m13:58:20.219227 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.stg_shipment_status
[0m13:58:20.219755 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.stg_shipments
[0m13:58:20.220489 [info ] [Thread-2 (]: 2 of 8 START sql view model analytics.stg_couriers ............................. [RUN]
[0m13:58:20.221159 [info ] [Thread-1 (]: 1 of 8 START sql view model analytics.stg_courier_assigments ................... [RUN]
[0m13:58:20.223394 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_couriers'
[0m13:58:20.221971 [info ] [Thread-3 (]: 3 of 8 START sql view model analytics.stg_shipment_status ...................... [RUN]
[0m13:58:20.224342 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_courier_assigments'
[0m13:58:20.222723 [info ] [Thread-4 (]: 4 of 8 START sql view model analytics.stg_shipments ............................ [RUN]
[0m13:58:20.225009 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.stg_couriers
[0m13:58:20.225778 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_shipment_status'
[0m13:58:20.226494 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.stg_courier_assigments
[0m13:58:20.227110 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_shipments'
[0m13:58:20.234478 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_couriers"
[0m13:58:20.235142 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.stg_shipment_status
[0m13:58:20.238652 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_courier_assigments"
[0m13:58:20.239242 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.stg_shipments
[0m13:58:20.242517 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_shipment_status"
[0m13:58:20.245746 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_shipments"
[0m13:58:20.251972 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.stg_shipment_status
[0m13:58:20.252504 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.stg_couriers
[0m13:58:20.253308 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.stg_shipments
[0m13:58:20.288065 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.stg_courier_assigments
[0m13:58:20.307292 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_shipment_status"
[0m13:58:20.310490 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_couriers"
[0m13:58:20.314519 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_shipments"
[0m13:58:20.317659 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_courier_assigments"
[0m13:58:20.325295 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_courier_assigments"
[0m13:58:20.326099 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m13:58:20.326747 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_courier_assigments: BEGIN
[0m13:58:20.327332 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_couriers: BEGIN
[0m13:58:20.327927 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipment_status"
[0m13:58:20.328414 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m13:58:20.328988 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:58:20.329472 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:58:20.330002 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipment_status: BEGIN
[0m13:58:20.330564 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_shipments: BEGIN
[0m13:58:20.331498 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:58:20.332002 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:58:20.409807 [debug] [Thread-2 (]: SQL status: BEGIN in 0.080 seconds
[0m13:58:20.410637 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m13:58:20.411228 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_couriers"} */

  create view "delivery_analytics"."analytics"."stg_couriers__dbt_tmp"
    
    
  as (
    with source as (
    select *
    from "delivery_analytics"."raw"."couriers"
)

select
    courier_id,
    user_id,
    vehicle_type,
    city
from source;
  );
[0m13:58:20.414126 [debug] [Thread-2 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 17: from source;
                    ^

[0m13:58:20.414837 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_couriers: ROLLBACK
[0m13:58:20.415799 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_couriers: Close
[0m13:58:20.431106 [debug] [Thread-2 (]: Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  syntax error at or near ";"
  LINE 17: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_couriers.sql
[0m13:58:20.431881 [debug] [Thread-1 (]: SQL status: BEGIN in 0.103 seconds
[0m13:58:20.433834 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_courier_assigments"
[0m13:58:20.434546 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_courier_assigments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_courier_assigments"} */

  create view "delivery_analytics"."analytics"."stg_courier_assigments__dbt_tmp"
    
    
  as (
    with source as (
    select *
    from "delivery_analytics"."raw"."courier_assignments"
)

select
    assignment_id,
    courier_id,
    shipment_id,
    assigned_at,
    delivered_at
from source;
  );
[0m13:58:20.435924 [debug] [Thread-1 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 18: from source;
                    ^

[0m13:58:20.436630 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a785f218-1083-4e43-92cc-02925959720b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C968116A50>]}
[0m13:58:20.437214 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_courier_assigments: ROLLBACK
[0m13:58:20.438179 [error] [Thread-2 (]: 2 of 8 ERROR creating sql view model analytics.stg_couriers .................... [[31mERROR[0m in 0.21s]
[0m13:58:20.439230 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.stg_couriers
[0m13:58:20.439851 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_courier_assigments: Close
[0m13:58:20.440481 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.stg_users
[0m13:58:20.441179 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_couriers' to be skipped because of status 'error'.  Reason: Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  syntax error at or near ";"
  LINE 17: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_couriers.sql.
[0m13:58:20.441957 [info ] [Thread-2 (]: 5 of 8 START sql view model analytics.stg_users ................................ [RUN]
[0m13:58:20.444761 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.stg_couriers, now model.dbt_delivery_analytics.stg_users)
[0m13:58:20.445429 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.stg_users
[0m13:58:20.449511 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_users"
[0m13:58:20.451287 [debug] [Thread-3 (]: SQL status: BEGIN in 0.120 seconds
[0m13:58:20.451832 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipment_status"
[0m13:58:20.452833 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipment_status"} */

  create view "delivery_analytics"."analytics"."stg_shipment_status__dbt_tmp"
    
    
  as (
    with source as (
    select *
    from "delivery_analytics"."raw"."shipment_status"
)

select
    shipment_status_id,
    shipment_id,
    status,
    status_timestamp,
    courier_id
from source;
  );
[0m13:58:20.456767 [debug] [Thread-3 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 18: from source;
                    ^

[0m13:58:20.457559 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.stg_users
[0m13:58:20.458651 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipment_status: ROLLBACK
[0m13:58:20.459886 [debug] [Thread-1 (]: Database Error in model stg_courier_assigments (models\staging\stg_courier_assigments.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_courier_assigments.sql
[0m13:58:20.471291 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_users"
[0m13:58:20.472394 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a785f218-1083-4e43-92cc-02925959720b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9681322B0>]}
[0m13:58:20.472979 [debug] [Thread-4 (]: SQL status: BEGIN in 0.141 seconds
[0m13:58:20.473567 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipment_status: Close
[0m13:58:20.474958 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m13:58:20.474475 [error] [Thread-1 (]: 1 of 8 ERROR creating sql view model analytics.stg_courier_assigments .......... [[31mERROR[0m in 0.25s]
[0m13:58:20.475906 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipments"} */

  create view "delivery_analytics"."analytics"."stg_shipments__dbt_tmp"
    
    
  as (
    with source as (
    select *
    from "delivery_analytics"."raw"."shipments"
)

select
    shipment_id,
    sender_id,
    receiver_id,
    shipment_date,
    expected_delivery_date,
    price
from source;
  );
[0m13:58:20.477992 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.stg_courier_assigments
[0m13:58:20.478632 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_users"
[0m13:58:20.479753 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_courier_assigments' to be skipped because of status 'error'.  Reason: Database Error in model stg_courier_assigments (models\staging\stg_courier_assigments.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_courier_assigments.sql.
[0m13:58:20.480313 [debug] [Thread-4 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 19: from source;
                    ^

[0m13:58:20.480802 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_users: BEGIN
[0m13:58:20.481594 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_shipments: ROLLBACK
[0m13:58:20.482164 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:58:20.483145 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_shipments: Close
[0m13:58:20.489565 [debug] [Thread-3 (]: Database Error in model stg_shipment_status (models\staging\stg_shipment_status.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipment_status.sql
[0m13:58:20.490730 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a785f218-1083-4e43-92cc-02925959720b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9683DA990>]}
[0m13:58:20.491967 [error] [Thread-3 (]: 3 of 8 ERROR creating sql view model analytics.stg_shipment_status ............. [[31mERROR[0m in 0.26s]
[0m13:58:20.493099 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.stg_shipment_status
[0m13:58:20.493884 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_shipment_status' to be skipped because of status 'error'.  Reason: Database Error in model stg_shipment_status (models\staging\stg_shipment_status.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipment_status.sql.
[0m13:58:20.498758 [debug] [Thread-4 (]: Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  syntax error at or near ";"
  LINE 19: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipments.sql
[0m13:58:20.499648 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a785f218-1083-4e43-92cc-02925959720b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9683F5850>]}
[0m13:58:20.500751 [error] [Thread-4 (]: 4 of 8 ERROR creating sql view model analytics.stg_shipments ................... [[31mERROR[0m in 0.27s]
[0m13:58:20.502022 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.stg_shipments
[0m13:58:20.502726 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_shipments' to be skipped because of status 'error'.  Reason: Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  syntax error at or near ";"
  LINE 19: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipments.sql.
[0m13:58:20.503562 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.dim_dates
[0m13:58:20.504172 [info ] [Thread-1 (]: 6 of 8 SKIP relation analytics.dim_dates ....................................... [[33mSKIP[0m]
[0m13:58:20.504961 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.dim_dates
[0m13:58:20.608113 [debug] [Thread-2 (]: SQL status: BEGIN in 0.126 seconds
[0m13:58:20.608735 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_users"
[0m13:58:20.609178 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_users: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_users"} */

  create view "delivery_analytics"."analytics"."stg_users__dbt_tmp"
    
    
  as (
    with source as (
    select *
    from "delivery_analytics"."raw"."users"
)

select
    user_id,
    username,
    email,
    role,
    created_at
from source;
  );
[0m13:58:20.610022 [debug] [Thread-2 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 18: from source;
                    ^

[0m13:58:20.610529 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_users: ROLLBACK
[0m13:58:20.611283 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_users: Close
[0m13:58:20.618241 [debug] [Thread-2 (]: Database Error in model stg_users (models\staging\stg_users.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_users.sql
[0m13:58:20.618905 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a785f218-1083-4e43-92cc-02925959720b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C968526B30>]}
[0m13:58:20.619616 [error] [Thread-2 (]: 5 of 8 ERROR creating sql view model analytics.stg_users ....................... [[31mERROR[0m in 0.17s]
[0m13:58:20.620388 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.stg_users
[0m13:58:20.621050 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_users' to be skipped because of status 'error'.  Reason: Database Error in model stg_users (models\staging\stg_users.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_users.sql.
[0m13:58:20.622009 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.dim_users
[0m13:58:20.623078 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.dim_couriers
[0m13:58:20.622515 [info ] [Thread-4 (]: 8 of 8 SKIP relation analytics.dim_users ....................................... [[33mSKIP[0m]
[0m13:58:20.624428 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.dim_users
[0m13:58:20.623812 [info ] [Thread-3 (]: 7 of 8 SKIP relation analytics.dim_couriers .................................... [[33mSKIP[0m]
[0m13:58:20.625340 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.dim_couriers
[0m13:58:20.627099 [debug] [MainThread]: Using postgres connection "master"
[0m13:58:20.627561 [debug] [MainThread]: On master: BEGIN
[0m13:58:20.627847 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:58:20.735795 [debug] [MainThread]: SQL status: BEGIN in 0.108 seconds
[0m13:58:20.736264 [debug] [MainThread]: On master: COMMIT
[0m13:58:20.736566 [debug] [MainThread]: Using postgres connection "master"
[0m13:58:20.736834 [debug] [MainThread]: On master: COMMIT
[0m13:58:20.737268 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:58:20.737573 [debug] [MainThread]: On master: Close
[0m13:58:20.738115 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:58:20.738532 [debug] [MainThread]: Connection 'create_delivery_analytics_analytics' was properly closed.
[0m13:58:20.738805 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m13:58:20.739277 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_users' was properly closed.
[0m13:58:20.739674 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_courier_assigments' was properly closed.
[0m13:58:20.740142 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_shipment_status' was properly closed.
[0m13:58:20.740505 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_shipments' was properly closed.
[0m13:58:20.741029 [info ] [MainThread]: 
[0m13:58:20.741813 [info ] [MainThread]: Finished running 3 table models, 5 view models in 0 hours 0 minutes and 1.23 seconds (1.23s).
[0m13:58:20.743408 [debug] [MainThread]: Command end result
[0m13:58:20.857690 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:58:20.862329 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:58:20.870576 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m13:58:20.871305 [info ] [MainThread]: 
[0m13:58:20.872049 [info ] [MainThread]: [31mCompleted with 5 errors, 0 partial successes, and 0 warnings:[0m
[0m13:58:20.872576 [info ] [MainThread]: 
[0m13:58:20.873127 [error] [MainThread]: [31mFailure in model stg_couriers (models\staging\stg_couriers.sql)[0m
[0m13:58:20.873622 [error] [MainThread]:   Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  syntax error at or near ";"
  LINE 17: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_couriers.sql
[0m13:58:20.874172 [info ] [MainThread]: 
[0m13:58:20.874737 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_couriers.sql
[0m13:58:20.875202 [info ] [MainThread]: 
[0m13:58:20.875761 [error] [MainThread]: [31mFailure in model stg_courier_assigments (models\staging\stg_courier_assigments.sql)[0m
[0m13:58:20.876304 [error] [MainThread]:   Database Error in model stg_courier_assigments (models\staging\stg_courier_assigments.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_courier_assigments.sql
[0m13:58:20.876808 [info ] [MainThread]: 
[0m13:58:20.877449 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_courier_assigments.sql
[0m13:58:20.877918 [info ] [MainThread]: 
[0m13:58:20.878551 [error] [MainThread]: [31mFailure in model stg_shipment_status (models\staging\stg_shipment_status.sql)[0m
[0m13:58:20.879279 [error] [MainThread]:   Database Error in model stg_shipment_status (models\staging\stg_shipment_status.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipment_status.sql
[0m13:58:20.879817 [info ] [MainThread]: 
[0m13:58:20.880432 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_shipment_status.sql
[0m13:58:20.880951 [info ] [MainThread]: 
[0m13:58:20.881600 [error] [MainThread]: [31mFailure in model stg_shipments (models\staging\stg_shipments.sql)[0m
[0m13:58:20.882303 [error] [MainThread]:   Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  syntax error at or near ";"
  LINE 19: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipments.sql
[0m13:58:20.882897 [info ] [MainThread]: 
[0m13:58:20.883530 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_shipments.sql
[0m13:58:20.884091 [info ] [MainThread]: 
[0m13:58:20.884709 [error] [MainThread]: [31mFailure in model stg_users (models\staging\stg_users.sql)[0m
[0m13:58:20.885822 [error] [MainThread]:   Database Error in model stg_users (models\staging\stg_users.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_users.sql
[0m13:58:20.886567 [info ] [MainThread]: 
[0m13:58:20.887334 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_users.sql
[0m13:58:20.888447 [info ] [MainThread]: 
[0m13:58:20.889064 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=5 SKIP=3 NO-OP=0 TOTAL=8
[0m13:58:20.892089 [debug] [MainThread]: Command `dbt run` failed at 13:58:20.891765 after 2.90 seconds
[0m13:58:20.892802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C9652E4290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C965967470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C967CD7D10>]}
[0m13:58:20.893195 [debug] [MainThread]: Flushing usage events
[0m13:58:22.120898 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:58:31.637041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D490FF0980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D491A9CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4920E7D90>]}


============================== 13:58:31.642708 | f87eca52-3f68-4181-ac3e-923b893f22a7 ==============================
[0m13:58:31.642708 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:58:31.643445 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run --select staging', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:58:31.875676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f87eca52-3f68-4181-ac3e-923b893f22a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D491458770>]}
[0m13:58:31.943564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f87eca52-3f68-4181-ac3e-923b893f22a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D491F8AE00>]}
[0m13:58:31.945160 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:58:32.386923 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:58:32.556928 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:58:32.557373 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:58:32.614898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f87eca52-3f68-4181-ac3e-923b893f22a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D493A60150>]}
[0m13:58:32.692542 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:58:32.696474 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:58:32.728944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f87eca52-3f68-4181-ac3e-923b893f22a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D491F7FD40>]}
[0m13:58:32.729575 [info ] [MainThread]: Found 11 models, 5 sources, 459 macros
[0m13:58:32.730074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f87eca52-3f68-4181-ac3e-923b893f22a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D492277310>]}
[0m13:58:32.732210 [info ] [MainThread]: 
[0m13:58:32.732790 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:58:32.733325 [info ] [MainThread]: 
[0m13:58:32.733979 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m13:58:32.738210 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m13:58:32.885732 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m13:58:32.886198 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m13:58:32.886497 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:58:32.992348 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.106 seconds
[0m13:58:32.993839 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m13:58:32.996661 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m13:58:33.002950 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m13:58:33.003431 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m13:58:33.003703 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:58:33.071623 [debug] [ThreadPool]: SQL status: BEGIN in 0.068 seconds
[0m13:58:33.072152 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m13:58:33.072518 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m13:58:33.082056 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.009 seconds
[0m13:58:33.083385 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m13:58:33.083885 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m13:58:33.089943 [debug] [MainThread]: Using postgres connection "master"
[0m13:58:33.090436 [debug] [MainThread]: On master: BEGIN
[0m13:58:33.090798 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:58:33.178999 [debug] [MainThread]: SQL status: BEGIN in 0.088 seconds
[0m13:58:33.179599 [debug] [MainThread]: Using postgres connection "master"
[0m13:58:33.180210 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m13:58:33.189700 [debug] [MainThread]: SQL status: SELECT 0 in 0.009 seconds
[0m13:58:33.191613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f87eca52-3f68-4181-ac3e-923b893f22a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D493C7BBA0>]}
[0m13:58:33.192328 [debug] [MainThread]: On master: ROLLBACK
[0m13:58:33.193066 [debug] [MainThread]: Using postgres connection "master"
[0m13:58:33.193520 [debug] [MainThread]: On master: BEGIN
[0m13:58:33.194343 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m13:58:33.194778 [debug] [MainThread]: On master: COMMIT
[0m13:58:33.195191 [debug] [MainThread]: Using postgres connection "master"
[0m13:58:33.195579 [debug] [MainThread]: On master: COMMIT
[0m13:58:33.196155 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:58:33.196605 [debug] [MainThread]: On master: Close
[0m13:58:33.202937 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.stg_courier_assigments
[0m13:58:33.203534 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.stg_shipments
[0m13:58:33.207380 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.stg_couriers
[0m13:58:33.208763 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.stg_shipment_status
[0m13:58:33.209607 [info ] [Thread-1 (]: 1 of 5 START sql view model analytics.stg_courier_assigments ................... [RUN]
[0m13:58:33.210126 [info ] [Thread-4 (]: 4 of 5 START sql view model analytics.stg_shipments ............................ [RUN]
[0m13:58:33.212659 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_shipments'
[0m13:58:33.213125 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.stg_shipments
[0m13:58:33.212000 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_courier_assigments'
[0m13:58:33.225352 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_shipments"
[0m13:58:33.210709 [info ] [Thread-2 (]: 2 of 5 START sql view model analytics.stg_couriers ............................. [RUN]
[0m13:58:33.231153 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.stg_courier_assigments
[0m13:58:33.211246 [info ] [Thread-3 (]: 3 of 5 START sql view model analytics.stg_shipment_status ...................... [RUN]
[0m13:58:33.232455 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_couriers'
[0m13:58:33.236730 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_courier_assigments"
[0m13:58:33.237586 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_shipment_status'
[0m13:58:33.238094 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.stg_couriers
[0m13:58:33.238739 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.stg_shipment_status
[0m13:58:33.239220 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.stg_shipments
[0m13:58:33.242524 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_couriers"
[0m13:58:33.244883 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_shipment_status"
[0m13:58:33.280397 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.stg_courier_assigments
[0m13:58:33.286137 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_shipments"
[0m13:58:33.289881 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_courier_assigments"
[0m13:58:33.291024 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.stg_couriers
[0m13:58:33.294387 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_couriers"
[0m13:58:33.295653 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m13:58:33.296132 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.stg_shipment_status
[0m13:58:33.296555 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_courier_assigments"
[0m13:58:33.297005 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_shipments: BEGIN
[0m13:58:33.300223 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_shipment_status"
[0m13:58:33.300659 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_courier_assigments: BEGIN
[0m13:58:33.301071 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m13:58:33.301464 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:58:33.302150 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:58:33.302625 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_couriers: BEGIN
[0m13:58:33.303500 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:58:33.304102 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipment_status"
[0m13:58:33.304656 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipment_status: BEGIN
[0m13:58:33.305254 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:58:33.371500 [debug] [Thread-4 (]: SQL status: BEGIN in 0.070 seconds
[0m13:58:33.372016 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m13:58:33.372600 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipments"} */

  create view "delivery_analytics"."analytics"."stg_shipments__dbt_tmp"
    
    
  as (
    with source as (
    select *
    from "delivery_analytics"."raw"."shipments"
)

select
    shipment_id,
    sender_id,
    receiver_id,
    shipment_date,
    expected_delivery_date,
    price
from source;
  );
[0m13:58:33.373710 [debug] [Thread-4 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 19: from source;
                    ^

[0m13:58:33.374358 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_shipments: ROLLBACK
[0m13:58:33.375262 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_shipments: Close
[0m13:58:33.385533 [debug] [Thread-4 (]: Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  syntax error at or near ";"
  LINE 19: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipments.sql
[0m13:58:33.388200 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f87eca52-3f68-4181-ac3e-923b893f22a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D493ADB410>]}
[0m13:58:33.388965 [error] [Thread-4 (]: 4 of 5 ERROR creating sql view model analytics.stg_shipments ................... [[31mERROR[0m in 0.17s]
[0m13:58:33.390013 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.stg_shipments
[0m13:58:33.390450 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.stg_users
[0m13:58:33.391081 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_shipments' to be skipped because of status 'error'.  Reason: Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  syntax error at or near ";"
  LINE 19: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipments.sql.
[0m13:58:33.391754 [info ] [Thread-4 (]: 5 of 5 START sql view model analytics.stg_users ................................ [RUN]
[0m13:58:33.393593 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.stg_shipments, now model.dbt_delivery_analytics.stg_users)
[0m13:58:33.394282 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.stg_users
[0m13:58:33.398075 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_users"
[0m13:58:33.399939 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.stg_users
[0m13:58:33.404065 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_users"
[0m13:58:33.405361 [debug] [Thread-1 (]: SQL status: BEGIN in 0.103 seconds
[0m13:58:33.405974 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_users"
[0m13:58:33.406611 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_courier_assigments"
[0m13:58:33.407110 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_users: BEGIN
[0m13:58:33.407632 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_courier_assigments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_courier_assigments"} */

  create view "delivery_analytics"."analytics"."stg_courier_assigments__dbt_tmp"
    
    
  as (
    with source as (
    select *
    from "delivery_analytics"."raw"."courier_assignments"
)

select
    assignment_id,
    courier_id,
    shipment_id,
    assigned_at,
    delivered_at
from source;
  );
[0m13:58:33.408143 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:58:33.409403 [debug] [Thread-1 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 18: from source;
                    ^

[0m13:58:33.409844 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_courier_assigments: ROLLBACK
[0m13:58:33.410602 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_courier_assigments: Close
[0m13:58:33.421022 [debug] [Thread-1 (]: Database Error in model stg_courier_assigments (models\staging\stg_courier_assigments.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_courier_assigments.sql
[0m13:58:33.422010 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f87eca52-3f68-4181-ac3e-923b893f22a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D493AF9B20>]}
[0m13:58:33.422809 [error] [Thread-1 (]: 1 of 5 ERROR creating sql view model analytics.stg_courier_assigments .......... [[31mERROR[0m in 0.21s]
[0m13:58:33.424002 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.stg_courier_assigments
[0m13:58:33.424714 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_courier_assigments' to be skipped because of status 'error'.  Reason: Database Error in model stg_courier_assigments (models\staging\stg_courier_assigments.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_courier_assigments.sql.
[0m13:58:33.427621 [debug] [Thread-2 (]: SQL status: BEGIN in 0.124 seconds
[0m13:58:33.428133 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m13:58:33.428495 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_couriers"} */

  create view "delivery_analytics"."analytics"."stg_couriers__dbt_tmp"
    
    
  as (
    with source as (
    select *
    from "delivery_analytics"."raw"."couriers"
)

select
    courier_id,
    user_id,
    vehicle_type,
    city
from source;
  );
[0m13:58:33.429484 [debug] [Thread-2 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 17: from source;
                    ^

[0m13:58:33.430022 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_couriers: ROLLBACK
[0m13:58:33.430861 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_couriers: Close
[0m13:58:33.437815 [debug] [Thread-2 (]: Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  syntax error at or near ";"
  LINE 17: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_couriers.sql
[0m13:58:33.438429 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f87eca52-3f68-4181-ac3e-923b893f22a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4913D4C30>]}
[0m13:58:33.439061 [error] [Thread-2 (]: 2 of 5 ERROR creating sql view model analytics.stg_couriers .................... [[31mERROR[0m in 0.21s]
[0m13:58:33.440002 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.stg_couriers
[0m13:58:33.440672 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_couriers' to be skipped because of status 'error'.  Reason: Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  syntax error at or near ";"
  LINE 17: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_couriers.sql.
[0m13:58:33.444255 [debug] [Thread-3 (]: SQL status: BEGIN in 0.139 seconds
[0m13:58:33.444732 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipment_status"
[0m13:58:33.445087 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipment_status"} */

  create view "delivery_analytics"."analytics"."stg_shipment_status__dbt_tmp"
    
    
  as (
    with source as (
    select *
    from "delivery_analytics"."raw"."shipment_status"
)

select
    shipment_status_id,
    shipment_id,
    status,
    status_timestamp,
    courier_id
from source;
  );
[0m13:58:33.445914 [debug] [Thread-3 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 18: from source;
                    ^

[0m13:58:33.446363 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipment_status: ROLLBACK
[0m13:58:33.447152 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipment_status: Close
[0m13:58:33.453772 [debug] [Thread-3 (]: Database Error in model stg_shipment_status (models\staging\stg_shipment_status.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipment_status.sql
[0m13:58:33.454426 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f87eca52-3f68-4181-ac3e-923b893f22a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D48EFA4450>]}
[0m13:58:33.455144 [error] [Thread-3 (]: 3 of 5 ERROR creating sql view model analytics.stg_shipment_status ............. [[31mERROR[0m in 0.22s]
[0m13:58:33.455950 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.stg_shipment_status
[0m13:58:33.456654 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_shipment_status' to be skipped because of status 'error'.  Reason: Database Error in model stg_shipment_status (models\staging\stg_shipment_status.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipment_status.sql.
[0m13:58:33.483896 [debug] [Thread-4 (]: SQL status: BEGIN in 0.076 seconds
[0m13:58:33.484429 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_users"
[0m13:58:33.484772 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_users: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_users"} */

  create view "delivery_analytics"."analytics"."stg_users__dbt_tmp"
    
    
  as (
    with source as (
    select *
    from "delivery_analytics"."raw"."users"
)

select
    user_id,
    username,
    email,
    role,
    created_at
from source;
  );
[0m13:58:33.485670 [debug] [Thread-4 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 18: from source;
                    ^

[0m13:58:33.486112 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_users: ROLLBACK
[0m13:58:33.486799 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_users: Close
[0m13:58:33.492128 [debug] [Thread-4 (]: Database Error in model stg_users (models\staging\stg_users.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_users.sql
[0m13:58:33.492718 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f87eca52-3f68-4181-ac3e-923b893f22a7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D493F5FD90>]}
[0m13:58:33.493329 [error] [Thread-4 (]: 5 of 5 ERROR creating sql view model analytics.stg_users ....................... [[31mERROR[0m in 0.10s]
[0m13:58:33.494224 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.stg_users
[0m13:58:33.494836 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_users' to be skipped because of status 'error'.  Reason: Database Error in model stg_users (models\staging\stg_users.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_users.sql.
[0m13:58:33.496650 [debug] [MainThread]: Using postgres connection "master"
[0m13:58:33.497204 [debug] [MainThread]: On master: BEGIN
[0m13:58:33.497543 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:58:33.563632 [debug] [MainThread]: SQL status: BEGIN in 0.066 seconds
[0m13:58:33.564116 [debug] [MainThread]: On master: COMMIT
[0m13:58:33.564419 [debug] [MainThread]: Using postgres connection "master"
[0m13:58:33.564700 [debug] [MainThread]: On master: COMMIT
[0m13:58:33.565133 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:58:33.565432 [debug] [MainThread]: On master: Close
[0m13:58:33.565930 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:58:33.566195 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m13:58:33.566449 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m13:58:33.566691 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_courier_assigments' was properly closed.
[0m13:58:33.566974 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_users' was properly closed.
[0m13:58:33.567215 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_couriers' was properly closed.
[0m13:58:33.567457 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_shipment_status' was properly closed.
[0m13:58:33.567857 [info ] [MainThread]: 
[0m13:58:33.568439 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 0.83 seconds (0.83s).
[0m13:58:33.570254 [debug] [MainThread]: Command end result
[0m13:58:33.664708 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:58:33.668318 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:58:33.675246 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m13:58:33.675670 [info ] [MainThread]: 
[0m13:58:33.676272 [info ] [MainThread]: [31mCompleted with 5 errors, 0 partial successes, and 0 warnings:[0m
[0m13:58:33.676813 [info ] [MainThread]: 
[0m13:58:33.677429 [error] [MainThread]: [31mFailure in model stg_shipments (models\staging\stg_shipments.sql)[0m
[0m13:58:33.678038 [error] [MainThread]:   Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  syntax error at or near ";"
  LINE 19: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipments.sql
[0m13:58:33.678636 [info ] [MainThread]: 
[0m13:58:33.679202 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_shipments.sql
[0m13:58:33.679779 [info ] [MainThread]: 
[0m13:58:33.680327 [error] [MainThread]: [31mFailure in model stg_courier_assigments (models\staging\stg_courier_assigments.sql)[0m
[0m13:58:33.680942 [error] [MainThread]:   Database Error in model stg_courier_assigments (models\staging\stg_courier_assigments.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_courier_assigments.sql
[0m13:58:33.681516 [info ] [MainThread]: 
[0m13:58:33.682123 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_courier_assigments.sql
[0m13:58:33.682614 [info ] [MainThread]: 
[0m13:58:33.683228 [error] [MainThread]: [31mFailure in model stg_couriers (models\staging\stg_couriers.sql)[0m
[0m13:58:33.683949 [error] [MainThread]:   Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  syntax error at or near ";"
  LINE 17: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_couriers.sql
[0m13:58:33.684472 [info ] [MainThread]: 
[0m13:58:33.685094 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_couriers.sql
[0m13:58:33.685673 [info ] [MainThread]: 
[0m13:58:33.686371 [error] [MainThread]: [31mFailure in model stg_shipment_status (models\staging\stg_shipment_status.sql)[0m
[0m13:58:33.687132 [error] [MainThread]:   Database Error in model stg_shipment_status (models\staging\stg_shipment_status.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipment_status.sql
[0m13:58:33.687841 [info ] [MainThread]: 
[0m13:58:33.688382 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_shipment_status.sql
[0m13:58:33.688857 [info ] [MainThread]: 
[0m13:58:33.689445 [error] [MainThread]: [31mFailure in model stg_users (models\staging\stg_users.sql)[0m
[0m13:58:33.690027 [error] [MainThread]:   Database Error in model stg_users (models\staging\stg_users.sql)
  syntax error at or near ";"
  LINE 18: from source;
                      ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_users.sql
[0m13:58:33.690478 [info ] [MainThread]: 
[0m13:58:33.690996 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_users.sql
[0m13:58:33.691488 [info ] [MainThread]: 
[0m13:58:33.692067 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=5 SKIP=0 NO-OP=0 TOTAL=5
[0m13:58:33.693732 [debug] [MainThread]: Command `dbt run` failed at 13:58:33.693588 after 2.22 seconds
[0m13:58:33.694145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D491A95FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D491333AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4936F08F0>]}
[0m13:58:33.694516 [debug] [MainThread]: Flushing usage events
[0m13:58:34.721407 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:59:34.512828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD268D4980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD2737CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD279CBD90>]}


============================== 13:59:34.518025 | e216441e-60d6-468b-88cc-51556d6cf323 ==============================
[0m13:59:34.518025 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:59:34.519017 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select staging', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:59:34.775508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e216441e-60d6-468b-88cc-51556d6cf323', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD26D38770>]}
[0m13:59:34.843360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e216441e-60d6-468b-88cc-51556d6cf323', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD2786EE00>]}
[0m13:59:34.845399 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:59:35.284785 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:59:35.434338 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 5 files changed.
[0m13:59:35.435306 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\staging\stg_shipments.sql
[0m13:59:35.435732 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\staging\stg_couriers.sql
[0m13:59:35.436103 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\staging\stg_shipment_status.sql
[0m13:59:35.436460 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\staging\stg_users.sql
[0m13:59:35.436816 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\staging\stg_courier_assigments.sql
[0m13:59:35.737085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e216441e-60d6-468b-88cc-51556d6cf323', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD28E59850>]}
[0m13:59:35.819247 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:59:35.823534 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:59:35.865021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e216441e-60d6-468b-88cc-51556d6cf323', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD2785FD40>]}
[0m13:59:35.865975 [info ] [MainThread]: Found 11 models, 5 sources, 459 macros
[0m13:59:35.866683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e216441e-60d6-468b-88cc-51556d6cf323', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD29475EF0>]}
[0m13:59:35.869126 [info ] [MainThread]: 
[0m13:59:35.869746 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:59:35.870249 [info ] [MainThread]: 
[0m13:59:35.871015 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m13:59:35.876165 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m13:59:36.099825 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m13:59:36.100249 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m13:59:36.100541 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:59:36.223725 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.123 seconds
[0m13:59:36.225263 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m13:59:36.228101 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m13:59:36.234081 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m13:59:36.234490 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m13:59:36.234779 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:59:36.305804 [debug] [ThreadPool]: SQL status: BEGIN in 0.071 seconds
[0m13:59:36.306325 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m13:59:36.306706 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m13:59:36.315587 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.008 seconds
[0m13:59:36.316848 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m13:59:36.317329 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m13:59:36.322984 [debug] [MainThread]: Using postgres connection "master"
[0m13:59:36.323578 [debug] [MainThread]: On master: BEGIN
[0m13:59:36.323870 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:59:36.398967 [debug] [MainThread]: SQL status: BEGIN in 0.075 seconds
[0m13:59:36.399508 [debug] [MainThread]: Using postgres connection "master"
[0m13:59:36.399956 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m13:59:36.409154 [debug] [MainThread]: SQL status: SELECT 0 in 0.009 seconds
[0m13:59:36.410559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e216441e-60d6-468b-88cc-51556d6cf323', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD294EADD0>]}
[0m13:59:36.411115 [debug] [MainThread]: On master: ROLLBACK
[0m13:59:36.411685 [debug] [MainThread]: Using postgres connection "master"
[0m13:59:36.412058 [debug] [MainThread]: On master: BEGIN
[0m13:59:36.412769 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m13:59:36.413150 [debug] [MainThread]: On master: COMMIT
[0m13:59:36.413495 [debug] [MainThread]: Using postgres connection "master"
[0m13:59:36.413816 [debug] [MainThread]: On master: COMMIT
[0m13:59:36.414298 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:59:36.414596 [debug] [MainThread]: On master: Close
[0m13:59:36.420046 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.stg_shipments
[0m13:59:36.420611 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.stg_couriers
[0m13:59:36.420997 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.stg_courier_assigments
[0m13:59:36.421361 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.stg_shipment_status
[0m13:59:36.422127 [info ] [Thread-4 (]: 4 of 5 START sql view model analytics.stg_shipments ............................ [RUN]
[0m13:59:36.422836 [info ] [Thread-2 (]: 2 of 5 START sql view model analytics.stg_couriers ............................. [RUN]
[0m13:59:36.425111 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_shipments'
[0m13:59:36.423623 [info ] [Thread-1 (]: 1 of 5 START sql view model analytics.stg_courier_assigments ................... [RUN]
[0m13:59:36.425963 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_couriers'
[0m13:59:36.426592 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.stg_shipments
[0m13:59:36.424257 [info ] [Thread-3 (]: 3 of 5 START sql view model analytics.stg_shipment_status ...................... [RUN]
[0m13:59:36.427456 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_courier_assigments'
[0m13:59:36.427955 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.stg_couriers
[0m13:59:36.434847 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_shipments"
[0m13:59:36.435525 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_shipment_status'
[0m13:59:36.436047 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.stg_courier_assigments
[0m13:59:36.439359 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_couriers"
[0m13:59:36.440199 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.stg_shipment_status
[0m13:59:36.443373 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_courier_assigments"
[0m13:59:36.446145 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_shipment_status"
[0m13:59:36.447208 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.stg_shipments
[0m13:59:36.485351 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_shipments"
[0m13:59:36.486503 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.stg_shipment_status
[0m13:59:36.490208 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_shipment_status"
[0m13:59:36.491025 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.stg_couriers
[0m13:59:36.494612 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_couriers"
[0m13:59:36.495243 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m13:59:36.495773 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.stg_courier_assigments
[0m13:59:36.496331 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_shipments: BEGIN
[0m13:59:36.499618 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_courier_assigments"
[0m13:59:36.500118 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:59:36.501043 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipment_status"
[0m13:59:36.501635 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m13:59:36.502183 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipment_status: BEGIN
[0m13:59:36.502757 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_couriers: BEGIN
[0m13:59:36.503143 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:59:36.503661 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:59:36.504303 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_courier_assigments"
[0m13:59:36.505242 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_courier_assigments: BEGIN
[0m13:59:36.505819 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:59:36.568610 [debug] [Thread-4 (]: SQL status: BEGIN in 0.068 seconds
[0m13:59:36.569341 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m13:59:36.569921 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipments"} */

  create view "delivery_analytics"."analytics"."stg_shipments__dbt_tmp"
    
    
  as (
    with source as (
    select *
    from "delivery_analytics"."raw"."shipments"
)

select
    shipment_id,
    sender_id,
    receiver_id,
    shipment_date,
    expected_delivery_date,
    price
from source
  );
[0m13:59:36.580395 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.010 seconds
[0m13:59:36.586709 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m13:59:36.587276 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipments"} */
alter table "delivery_analytics"."analytics"."stg_shipments__dbt_tmp" rename to "stg_shipments"
[0m13:59:36.588037 [debug] [Thread-3 (]: SQL status: BEGIN in 0.085 seconds
[0m13:59:36.588606 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipment_status"
[0m13:59:36.589227 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipment_status"} */

  create view "delivery_analytics"."analytics"."stg_shipment_status__dbt_tmp"
    
    
  as (
    with source as (
    select *
    from "delivery_analytics"."raw"."shipment_status"
)

select
    shipment_status_id,
    shipment_id,
    status,
    status_timestamp,
    courier_id
from source
  );
[0m13:59:36.591325 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m13:59:36.605525 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_shipments: COMMIT
[0m13:59:36.606054 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.016 seconds
[0m13:59:36.606496 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m13:59:36.609958 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipment_status"
[0m13:59:36.610486 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_shipments: COMMIT
[0m13:59:36.611062 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipment_status"} */
alter table "delivery_analytics"."analytics"."stg_shipment_status__dbt_tmp" rename to "stg_shipment_status"
[0m13:59:36.611832 [debug] [Thread-2 (]: SQL status: BEGIN in 0.108 seconds
[0m13:59:36.612271 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:59:36.612707 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m13:59:36.613083 [debug] [Thread-4 (]: SQL status: COMMIT in 0.002 seconds
[0m13:59:36.614491 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipment_status: COMMIT
[0m13:59:36.614963 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_couriers"} */

  create view "delivery_analytics"."analytics"."stg_couriers__dbt_tmp"
    
    
  as (
    with source as (
    select *
    from "delivery_analytics"."raw"."couriers"
)

select
    courier_id,
    user_id,
    vehicle_type,
    city
from source
  );
[0m13:59:36.620979 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."stg_shipments__dbt_backup"
[0m13:59:36.621491 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipment_status"
[0m13:59:36.626636 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m13:59:36.627131 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipment_status: COMMIT
[0m13:59:36.627640 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipments"} */
drop view if exists "delivery_analytics"."analytics"."stg_shipments__dbt_backup" cascade
[0m13:59:36.628250 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.006 seconds
[0m13:59:36.631479 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m13:59:36.631991 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.003 seconds
[0m13:59:36.632443 [debug] [Thread-3 (]: SQL status: COMMIT in 0.004 seconds
[0m13:59:36.632867 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_couriers"} */
alter table "delivery_analytics"."analytics"."stg_couriers__dbt_tmp" rename to "stg_couriers"
[0m13:59:36.635217 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_shipments: Close
[0m13:59:36.637266 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."stg_shipment_status__dbt_backup"
[0m13:59:36.638292 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipment_status"
[0m13:59:36.638858 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:59:36.639305 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipment_status"} */
drop view if exists "delivery_analytics"."analytics"."stg_shipment_status__dbt_backup" cascade
[0m13:59:36.642518 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_couriers: COMMIT
[0m13:59:36.643010 [debug] [Thread-1 (]: SQL status: BEGIN in 0.137 seconds
[0m13:59:36.643595 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m13:59:36.644090 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.001 seconds
[0m13:59:36.644516 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_courier_assigments"
[0m13:59:36.644930 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_couriers: COMMIT
[0m13:59:36.646341 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipment_status: Close
[0m13:59:36.646797 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_courier_assigments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_courier_assigments"} */

  create view "delivery_analytics"."analytics"."stg_courier_assigments__dbt_tmp"
    
    
  as (
    with source as (
    select *
    from "delivery_analytics"."raw"."courier_assignments"
)

select
    assignment_id,
    courier_id,
    shipment_id,
    assigned_at,
    delivered_at
from source
  );
[0m13:59:36.647258 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e216441e-60d6-468b-88cc-51556d6cf323', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD298DC350>]}
[0m13:59:36.648206 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e216441e-60d6-468b-88cc-51556d6cf323', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD29542D00>]}
[0m13:59:36.649582 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m13:59:36.649081 [info ] [Thread-4 (]: 4 of 5 OK created sql view model analytics.stg_shipments ....................... [[32mCREATE VIEW[0m in 0.22s]
[0m13:59:36.653792 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."stg_couriers__dbt_backup"
[0m13:59:36.650474 [info ] [Thread-3 (]: 3 of 5 OK created sql view model analytics.stg_shipment_status ................. [[32mCREATE VIEW[0m in 0.21s]
[0m13:59:36.654844 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.stg_shipments
[0m13:59:36.655274 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.007 seconds
[0m13:59:36.656075 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m13:59:36.656699 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.stg_shipment_status
[0m13:59:36.657198 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.stg_users
[0m13:59:36.660325 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_courier_assigments"
[0m13:59:36.660801 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_couriers"} */
drop view if exists "delivery_analytics"."analytics"."stg_couriers__dbt_backup" cascade
[0m13:59:36.662112 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_courier_assigments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_courier_assigments"} */
alter table "delivery_analytics"."analytics"."stg_courier_assigments__dbt_tmp" rename to "stg_courier_assigments"
[0m13:59:36.661700 [info ] [Thread-4 (]: 5 of 5 START sql view model analytics.stg_users ................................ [RUN]
[0m13:59:36.662974 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.stg_shipments, now model.dbt_delivery_analytics.stg_users)
[0m13:59:36.663494 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.stg_users
[0m13:59:36.664003 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:59:36.664418 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.002 seconds
[0m13:59:36.667718 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_users"
[0m13:59:36.669451 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_courier_assigments: COMMIT
[0m13:59:36.670999 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_couriers: Close
[0m13:59:36.671594 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_courier_assigments"
[0m13:59:36.672117 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_courier_assigments: COMMIT
[0m13:59:36.672651 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e216441e-60d6-468b-88cc-51556d6cf323', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD298ECEB0>]}
[0m13:59:36.674211 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m13:59:36.674850 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.stg_users
[0m13:59:36.673633 [info ] [Thread-2 (]: 2 of 5 OK created sql view model analytics.stg_couriers ........................ [[32mCREATE VIEW[0m in 0.25s]
[0m13:59:36.678087 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."stg_courier_assigments__dbt_backup"
[0m13:59:36.681748 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_users"
[0m13:59:36.682503 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.stg_couriers
[0m13:59:36.683286 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_courier_assigments"
[0m13:59:36.684064 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_courier_assigments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_courier_assigments"} */
drop view if exists "delivery_analytics"."analytics"."stg_courier_assigments__dbt_backup" cascade
[0m13:59:36.684947 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.000 seconds
[0m13:59:36.686428 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_courier_assigments: Close
[0m13:59:36.686873 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_users"
[0m13:59:36.687387 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_users: BEGIN
[0m13:59:36.688055 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e216441e-60d6-468b-88cc-51556d6cf323', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD294385D0>]}
[0m13:59:36.688590 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:59:36.689443 [info ] [Thread-1 (]: 1 of 5 OK created sql view model analytics.stg_courier_assigments .............. [[32mCREATE VIEW[0m in 0.26s]
[0m13:59:36.690512 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.stg_courier_assigments
[0m13:59:36.761152 [debug] [Thread-4 (]: SQL status: BEGIN in 0.072 seconds
[0m13:59:36.761709 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_users"
[0m13:59:36.762113 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_users: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_users"} */

  create view "delivery_analytics"."analytics"."stg_users__dbt_tmp"
    
    
  as (
    with source as (
    select *
    from "delivery_analytics"."raw"."users"
)

select
    user_id,
    username,
    email,
    role,
    created_at
from source
  );
[0m13:59:36.768100 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.006 seconds
[0m13:59:36.771127 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_users"
[0m13:59:36.771525 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_users: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_users"} */
alter table "delivery_analytics"."analytics"."stg_users__dbt_tmp" rename to "stg_users"
[0m13:59:36.772381 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m13:59:36.773750 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_users: COMMIT
[0m13:59:36.774116 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_users"
[0m13:59:36.774450 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_users: COMMIT
[0m13:59:36.775806 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m13:59:36.778343 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."stg_users__dbt_backup"
[0m13:59:36.778975 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.stg_users"
[0m13:59:36.779312 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_users: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_users"} */
drop view if exists "delivery_analytics"."analytics"."stg_users__dbt_backup" cascade
[0m13:59:36.779931 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.000 seconds
[0m13:59:36.781400 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.stg_users: Close
[0m13:59:36.782136 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e216441e-60d6-468b-88cc-51556d6cf323', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD298F6120>]}
[0m13:59:36.782732 [info ] [Thread-4 (]: 5 of 5 OK created sql view model analytics.stg_users ........................... [[32mCREATE VIEW[0m in 0.12s]
[0m13:59:36.783529 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.stg_users
[0m13:59:36.785245 [debug] [MainThread]: Using postgres connection "master"
[0m13:59:36.785603 [debug] [MainThread]: On master: BEGIN
[0m13:59:36.785916 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:59:36.857565 [debug] [MainThread]: SQL status: BEGIN in 0.072 seconds
[0m13:59:36.858101 [debug] [MainThread]: On master: COMMIT
[0m13:59:36.858524 [debug] [MainThread]: Using postgres connection "master"
[0m13:59:36.858818 [debug] [MainThread]: On master: COMMIT
[0m13:59:36.859256 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:59:36.859575 [debug] [MainThread]: On master: Close
[0m13:59:36.860066 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:59:36.860376 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m13:59:36.860736 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m13:59:36.861090 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_users' was properly closed.
[0m13:59:36.861425 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_couriers' was properly closed.
[0m13:59:36.861785 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_courier_assigments' was properly closed.
[0m13:59:36.862151 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_shipment_status' was properly closed.
[0m13:59:36.862666 [info ] [MainThread]: 
[0m13:59:36.863280 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 0.99 seconds (0.99s).
[0m13:59:36.865137 [debug] [MainThread]: Command end result
[0m13:59:36.901986 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:59:36.906009 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:59:36.913925 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m13:59:36.914326 [info ] [MainThread]: 
[0m13:59:36.914892 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:59:36.915370 [info ] [MainThread]: 
[0m13:59:36.915894 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m13:59:36.917244 [debug] [MainThread]: Command `dbt run` succeeded at 13:59:36.917126 after 2.61 seconds
[0m13:59:36.917604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD26CCEB70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD28FCB590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CD28FC9BB0>]}
[0m13:59:36.917971 [debug] [MainThread]: Flushing usage events
[0m13:59:38.083843 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:14:09.307235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023591C60980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002359271CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023592D57D90>]}


============================== 14:14:09.313804 | c2798855-b4cc-4265-9773-0ff7141fbe74 ==============================
[0m14:14:09.313804 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:14:09.314779 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run --select marts.core', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:14:09.614750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c2798855-b4cc-4265-9773-0ff7141fbe74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235920C8770>]}
[0m14:14:09.697181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c2798855-b4cc-4265-9773-0ff7141fbe74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023592BFEE00>]}
[0m14:14:09.699030 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:14:10.312062 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:14:10.515198 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m14:14:10.516146 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\core\dim_dates.sql
[0m14:14:10.516708 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\core\dim_couriers.sql
[0m14:14:10.517203 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\core\dim_users.sql
[0m14:14:10.867816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c2798855-b4cc-4265-9773-0ff7141fbe74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235946D1650>]}
[0m14:14:10.965364 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:14:10.969719 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:14:11.011366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c2798855-b4cc-4265-9773-0ff7141fbe74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023592BEFD40>]}
[0m14:14:11.012039 [info ] [MainThread]: Found 11 models, 5 sources, 459 macros
[0m14:14:11.012703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c2798855-b4cc-4265-9773-0ff7141fbe74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023594816270>]}
[0m14:14:11.014859 [info ] [MainThread]: 
[0m14:14:11.015509 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:14:11.016165 [info ] [MainThread]: 
[0m14:14:11.017223 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:14:11.021750 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m14:14:11.259468 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m14:14:11.259920 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m14:14:11.260304 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:14:11.378067 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.118 seconds
[0m14:14:11.379427 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m14:14:11.382501 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m14:14:11.391921 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:14:11.392693 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m14:14:11.393082 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:14:11.453570 [debug] [ThreadPool]: SQL status: BEGIN in 0.060 seconds
[0m14:14:11.454023 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:14:11.454330 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m14:14:11.461917 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.007 seconds
[0m14:14:11.463362 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m14:14:11.463938 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m14:14:11.470888 [debug] [MainThread]: Using postgres connection "master"
[0m14:14:11.471356 [debug] [MainThread]: On master: BEGIN
[0m14:14:11.471628 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:14:11.532114 [debug] [MainThread]: SQL status: BEGIN in 0.060 seconds
[0m14:14:11.532566 [debug] [MainThread]: Using postgres connection "master"
[0m14:14:11.532930 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:14:11.540167 [debug] [MainThread]: SQL status: SELECT 5 in 0.007 seconds
[0m14:14:11.541492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c2798855-b4cc-4265-9773-0ff7141fbe74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023594886EA0>]}
[0m14:14:11.541952 [debug] [MainThread]: On master: ROLLBACK
[0m14:14:11.542637 [debug] [MainThread]: Using postgres connection "master"
[0m14:14:11.542950 [debug] [MainThread]: On master: BEGIN
[0m14:14:11.543689 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m14:14:11.544001 [debug] [MainThread]: On master: COMMIT
[0m14:14:11.544284 [debug] [MainThread]: Using postgres connection "master"
[0m14:14:11.544627 [debug] [MainThread]: On master: COMMIT
[0m14:14:11.545104 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:14:11.545405 [debug] [MainThread]: On master: Close
[0m14:14:11.550961 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.dim_couriers
[0m14:14:11.551427 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.dim_users
[0m14:14:11.551849 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.dim_dates
[0m14:14:11.552374 [info ] [Thread-1 (]: 1 of 3 START sql table model analytics.dim_couriers ............................ [RUN]
[0m14:14:11.554050 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.dim_couriers'
[0m14:14:11.552884 [info ] [Thread-3 (]: 3 of 3 START sql table model analytics.dim_users ............................... [RUN]
[0m14:14:11.554612 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.dim_couriers
[0m14:14:11.555246 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.dim_users'
[0m14:14:11.553452 [info ] [Thread-2 (]: 2 of 3 START sql table model analytics.dim_dates ............................... [RUN]
[0m14:14:11.563832 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.dim_couriers"
[0m14:14:11.564336 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.dim_users
[0m14:14:11.565000 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.dim_dates'
[0m14:14:11.567766 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.dim_users"
[0m14:14:11.568247 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.dim_dates
[0m14:14:11.570959 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.dim_dates"
[0m14:14:11.574338 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.dim_couriers
[0m14:14:11.574933 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.dim_users
[0m14:14:11.575393 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.dim_dates
[0m14:14:11.621201 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.dim_couriers"
[0m14:14:11.626156 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.dim_users"
[0m14:14:11.629543 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.dim_dates"
[0m14:14:11.633578 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.dim_couriers"
[0m14:14:11.634083 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.dim_users"
[0m14:14:11.634450 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.dim_dates"
[0m14:14:11.634870 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.dim_couriers: BEGIN
[0m14:14:11.635400 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.dim_users: BEGIN
[0m14:14:11.635875 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.dim_dates: BEGIN
[0m14:14:11.636317 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:14:11.636834 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:14:11.637313 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:14:11.705029 [debug] [Thread-3 (]: SQL status: BEGIN in 0.068 seconds
[0m14:14:11.705664 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.dim_users"
[0m14:14:11.706187 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_users"} */

  
    

  create  table "delivery_analytics"."analytics"."dim_users__dbt_tmp"
  
  
    as
  
  (
    with users as (
    select *
    from "delivery_analytics"."analytics"."stg_users"
)

select
    user_id,
    username,
    email,
    role,
    created_at
from users
  );
  
[0m14:14:11.713794 [debug] [Thread-3 (]: SQL status: SELECT 8 in 0.007 seconds
[0m14:14:11.723440 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.dim_users"
[0m14:14:11.723933 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_users"} */
alter table "delivery_analytics"."analytics"."dim_users__dbt_tmp" rename to "dim_users"
[0m14:14:11.724605 [debug] [Thread-1 (]: SQL status: BEGIN in 0.088 seconds
[0m14:14:11.725021 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.dim_couriers"
[0m14:14:11.725416 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:14:11.725863 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.dim_couriers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_couriers"} */

  
    

  create  table "delivery_analytics"."analytics"."dim_couriers__dbt_tmp"
  
  
    as
  
  (
    with couriers as (
    select *
    from "delivery_analytics"."analytics"."stg_couriers"
),

users as (
    select user_id, username
    from "delivery_analytics"."analytics"."stg_users"
)

select
    c.courier_id,
    c.user_id,
    u.username,
    c.vehicle_type,
    c.city
from couriers c
left join users u
    on c.user_id = u.user_id
  );
  
[0m14:14:11.740057 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.dim_users: COMMIT
[0m14:14:11.740794 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.dim_users"
[0m14:14:11.741354 [debug] [Thread-2 (]: SQL status: BEGIN in 0.104 seconds
[0m14:14:11.741771 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.dim_users: COMMIT
[0m14:14:11.742211 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.dim_dates"
[0m14:14:11.742706 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.dim_dates: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_dates"} */

  
    

  create  table "delivery_analytics"."analytics"."dim_dates__dbt_tmp"
  
  
    as
  
  (
    with users as (
    select *
    from "delivery_analytics"."analytics"."stg_users"
)

select
    user_id,
    username,
    email,
    role,
    created_at
from users
  );
  
[0m14:14:11.743507 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m14:14:11.749404 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."dim_users__dbt_backup"
[0m14:14:11.749937 [debug] [Thread-2 (]: SQL status: SELECT 8 in 0.007 seconds
[0m14:14:11.750372 [debug] [Thread-1 (]: SQL status: SELECT 3 in 0.010 seconds
[0m14:14:11.755249 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.dim_users"
[0m14:14:11.758398 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.dim_dates"
[0m14:14:11.761641 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.dim_couriers"
[0m14:14:11.762140 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.dim_users: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_users"} */
drop table if exists "delivery_analytics"."analytics"."dim_users__dbt_backup" cascade
[0m14:14:11.762639 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.dim_dates: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_dates"} */
alter table "delivery_analytics"."analytics"."dim_dates__dbt_tmp" rename to "dim_dates"
[0m14:14:11.763080 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.dim_couriers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_couriers"} */
alter table "delivery_analytics"."analytics"."dim_couriers__dbt_tmp" rename to "dim_couriers"
[0m14:14:11.764309 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.001 seconds
[0m14:14:11.764724 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:14:11.765100 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m14:14:11.767411 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.dim_users: Close
[0m14:14:11.768712 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.dim_couriers: COMMIT
[0m14:14:11.769917 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.dim_dates: COMMIT
[0m14:14:11.770408 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.dim_couriers"
[0m14:14:11.772016 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.dim_dates"
[0m14:14:11.772468 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.dim_couriers: COMMIT
[0m14:14:11.772956 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.dim_dates: COMMIT
[0m14:14:11.773662 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c2798855-b4cc-4265-9773-0ff7141fbe74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023594847F50>]}
[0m14:14:11.774780 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m14:14:11.775179 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m14:14:11.774426 [info ] [Thread-3 (]: 3 of 3 OK created sql table model analytics.dim_users .......................... [[32mSELECT 8[0m in 0.22s]
[0m14:14:11.777834 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."dim_couriers__dbt_backup"
[0m14:14:11.780205 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."dim_dates__dbt_backup"
[0m14:14:11.781486 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.dim_users
[0m14:14:11.782373 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.dim_couriers"
[0m14:14:11.783167 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.dim_dates"
[0m14:14:11.783877 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.dim_couriers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_couriers"} */
drop table if exists "delivery_analytics"."analytics"."dim_couriers__dbt_backup" cascade
[0m14:14:11.784356 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.dim_dates: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_dates"} */
drop table if exists "delivery_analytics"."analytics"."dim_dates__dbt_backup" cascade
[0m14:14:11.785308 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.000 seconds
[0m14:14:11.785696 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m14:14:11.787056 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.dim_couriers: Close
[0m14:14:11.788279 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.dim_dates: Close
[0m14:14:11.789024 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c2798855-b4cc-4265-9773-0ff7141fbe74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235948D7330>]}
[0m14:14:11.789541 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c2798855-b4cc-4265-9773-0ff7141fbe74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023594315DB0>]}
[0m14:14:11.790189 [info ] [Thread-1 (]: 1 of 3 OK created sql table model analytics.dim_couriers ....................... [[32mSELECT 3[0m in 0.24s]
[0m14:14:11.791740 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.dim_couriers
[0m14:14:11.790768 [info ] [Thread-2 (]: 2 of 3 OK created sql table model analytics.dim_dates .......................... [[32mSELECT 8[0m in 0.22s]
[0m14:14:11.792909 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.dim_dates
[0m14:14:11.794790 [debug] [MainThread]: Using postgres connection "master"
[0m14:14:11.795245 [debug] [MainThread]: On master: BEGIN
[0m14:14:11.795558 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:14:11.865350 [debug] [MainThread]: SQL status: BEGIN in 0.070 seconds
[0m14:14:11.865804 [debug] [MainThread]: On master: COMMIT
[0m14:14:11.866100 [debug] [MainThread]: Using postgres connection "master"
[0m14:14:11.866400 [debug] [MainThread]: On master: COMMIT
[0m14:14:11.866899 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:14:11.867228 [debug] [MainThread]: On master: Close
[0m14:14:11.867739 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:14:11.868028 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m14:14:11.868274 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m14:14:11.868606 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.dim_couriers' was properly closed.
[0m14:14:11.868849 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.dim_users' was properly closed.
[0m14:14:11.869082 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.dim_dates' was properly closed.
[0m14:14:11.869455 [info ] [MainThread]: 
[0m14:14:11.870020 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 0.85 seconds (0.85s).
[0m14:14:11.871470 [debug] [MainThread]: Command end result
[0m14:14:11.897867 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:14:11.901409 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:14:11.908560 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m14:14:11.908988 [info ] [MainThread]: 
[0m14:14:11.909573 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:14:11.910111 [info ] [MainThread]: 
[0m14:14:11.910677 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m14:14:11.912071 [debug] [MainThread]: Command `dbt run` succeeded at 14:14:11.911955 after 2.83 seconds
[0m14:14:11.912558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002359488CBD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023594BFD150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023592C6FA80>]}
[0m14:14:11.913016 [debug] [MainThread]: Flushing usage events
[0m14:14:12.956616 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:17:12.889210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E99E0F0980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E99EB9CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E99F1E7D90>]}


============================== 14:17:12.895295 | 4ec62e9b-f819-443c-9d87-0993581562d3 ==============================
[0m14:17:12.895295 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:17:12.896598 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run --select marts.analytics', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:17:13.188851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4ec62e9b-f819-443c-9d87-0993581562d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E99E558770>]}
[0m14:17:13.271859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4ec62e9b-f819-443c-9d87-0993581562d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E99F08EE00>]}
[0m14:17:13.273900 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:17:13.769554 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:17:13.933824 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m14:17:13.934670 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\daily_shipments_status.sql
[0m14:17:13.935199 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\fct_shipments.sql
[0m14:17:13.935697 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\courier_performance.sql
[0m14:17:14.220058 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4ec62e9b-f819-443c-9d87-0993581562d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9A0B05950>]}
[0m14:17:14.299455 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:17:14.303712 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:17:14.341638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4ec62e9b-f819-443c-9d87-0993581562d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E99F07FD40>]}
[0m14:17:14.342777 [info ] [MainThread]: Found 11 models, 5 sources, 459 macros
[0m14:17:14.343482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4ec62e9b-f819-443c-9d87-0993581562d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9A0C465F0>]}
[0m14:17:14.345658 [info ] [MainThread]: 
[0m14:17:14.346463 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:17:14.347008 [info ] [MainThread]: 
[0m14:17:14.347735 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:17:14.351828 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m14:17:14.562179 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m14:17:14.562607 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m14:17:14.562928 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:17:14.702454 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.139 seconds
[0m14:17:14.703918 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m14:17:14.706192 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m14:17:14.712382 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:17:14.712857 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m14:17:14.713192 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:17:14.777134 [debug] [ThreadPool]: SQL status: BEGIN in 0.064 seconds
[0m14:17:14.777594 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:17:14.777909 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m14:17:14.784271 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.006 seconds
[0m14:17:14.786262 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m14:17:14.786953 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m14:17:14.794207 [debug] [MainThread]: Using postgres connection "master"
[0m14:17:14.794624 [debug] [MainThread]: On master: BEGIN
[0m14:17:14.794919 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:17:14.853105 [debug] [MainThread]: SQL status: BEGIN in 0.058 seconds
[0m14:17:14.853603 [debug] [MainThread]: Using postgres connection "master"
[0m14:17:14.853949 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:17:14.860567 [debug] [MainThread]: SQL status: SELECT 5 in 0.006 seconds
[0m14:17:14.862482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4ec62e9b-f819-443c-9d87-0993581562d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9A0EB2F70>]}
[0m14:17:14.862924 [debug] [MainThread]: On master: ROLLBACK
[0m14:17:14.863482 [debug] [MainThread]: Using postgres connection "master"
[0m14:17:14.863853 [debug] [MainThread]: On master: BEGIN
[0m14:17:14.864641 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m14:17:14.864954 [debug] [MainThread]: On master: COMMIT
[0m14:17:14.865239 [debug] [MainThread]: Using postgres connection "master"
[0m14:17:14.865510 [debug] [MainThread]: On master: COMMIT
[0m14:17:14.866000 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:17:14.866321 [debug] [MainThread]: On master: Close
[0m14:17:14.870822 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m14:17:14.871644 [info ] [Thread-1 (]: 1 of 3 START sql table model analytics.fct_shipments ........................... [RUN]
[0m14:17:14.872548 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m14:17:14.872919 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m14:17:14.880775 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:17:14.883409 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m14:17:14.923658 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:17:14.926318 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:17:14.927207 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m14:17:14.927795 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:17:14.987217 [debug] [Thread-1 (]: SQL status: BEGIN in 0.059 seconds
[0m14:17:14.987779 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:17:14.988166 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."analytics"."stg_shipments"
),

shipment_status as (
    select *
    from "delivery_analytics"."analytics"."stg_shipment_status"
),

users as (
    select *
    from "delivery_analytics"."analytics"."dim_users"
),

couriers as (
    select *
    from "delivery_analytics"."analytics"."dim_couriers"
)

select
    s.shipment_id,
    s.sender_id,
    su.username as sender_name,
    s.receiver_id,
    ru.username as receiver_name,
    s.shipment_date,
    s.expected_delivery_date,
    ss.status,
    ss.status_timestamp,
    ss.courier_id,
    c.user_id as courier_user_id,
    c.vehicle_type,
    s.price
from shipments s
left join shipment_status ss on s.shipment_id = ss.shipment_id
left join users su on s.sender_id = su.user_id
left join users ru on s.receiver_id = ru.user_id
left join couriers c on ss.courier_id = c.courier_id
  );
  
[0m14:17:15.105015 [debug] [Thread-1 (]: SQL status: SELECT 10 in 0.116 seconds
[0m14:17:15.115845 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:17:15.116494 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m14:17:15.117636 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:17:15.131712 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:17:15.132233 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:17:15.132663 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:17:15.133959 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m14:17:15.139547 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m14:17:15.144180 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:17:15.144553 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m14:17:15.145296 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.000 seconds
[0m14:17:15.147827 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m14:17:15.150436 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ec62e9b-f819-443c-9d87-0993581562d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9A0A73590>]}
[0m14:17:15.151252 [info ] [Thread-1 (]: 1 of 3 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 10[0m in 0.28s]
[0m14:17:15.152387 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m14:17:15.153376 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m14:17:15.154398 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:17:15.154006 [info ] [Thread-2 (]: 2 of 3 START sql table model analytics.courier_performance ..................... [RUN]
[0m14:17:15.155808 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m14:17:15.155168 [info ] [Thread-3 (]: 3 of 3 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m14:17:15.156458 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m14:17:15.157009 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m14:17:15.160048 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m14:17:15.160511 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m14:17:15.163087 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:17:15.169494 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m14:17:15.173799 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:17:15.174690 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m14:17:15.181448 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m14:17:15.183529 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:17:15.184250 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m14:17:15.184780 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:17:15.187244 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:17:15.187782 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m14:17:15.188269 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:17:15.252587 [debug] [Thread-3 (]: SQL status: BEGIN in 0.068 seconds
[0m14:17:15.253150 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:17:15.253519 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m14:17:15.263105 [debug] [Thread-3 (]: SQL status: SELECT 9 in 0.009 seconds
[0m14:17:15.266466 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:17:15.267063 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m14:17:15.267719 [debug] [Thread-2 (]: SQL status: BEGIN in 0.079 seconds
[0m14:17:15.268266 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:17:15.268716 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:17:15.269203 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when status = 'delivered' then 1 else 0 end) as delivered_shipments,
    avg(s.expected_delivery_date - s.shipment_date) as avg_delivery_time
from fct
group by courier_id
  );
  
[0m14:17:15.270604 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:17:15.271245 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:17:15.271752 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:17:15.273287 [debug] [Thread-2 (]: Postgres adapter: Postgres error: missing FROM-clause entry for table "s"
LINE 21:     avg(s.expected_delivery_date - s.shipment_date) as avg_d...
                 ^

[0m14:17:15.273838 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m14:17:15.274426 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m14:17:15.278144 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m14:17:15.279020 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m14:17:15.279998 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:17:15.280550 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m14:17:15.282702 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.000 seconds
[0m14:17:15.284347 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m14:17:15.285293 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ec62e9b-f819-443c-9d87-0993581562d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9A0D176A0>]}
[0m14:17:15.286194 [info ] [Thread-3 (]: 3 of 3 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.13s]
[0m14:17:15.287497 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:17:15.296618 [debug] [Thread-2 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  missing FROM-clause entry for table "s"
  LINE 21:     avg(s.expected_delivery_date - s.shipment_date) as avg_d...
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:17:15.297352 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ec62e9b-f819-443c-9d87-0993581562d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9A0777110>]}
[0m14:17:15.298069 [error] [Thread-2 (]: 2 of 3 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.14s]
[0m14:17:15.298988 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m14:17:15.299848 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  missing FROM-clause entry for table "s"
  LINE 21:     avg(s.expected_delivery_date - s.shipment_date) as avg_d...
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m14:17:15.302537 [debug] [MainThread]: Using postgres connection "master"
[0m14:17:15.303133 [debug] [MainThread]: On master: BEGIN
[0m14:17:15.303520 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:17:15.369396 [debug] [MainThread]: SQL status: BEGIN in 0.066 seconds
[0m14:17:15.369844 [debug] [MainThread]: On master: COMMIT
[0m14:17:15.370139 [debug] [MainThread]: Using postgres connection "master"
[0m14:17:15.370420 [debug] [MainThread]: On master: COMMIT
[0m14:17:15.370841 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:17:15.371219 [debug] [MainThread]: On master: Close
[0m14:17:15.371718 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:17:15.371989 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m14:17:15.372235 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m14:17:15.372539 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments' was properly closed.
[0m14:17:15.372785 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m14:17:15.373022 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m14:17:15.373374 [info ] [MainThread]: 
[0m14:17:15.373918 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 1.03 seconds (1.03s).
[0m14:17:15.375602 [debug] [MainThread]: Command end result
[0m14:17:15.404005 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:17:15.408154 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:17:15.416903 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m14:17:15.417392 [info ] [MainThread]: 
[0m14:17:15.418075 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:17:15.418580 [info ] [MainThread]: 
[0m14:17:15.419344 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m14:17:15.420020 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  missing FROM-clause entry for table "s"
  LINE 21:     avg(s.expected_delivery_date - s.shipment_date) as avg_d...
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:17:15.420577 [info ] [MainThread]: 
[0m14:17:15.421168 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:17:15.421679 [info ] [MainThread]: 
[0m14:17:15.422217 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m14:17:15.423981 [debug] [MainThread]: Command `dbt run` failed at 14:17:15.423830 after 2.84 seconds
[0m14:17:15.424463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9A0CC09D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9A0FBB2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E99F0FFA80>]}
[0m14:17:15.424919 [debug] [MainThread]: Flushing usage events
[0m14:17:16.505531 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:18:41.446939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC4210980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC4CCCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC530BD90>]}


============================== 14:18:41.452995 | 1962fec5-49a5-4d57-8198-7de58a63d7f8 ==============================
[0m14:18:41.452995 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:18:41.454223 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt run --select marts.analytics', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:18:41.734997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1962fec5-49a5-4d57-8198-7de58a63d7f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC4678770>]}
[0m14:18:41.817168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1962fec5-49a5-4d57-8198-7de58a63d7f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC51AEE00>]}
[0m14:18:41.818966 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:18:42.412622 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:18:42.599636 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:18:42.600849 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\courier_performance.sql
[0m14:18:42.962123 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1962fec5-49a5-4d57-8198-7de58a63d7f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC6C79850>]}
[0m14:18:43.051674 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:18:43.056154 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:18:43.097409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1962fec5-49a5-4d57-8198-7de58a63d7f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC519FD40>]}
[0m14:18:43.098430 [info ] [MainThread]: Found 11 models, 5 sources, 459 macros
[0m14:18:43.099331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1962fec5-49a5-4d57-8198-7de58a63d7f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC6E7E5F0>]}
[0m14:18:43.101713 [info ] [MainThread]: 
[0m14:18:43.102455 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:18:43.103164 [info ] [MainThread]: 
[0m14:18:43.104248 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:18:43.111587 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m14:18:43.379119 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m14:18:43.379660 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m14:18:43.380137 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:18:43.534321 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.154 seconds
[0m14:18:43.536143 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m14:18:43.539529 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m14:18:43.547534 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:18:43.548077 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m14:18:43.548444 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:18:43.621391 [debug] [ThreadPool]: SQL status: BEGIN in 0.073 seconds
[0m14:18:43.621950 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:18:43.622373 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m14:18:43.629895 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.007 seconds
[0m14:18:43.632126 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m14:18:43.632858 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m14:18:43.642255 [debug] [MainThread]: Using postgres connection "master"
[0m14:18:43.642764 [debug] [MainThread]: On master: BEGIN
[0m14:18:43.643125 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:18:43.716593 [debug] [MainThread]: SQL status: BEGIN in 0.073 seconds
[0m14:18:43.717151 [debug] [MainThread]: Using postgres connection "master"
[0m14:18:43.717623 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:18:43.727649 [debug] [MainThread]: SQL status: SELECT 5 in 0.009 seconds
[0m14:18:43.730364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1962fec5-49a5-4d57-8198-7de58a63d7f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC7013040>]}
[0m14:18:43.730975 [debug] [MainThread]: On master: ROLLBACK
[0m14:18:43.731760 [debug] [MainThread]: Using postgres connection "master"
[0m14:18:43.732296 [debug] [MainThread]: On master: BEGIN
[0m14:18:43.733277 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m14:18:43.733692 [debug] [MainThread]: On master: COMMIT
[0m14:18:43.734108 [debug] [MainThread]: Using postgres connection "master"
[0m14:18:43.734466 [debug] [MainThread]: On master: COMMIT
[0m14:18:43.735117 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:18:43.735538 [debug] [MainThread]: On master: Close
[0m14:18:43.742116 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m14:18:43.743198 [info ] [Thread-1 (]: 1 of 3 START sql table model analytics.fct_shipments ........................... [RUN]
[0m14:18:43.744193 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m14:18:43.744685 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m14:18:43.754159 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:18:43.756503 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m14:18:43.808203 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:18:43.810544 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:18:43.811185 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m14:18:43.811993 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:18:43.887796 [debug] [Thread-1 (]: SQL status: BEGIN in 0.076 seconds
[0m14:18:43.888452 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:18:43.888934 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."analytics"."stg_shipments"
),

shipment_status as (
    select *
    from "delivery_analytics"."analytics"."stg_shipment_status"
),

users as (
    select *
    from "delivery_analytics"."analytics"."dim_users"
),

couriers as (
    select *
    from "delivery_analytics"."analytics"."dim_couriers"
)

select
    s.shipment_id,
    s.sender_id,
    su.username as sender_name,
    s.receiver_id,
    ru.username as receiver_name,
    s.shipment_date,
    s.expected_delivery_date,
    ss.status,
    ss.status_timestamp,
    ss.courier_id,
    c.user_id as courier_user_id,
    c.vehicle_type,
    s.price
from shipments s
left join shipment_status ss on s.shipment_id = ss.shipment_id
left join users su on s.sender_id = su.user_id
left join users ru on s.receiver_id = ru.user_id
left join couriers c on ss.courier_id = c.courier_id
  );
  
[0m14:18:43.899734 [debug] [Thread-1 (]: SQL status: SELECT 10 in 0.010 seconds
[0m14:18:43.912996 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:18:43.913518 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m14:18:43.915088 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:18:43.918899 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:18:43.919398 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m14:18:43.920949 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:18:43.941487 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:18:43.942035 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:18:43.942486 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:18:43.944414 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m14:18:43.953776 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m14:18:43.961215 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:18:43.962600 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m14:18:43.969932 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.007 seconds
[0m14:18:43.973817 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m14:18:43.976723 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1962fec5-49a5-4d57-8198-7de58a63d7f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC6BD3890>]}
[0m14:18:43.977590 [info ] [Thread-1 (]: 1 of 3 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 10[0m in 0.23s]
[0m14:18:43.979216 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m14:18:43.980493 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m14:18:43.981128 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:18:43.981857 [info ] [Thread-2 (]: 2 of 3 START sql table model analytics.courier_performance ..................... [RUN]
[0m14:18:43.983463 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m14:18:43.984008 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m14:18:43.982600 [info ] [Thread-3 (]: 3 of 3 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m14:18:43.987726 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m14:18:43.988593 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m14:18:43.989523 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m14:18:43.994677 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:18:43.995882 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m14:18:44.000752 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m14:18:44.002280 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m14:18:44.006367 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:18:44.008110 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:18:44.008953 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m14:18:44.009810 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:18:44.011079 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:18:44.011615 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m14:18:44.012121 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:18:44.093192 [debug] [Thread-2 (]: SQL status: BEGIN in 0.083 seconds
[0m14:18:44.094110 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:18:44.094872 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when status = 'delivered' then 1 else 0 end) as delivered_shipments,
    AVG(EXTRACT(EPOCH FROM (s.expected_delivery_date - s.shipment_date)) / 86400) AS avg_delivery_time
from fct
group by courier_id
  );
  
[0m14:18:44.098050 [debug] [Thread-2 (]: Postgres adapter: Postgres error: missing FROM-clause entry for table "s"
LINE 21:     AVG(EXTRACT(EPOCH FROM (s.expected_delivery_date - s.shi...
                                     ^

[0m14:18:44.098687 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m14:18:44.099662 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m14:18:44.114925 [debug] [Thread-2 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  missing FROM-clause entry for table "s"
  LINE 21:     AVG(EXTRACT(EPOCH FROM (s.expected_delivery_date - s.shi...
                                       ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:18:44.116047 [debug] [Thread-3 (]: SQL status: BEGIN in 0.104 seconds
[0m14:18:44.116973 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1962fec5-49a5-4d57-8198-7de58a63d7f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC6E6BD80>]}
[0m14:18:44.117629 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:18:44.119463 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m14:18:44.118744 [error] [Thread-2 (]: 2 of 3 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.13s]
[0m14:18:44.120533 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m14:18:44.121288 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  missing FROM-clause entry for table "s"
  LINE 21:     AVG(EXTRACT(EPOCH FROM (s.expected_delivery_date - s.shi...
                                       ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m14:18:44.129853 [debug] [Thread-3 (]: SQL status: SELECT 9 in 0.010 seconds
[0m14:18:44.134870 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:18:44.135450 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m14:18:44.136766 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:18:44.140203 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:18:44.140739 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m14:18:44.142329 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:18:44.144336 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:18:44.144846 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:18:44.145287 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:18:44.147198 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m14:18:44.150561 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m14:18:44.151477 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:18:44.151943 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m14:18:44.157011 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.005 seconds
[0m14:18:44.159119 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m14:18:44.159914 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1962fec5-49a5-4d57-8198-7de58a63d7f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC68A02D0>]}
[0m14:18:44.160874 [info ] [Thread-3 (]: 3 of 3 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.17s]
[0m14:18:44.162279 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:18:44.164523 [debug] [MainThread]: Using postgres connection "master"
[0m14:18:44.165017 [debug] [MainThread]: On master: BEGIN
[0m14:18:44.165418 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:18:44.240058 [debug] [MainThread]: SQL status: BEGIN in 0.074 seconds
[0m14:18:44.240739 [debug] [MainThread]: On master: COMMIT
[0m14:18:44.241217 [debug] [MainThread]: Using postgres connection "master"
[0m14:18:44.241603 [debug] [MainThread]: On master: COMMIT
[0m14:18:44.242274 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:18:44.242688 [debug] [MainThread]: On master: Close
[0m14:18:44.243326 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:18:44.243692 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m14:18:44.244041 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m14:18:44.244494 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments' was properly closed.
[0m14:18:44.244838 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m14:18:44.245183 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m14:18:44.245653 [info ] [MainThread]: 
[0m14:18:44.246405 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 1.14 seconds (1.14s).
[0m14:18:44.248064 [debug] [MainThread]: Command end result
[0m14:18:44.279736 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:18:44.284443 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:18:44.293033 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m14:18:44.293574 [info ] [MainThread]: 
[0m14:18:44.294331 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:18:44.295181 [info ] [MainThread]: 
[0m14:18:44.295917 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m14:18:44.296705 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  missing FROM-clause entry for table "s"
  LINE 21:     AVG(EXTRACT(EPOCH FROM (s.expected_delivery_date - s.shi...
                                       ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:18:44.297390 [info ] [MainThread]: 
[0m14:18:44.298109 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:18:44.298799 [info ] [MainThread]: 
[0m14:18:44.299669 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m14:18:44.301634 [debug] [MainThread]: Command `dbt run` failed at 14:18:44.301479 after 3.09 seconds
[0m14:18:44.302206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC6E28950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC6E30AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DCC521FA80>]}
[0m14:18:44.302660 [debug] [MainThread]: Flushing usage events
[0m14:18:45.344282 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:19:19.612296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215443F0980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021544E9CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215454EBD90>]}


============================== 14:19:19.618293 | bc10e861-d968-4a1c-801a-5442778b6c22 ==============================
[0m14:19:19.618293 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:19:19.620032 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run --select marts.analytics', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:19:19.920560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bc10e861-d968-4a1c-801a-5442778b6c22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021544858770>]}
[0m14:19:20.006230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bc10e861-d968-4a1c-801a-5442778b6c22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002154538EE00>]}
[0m14:19:20.008020 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:19:20.633002 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:19:20.832521 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:19:20.833333 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\courier_performance.sql
[0m14:19:21.191065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bc10e861-d968-4a1c-801a-5442778b6c22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021546E31850>]}
[0m14:19:21.284569 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:19:21.289722 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:19:21.332228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bc10e861-d968-4a1c-801a-5442778b6c22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002154537FD40>]}
[0m14:19:21.332945 [info ] [MainThread]: Found 11 models, 5 sources, 459 macros
[0m14:19:21.333746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc10e861-d968-4a1c-801a-5442778b6c22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021546FA25F0>]}
[0m14:19:21.336740 [info ] [MainThread]: 
[0m14:19:21.337495 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:19:21.338134 [info ] [MainThread]: 
[0m14:19:21.339054 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:19:21.344666 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m14:19:21.657382 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m14:19:21.657947 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m14:19:21.658401 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:19:21.797991 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.139 seconds
[0m14:19:21.799567 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m14:19:21.802100 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m14:19:21.811648 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:19:21.812313 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m14:19:21.812762 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:19:21.882773 [debug] [ThreadPool]: SQL status: BEGIN in 0.070 seconds
[0m14:19:21.883437 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:19:21.883959 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m14:19:21.891913 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.007 seconds
[0m14:19:21.894206 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m14:19:21.894929 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m14:19:21.904327 [debug] [MainThread]: Using postgres connection "master"
[0m14:19:21.904930 [debug] [MainThread]: On master: BEGIN
[0m14:19:21.905329 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:19:21.996629 [debug] [MainThread]: SQL status: BEGIN in 0.091 seconds
[0m14:19:21.997275 [debug] [MainThread]: Using postgres connection "master"
[0m14:19:21.997845 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:19:22.006473 [debug] [MainThread]: SQL status: SELECT 5 in 0.008 seconds
[0m14:19:22.008681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc10e861-d968-4a1c-801a-5442778b6c22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215471D3040>]}
[0m14:19:22.009301 [debug] [MainThread]: On master: ROLLBACK
[0m14:19:22.009879 [debug] [MainThread]: Using postgres connection "master"
[0m14:19:22.010225 [debug] [MainThread]: On master: BEGIN
[0m14:19:22.010873 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m14:19:22.011198 [debug] [MainThread]: On master: COMMIT
[0m14:19:22.011496 [debug] [MainThread]: Using postgres connection "master"
[0m14:19:22.011786 [debug] [MainThread]: On master: COMMIT
[0m14:19:22.012257 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:19:22.012670 [debug] [MainThread]: On master: Close
[0m14:19:22.018531 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m14:19:22.019384 [info ] [Thread-1 (]: 1 of 3 START sql table model analytics.fct_shipments ........................... [RUN]
[0m14:19:22.020266 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m14:19:22.020897 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m14:19:22.032212 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:19:22.034475 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m14:19:22.087502 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:19:22.089895 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:19:22.090581 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m14:19:22.091181 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:19:22.162508 [debug] [Thread-1 (]: SQL status: BEGIN in 0.071 seconds
[0m14:19:22.163118 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:19:22.163595 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."analytics"."stg_shipments"
),

shipment_status as (
    select *
    from "delivery_analytics"."analytics"."stg_shipment_status"
),

users as (
    select *
    from "delivery_analytics"."analytics"."dim_users"
),

couriers as (
    select *
    from "delivery_analytics"."analytics"."dim_couriers"
)

select
    s.shipment_id,
    s.sender_id,
    su.username as sender_name,
    s.receiver_id,
    ru.username as receiver_name,
    s.shipment_date,
    s.expected_delivery_date,
    ss.status,
    ss.status_timestamp,
    ss.courier_id,
    c.user_id as courier_user_id,
    c.vehicle_type,
    s.price
from shipments s
left join shipment_status ss on s.shipment_id = ss.shipment_id
left join users su on s.sender_id = su.user_id
left join users ru on s.receiver_id = ru.user_id
left join couriers c on ss.courier_id = c.courier_id
  );
  
[0m14:19:22.174832 [debug] [Thread-1 (]: SQL status: SELECT 10 in 0.011 seconds
[0m14:19:22.187022 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:19:22.187616 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m14:19:22.188615 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:19:22.192067 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:19:22.192579 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m14:19:22.193966 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:19:22.212073 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:19:22.212523 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:19:22.212857 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:19:22.214158 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m14:19:22.220018 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m14:19:22.224671 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:19:22.225347 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m14:19:22.229800 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.004 seconds
[0m14:19:22.232268 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m14:19:22.234876 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bc10e861-d968-4a1c-801a-5442778b6c22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021546CEB890>]}
[0m14:19:22.235678 [info ] [Thread-1 (]: 1 of 3 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 10[0m in 0.21s]
[0m14:19:22.237036 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m14:19:22.238323 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m14:19:22.238869 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:19:22.239413 [info ] [Thread-2 (]: 2 of 3 START sql table model analytics.courier_performance ..................... [RUN]
[0m14:19:22.240195 [info ] [Thread-3 (]: 3 of 3 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m14:19:22.241132 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m14:19:22.241775 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m14:19:22.242408 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m14:19:22.242886 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m14:19:22.246657 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m14:19:22.250279 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:19:22.253721 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m14:19:22.254576 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m14:19:22.260139 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:19:22.264468 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m14:19:22.267114 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:19:22.267723 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m14:19:22.268371 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:19:22.268991 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:19:22.269484 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m14:19:22.270220 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:19:22.343675 [debug] [Thread-3 (]: SQL status: BEGIN in 0.075 seconds
[0m14:19:22.344171 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:19:22.344556 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m14:19:22.352986 [debug] [Thread-3 (]: SQL status: SELECT 9 in 0.008 seconds
[0m14:19:22.356981 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:19:22.357520 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m14:19:22.358310 [debug] [Thread-2 (]: SQL status: BEGIN in 0.088 seconds
[0m14:19:22.358740 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:19:22.359097 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:19:22.361580 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:19:22.362007 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when status = 'delivered' then 1 else 0 end) as delivered_shipments,
    AVG(EXTRACT(EPOCH FROM (expected_delivery_date - shipment_date)) / 86400) AS avg_delivery_time
from fct
group by courier_id
  );
  
[0m14:19:22.362412 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m14:19:22.363434 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:19:22.364899 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:19:22.365264 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:19:22.365941 [debug] [Thread-2 (]: Postgres adapter: Postgres error: function pg_catalog.extract(unknown, integer) does not exist
LINE 21:     AVG(EXTRACT(EPOCH FROM (expected_delivery_date - shipmen...
                 ^
HINT:  No function matches the given name and argument types. You might need to add explicit type casts.

[0m14:19:22.366489 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:19:22.367095 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m14:19:22.367854 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m14:19:22.369534 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m14:19:22.372634 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m14:19:22.373464 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:19:22.373882 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m14:19:22.381463 [debug] [Thread-2 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  function pg_catalog.extract(unknown, integer) does not exist
  LINE 21:     AVG(EXTRACT(EPOCH FROM (expected_delivery_date - shipmen...
                   ^
  HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:19:22.382313 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bc10e861-d968-4a1c-801a-5442778b6c22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021547033AC0>]}
[0m14:19:22.383831 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.009 seconds
[0m14:19:22.383397 [error] [Thread-2 (]: 2 of 3 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.14s]
[0m14:19:22.385463 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m14:19:22.386050 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m14:19:22.387002 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  function pg_catalog.extract(unknown, integer) does not exist
  LINE 21:     AVG(EXTRACT(EPOCH FROM (expected_delivery_date - shipmen...
                   ^
  HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m14:19:22.387729 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bc10e861-d968-4a1c-801a-5442778b6c22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021546A651D0>]}
[0m14:19:22.389225 [info ] [Thread-3 (]: 3 of 3 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.15s]
[0m14:19:22.390280 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:19:22.392107 [debug] [MainThread]: Using postgres connection "master"
[0m14:19:22.392552 [debug] [MainThread]: On master: BEGIN
[0m14:19:22.392919 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:19:22.458555 [debug] [MainThread]: SQL status: BEGIN in 0.065 seconds
[0m14:19:22.459280 [debug] [MainThread]: On master: COMMIT
[0m14:19:22.459870 [debug] [MainThread]: Using postgres connection "master"
[0m14:19:22.460368 [debug] [MainThread]: On master: COMMIT
[0m14:19:22.461050 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:19:22.461451 [debug] [MainThread]: On master: Close
[0m14:19:22.462150 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:19:22.462515 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m14:19:22.462787 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m14:19:22.463122 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments' was properly closed.
[0m14:19:22.463369 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m14:19:22.463617 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m14:19:22.464004 [info ] [MainThread]: 
[0m14:19:22.464644 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 1.13 seconds (1.13s).
[0m14:19:22.466520 [debug] [MainThread]: Command end result
[0m14:19:22.495829 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:19:22.500111 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:19:22.508571 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m14:19:22.509188 [info ] [MainThread]: 
[0m14:19:22.510000 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:19:22.510608 [info ] [MainThread]: 
[0m14:19:22.511290 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m14:19:22.511893 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  function pg_catalog.extract(unknown, integer) does not exist
  LINE 21:     AVG(EXTRACT(EPOCH FROM (expected_delivery_date - shipmen...
                   ^
  HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:19:22.512425 [info ] [MainThread]: 
[0m14:19:22.513014 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:19:22.513518 [info ] [MainThread]: 
[0m14:19:22.514061 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=3
[0m14:19:22.515489 [debug] [MainThread]: Command `dbt run` failed at 14:19:22.515359 after 3.11 seconds
[0m14:19:22.515881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215472C43D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021546FE8950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215453FFA80>]}
[0m14:19:22.516251 [debug] [MainThread]: Flushing usage events
[0m14:19:23.673259 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:32:42.672973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A174AA0980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A17553CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A175B8BD90>]}


============================== 14:32:42.680307 | 8544f83d-13f1-4b8d-b599-7afb5ac89319 ==============================
[0m14:32:42.680307 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:32:42.681673 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt run --select marts.analytics', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:32:42.984524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8544f83d-13f1-4b8d-b599-7afb5ac89319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A174F08770>]}
[0m14:32:43.069379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8544f83d-13f1-4b8d-b599-7afb5ac89319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A175A2EE00>]}
[0m14:32:43.071208 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:32:43.720208 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:32:43.959853 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 0 files changed.
[0m14:32:43.960634 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\analytics\fct_shipments_slq.sql
[0m14:32:43.961358 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\analytics\fct_shipments_revenue.sql
[0m14:32:44.338473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8544f83d-13f1-4b8d-b599-7afb5ac89319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A177539950>]}
[0m14:32:44.442560 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:32:44.451146 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:32:44.509387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8544f83d-13f1-4b8d-b599-7afb5ac89319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A175A1FD40>]}
[0m14:32:44.510424 [info ] [MainThread]: Found 13 models, 5 sources, 459 macros
[0m14:32:44.511355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8544f83d-13f1-4b8d-b599-7afb5ac89319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A1776BA6D0>]}
[0m14:32:44.514790 [info ] [MainThread]: 
[0m14:32:44.515487 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:32:44.516443 [info ] [MainThread]: 
[0m14:32:44.517417 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:32:44.525419 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m14:32:44.801655 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m14:32:44.802223 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m14:32:44.802651 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:32:44.968843 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.166 seconds
[0m14:32:44.971147 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m14:32:44.975197 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m14:32:44.987276 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:32:44.988021 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m14:32:44.988558 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:32:45.062144 [debug] [ThreadPool]: SQL status: BEGIN in 0.073 seconds
[0m14:32:45.062700 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:32:45.063128 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m14:32:45.072027 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.008 seconds
[0m14:32:45.074003 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m14:32:45.074796 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m14:32:45.086285 [debug] [MainThread]: Using postgres connection "master"
[0m14:32:45.086830 [debug] [MainThread]: On master: BEGIN
[0m14:32:45.087186 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:32:45.161850 [debug] [MainThread]: SQL status: BEGIN in 0.075 seconds
[0m14:32:45.162387 [debug] [MainThread]: Using postgres connection "master"
[0m14:32:45.162949 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:32:45.171977 [debug] [MainThread]: SQL status: SELECT 5 in 0.008 seconds
[0m14:32:45.174924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8544f83d-13f1-4b8d-b599-7afb5ac89319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A1778D2F70>]}
[0m14:32:45.175558 [debug] [MainThread]: On master: ROLLBACK
[0m14:32:45.176271 [debug] [MainThread]: Using postgres connection "master"
[0m14:32:45.176931 [debug] [MainThread]: On master: BEGIN
[0m14:32:45.178460 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m14:32:45.178963 [debug] [MainThread]: On master: COMMIT
[0m14:32:45.179368 [debug] [MainThread]: Using postgres connection "master"
[0m14:32:45.179721 [debug] [MainThread]: On master: COMMIT
[0m14:32:45.180381 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:32:45.180789 [debug] [MainThread]: On master: Close
[0m14:32:45.187037 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m14:32:45.187978 [info ] [Thread-1 (]: 1 of 5 START sql table model analytics.fct_shipments ........................... [RUN]
[0m14:32:45.189109 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m14:32:45.189723 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m14:32:45.200201 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:32:45.202642 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m14:32:45.253666 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:32:45.255997 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:32:45.256624 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m14:32:45.257188 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:32:45.327557 [debug] [Thread-1 (]: SQL status: BEGIN in 0.070 seconds
[0m14:32:45.328121 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:32:45.328591 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."analytics"."stg_shipments"
),

shipment_status as (
    select *
    from "delivery_analytics"."analytics"."stg_shipment_status"
),

users as (
    select *
    from "delivery_analytics"."analytics"."dim_users"
),

couriers as (
    select *
    from "delivery_analytics"."analytics"."dim_couriers"
)

select
    s.shipment_id,
    s.sender_id,
    su.username as sender_name,
    s.receiver_id,
    ru.username as receiver_name,
    s.shipment_date,
    s.expected_delivery_date,
    ss.status,
    ss.status_timestamp,
    ss.courier_id,
    c.user_id as courier_user_id,
    c.vehicle_type,
    s.price
from shipments s
left join shipment_status ss on s.shipment_id = ss.shipment_id
left join users su on s.sender_id = su.user_id
left join users ru on s.receiver_id = ru.user_id
left join couriers c on ss.courier_id = c.courier_id
  );
  
[0m14:32:45.340457 [debug] [Thread-1 (]: SQL status: SELECT 10 in 0.011 seconds
[0m14:32:45.351627 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:32:45.352100 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m14:32:45.353243 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:32:45.356884 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:32:45.357523 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m14:32:45.358988 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:32:45.378733 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:32:45.379296 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:32:45.379709 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:32:45.381345 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m14:32:45.388330 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m14:32:45.394470 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:32:45.394925 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m14:32:45.400763 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.005 seconds
[0m14:32:45.404481 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m14:32:45.407693 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8544f83d-13f1-4b8d-b599-7afb5ac89319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A1773EBF50>]}
[0m14:32:45.408625 [info ] [Thread-1 (]: 1 of 5 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 10[0m in 0.22s]
[0m14:32:45.410167 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m14:32:45.411306 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:32:45.411892 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:32:45.412411 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:32:45.413045 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m14:32:45.413807 [info ] [Thread-1 (]: 5 of 5 START sql table model analytics.fct_shipments_slq ....................... [RUN]
[0m14:32:45.414560 [info ] [Thread-3 (]: 3 of 5 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m14:32:45.415652 [info ] [Thread-4 (]: 4 of 5 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m14:32:45.417540 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.fct_shipments_slq)
[0m14:32:45.416558 [info ] [Thread-2 (]: 2 of 5 START sql table model analytics.courier_performance ..................... [RUN]
[0m14:32:45.418272 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m14:32:45.418969 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m14:32:45.419576 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:32:45.420352 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m14:32:45.421014 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m14:32:45.421779 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:32:45.426828 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:32:45.427605 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m14:32:45.431476 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:32:45.435830 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:32:45.438980 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m14:32:45.441107 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:32:45.447051 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:32:45.448397 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m14:32:45.449031 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:32:45.449596 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m14:32:45.454853 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:32:45.459767 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:32:45.464609 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m14:32:45.465307 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:32:45.466386 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: BEGIN
[0m14:32:45.467001 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:45.468270 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:32:45.468948 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:32:45.469508 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:32:45.469927 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m14:32:45.470366 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m14:32:45.470813 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m14:32:45.471292 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:32:45.471902 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:32:45.472429 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:32:45.541058 [debug] [Thread-1 (]: SQL status: BEGIN in 0.074 seconds
[0m14:32:45.541903 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:32:45.542596 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_slq"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_slq__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with base as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
),

sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,
        -- trajanje isporuke u danima
        extract(epoch from (delivered_at - shipment_date))/86400 as delivery_duration_days,
        -- kasnjenje
        case 
            when delivered_at > expected_delivery_date then true
            else false
        end as late_delivery
    from base
)

select * from sla;
  );
  
[0m14:32:45.543861 [debug] [Thread-1 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 44: select * from sla;
                          ^

[0m14:32:45.544491 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: ROLLBACK
[0m14:32:45.545347 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: Close
[0m14:32:45.555789 [debug] [Thread-1 (]: Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:32:45.556540 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8544f83d-13f1-4b8d-b599-7afb5ac89319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A17772FB70>]}
[0m14:32:45.557758 [debug] [Thread-4 (]: SQL status: BEGIN in 0.086 seconds
[0m14:32:45.557269 [error] [Thread-1 (]: 5 of 5 ERROR creating sql table model analytics.fct_shipments_slq .............. [[31mERROR[0m in 0.14s]
[0m14:32:45.558507 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:32:45.559443 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:32:45.560200 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

with base as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date::date as date,
    courier_id,
    sum(price) as total_revenue,
    count(shipment_id) as total_shipments
from base
group by 1, 2
order by 1, 2;
  );
  
[0m14:32:45.561054 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_slq' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql.
[0m14:32:45.562408 [debug] [Thread-4 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 26: order by 1, 2;
                      ^

[0m14:32:45.562814 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: ROLLBACK
[0m14:32:45.563479 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m14:32:45.570702 [debug] [Thread-4 (]: Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 26: order by 1, 2;
                        ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:32:45.571362 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8544f83d-13f1-4b8d-b599-7afb5ac89319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A17775F890>]}
[0m14:32:45.572057 [error] [Thread-4 (]: 4 of 5 ERROR creating sql table model analytics.fct_shipments_revenue .......... [[31mERROR[0m in 0.15s]
[0m14:32:45.573132 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:32:45.573803 [debug] [Thread-3 (]: SQL status: BEGIN in 0.102 seconds
[0m14:32:45.574663 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_revenue' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 26: order by 1, 2;
                        ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql.
[0m14:32:45.575400 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:32:45.576259 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m14:32:45.585056 [debug] [Thread-3 (]: SQL status: SELECT 9 in 0.008 seconds
[0m14:32:45.588822 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:32:45.589252 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m14:32:45.590150 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m14:32:45.590614 [debug] [Thread-2 (]: SQL status: BEGIN in 0.118 seconds
[0m14:32:45.593963 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:32:45.594474 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:32:45.594925 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m14:32:45.595385 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when status = 'delivered' then 1 else 0 end) as delivered_shipments,
    AVG(EXTRACT(EPOCH FROM (expected_delivery_date - shipment_date)) / 86400) AS avg_delivery_time
from fct
group by courier_id
  );
  
[0m14:32:45.596731 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:32:45.598350 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:32:45.598877 [debug] [Thread-2 (]: Postgres adapter: Postgres error: function pg_catalog.extract(unknown, integer) does not exist
LINE 21:     AVG(EXTRACT(EPOCH FROM (expected_delivery_date - shipmen...
                 ^
HINT:  No function matches the given name and argument types. You might need to add explicit type casts.

[0m14:32:45.599325 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:32:45.599784 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m14:32:45.600243 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:32:45.601025 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m14:32:45.602410 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m14:32:45.604881 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m14:32:45.605659 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:32:45.606064 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m14:32:45.610070 [debug] [Thread-2 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  function pg_catalog.extract(unknown, integer) does not exist
  LINE 21:     AVG(EXTRACT(EPOCH FROM (expected_delivery_date - shipmen...
                   ^
  HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:32:45.610748 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8544f83d-13f1-4b8d-b599-7afb5ac89319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A177B3DA50>]}
[0m14:32:45.611524 [error] [Thread-2 (]: 2 of 5 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.19s]
[0m14:32:45.612443 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m14:32:45.613042 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  function pg_catalog.extract(unknown, integer) does not exist
  LINE 21:     AVG(EXTRACT(EPOCH FROM (expected_delivery_date - shipmen...
                   ^
  HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m14:32:45.617483 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.011 seconds
[0m14:32:45.619064 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m14:32:45.619749 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8544f83d-13f1-4b8d-b599-7afb5ac89319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A177AB4130>]}
[0m14:32:45.620385 [info ] [Thread-3 (]: 3 of 5 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.20s]
[0m14:32:45.621266 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:32:45.623262 [debug] [MainThread]: Using postgres connection "master"
[0m14:32:45.623655 [debug] [MainThread]: On master: BEGIN
[0m14:32:45.623959 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:32:45.696571 [debug] [MainThread]: SQL status: BEGIN in 0.073 seconds
[0m14:32:45.697098 [debug] [MainThread]: On master: COMMIT
[0m14:32:45.697411 [debug] [MainThread]: Using postgres connection "master"
[0m14:32:45.697705 [debug] [MainThread]: On master: COMMIT
[0m14:32:45.698211 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:32:45.698555 [debug] [MainThread]: On master: Close
[0m14:32:45.699093 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:32:45.699403 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m14:32:45.699668 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m14:32:45.699999 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_slq' was properly closed.
[0m14:32:45.700294 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m14:32:45.700638 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_revenue' was properly closed.
[0m14:32:45.700998 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m14:32:45.701482 [info ] [MainThread]: 
[0m14:32:45.702096 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 1.18 seconds (1.18s).
[0m14:32:45.703764 [debug] [MainThread]: Command end result
[0m14:32:45.730770 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:32:45.735363 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:32:45.743618 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m14:32:45.744290 [info ] [MainThread]: 
[0m14:32:45.745056 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m14:32:45.745720 [info ] [MainThread]: 
[0m14:32:45.746412 [error] [MainThread]: [31mFailure in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)[0m
[0m14:32:45.747096 [error] [MainThread]:   Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:32:45.747607 [info ] [MainThread]: 
[0m14:32:45.748243 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:32:45.748744 [info ] [MainThread]: 
[0m14:32:45.749382 [error] [MainThread]: [31mFailure in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)[0m
[0m14:32:45.749967 [error] [MainThread]:   Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 26: order by 1, 2;
                        ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:32:45.750491 [info ] [MainThread]: 
[0m14:32:45.750948 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:32:45.751385 [info ] [MainThread]: 
[0m14:32:45.752087 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m14:32:45.752861 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  function pg_catalog.extract(unknown, integer) does not exist
  LINE 21:     AVG(EXTRACT(EPOCH FROM (expected_delivery_date - shipmen...
                   ^
  HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:32:45.753406 [info ] [MainThread]: 
[0m14:32:45.753918 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:32:45.754366 [info ] [MainThread]: 
[0m14:32:45.754898 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=5
[0m14:32:45.756639 [debug] [MainThread]: Command `dbt run` failed at 14:32:45.756508 after 3.40 seconds
[0m14:32:45.757082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A16F36D490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A177AF0EF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A177B3BC50>]}
[0m14:32:45.757484 [debug] [MainThread]: Flushing usage events
[0m14:32:46.801814 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:45:28.332397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130D070980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130DB1CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130E16BD90>]}


============================== 14:45:28.339019 | 502b1357-9fda-4efc-b783-3bc6e6169e06 ==============================
[0m14:45:28.339019 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:45:28.340779 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt run --select marts.analytics', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:45:28.654343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '502b1357-9fda-4efc-b783-3bc6e6169e06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130D4D8770>]}
[0m14:45:28.745234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '502b1357-9fda-4efc-b783-3bc6e6169e06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130E00EE00>]}
[0m14:45:28.747564 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:45:29.343316 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:45:29.559669 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m14:45:29.560783 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\fct_shipments_revenue.sql
[0m14:45:29.561521 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\courier_performance.sql
[0m14:45:29.920024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '502b1357-9fda-4efc-b783-3bc6e6169e06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130FB05850>]}
[0m14:45:30.013565 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:45:30.017831 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:45:30.057444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '502b1357-9fda-4efc-b783-3bc6e6169e06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130DFFFD40>]}
[0m14:45:30.058286 [info ] [MainThread]: Found 13 models, 5 sources, 459 macros
[0m14:45:30.059287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '502b1357-9fda-4efc-b783-3bc6e6169e06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130FC9E7B0>]}
[0m14:45:30.061824 [info ] [MainThread]: 
[0m14:45:30.062629 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:45:30.063409 [info ] [MainThread]: 
[0m14:45:30.064712 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:45:30.070942 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m14:45:30.350634 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m14:45:30.351120 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m14:45:30.351425 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:45:30.480823 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.129 seconds
[0m14:45:30.482290 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m14:45:30.485471 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m14:45:30.492817 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:45:30.493239 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m14:45:30.493532 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:45:30.554270 [debug] [ThreadPool]: SQL status: BEGIN in 0.061 seconds
[0m14:45:30.554739 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:45:30.555047 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m14:45:30.561166 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.006 seconds
[0m14:45:30.562767 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m14:45:30.563371 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m14:45:30.572476 [debug] [MainThread]: Using postgres connection "master"
[0m14:45:30.573117 [debug] [MainThread]: On master: BEGIN
[0m14:45:30.573572 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:45:30.630710 [debug] [MainThread]: SQL status: BEGIN in 0.057 seconds
[0m14:45:30.631230 [debug] [MainThread]: Using postgres connection "master"
[0m14:45:30.631682 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:45:30.639131 [debug] [MainThread]: SQL status: SELECT 5 in 0.007 seconds
[0m14:45:30.641198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '502b1357-9fda-4efc-b783-3bc6e6169e06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130FEBF040>]}
[0m14:45:30.641694 [debug] [MainThread]: On master: ROLLBACK
[0m14:45:30.642285 [debug] [MainThread]: Using postgres connection "master"
[0m14:45:30.642657 [debug] [MainThread]: On master: BEGIN
[0m14:45:30.643398 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m14:45:30.643734 [debug] [MainThread]: On master: COMMIT
[0m14:45:30.644034 [debug] [MainThread]: Using postgres connection "master"
[0m14:45:30.644294 [debug] [MainThread]: On master: COMMIT
[0m14:45:30.644708 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:45:30.645014 [debug] [MainThread]: On master: Close
[0m14:45:30.650576 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m14:45:30.651179 [info ] [Thread-1 (]: 1 of 5 START sql table model analytics.fct_shipments ........................... [RUN]
[0m14:45:30.652035 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m14:45:30.652416 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m14:45:30.660175 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:45:30.662024 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m14:45:30.704569 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:45:30.706510 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:45:30.706916 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m14:45:30.707277 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:45:30.768409 [debug] [Thread-1 (]: SQL status: BEGIN in 0.061 seconds
[0m14:45:30.769036 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:45:30.769498 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."analytics"."stg_shipments"
),

shipment_status as (
    select *
    from "delivery_analytics"."analytics"."stg_shipment_status"
),

users as (
    select *
    from "delivery_analytics"."analytics"."dim_users"
),

couriers as (
    select *
    from "delivery_analytics"."analytics"."dim_couriers"
)

select
    s.shipment_id,
    s.sender_id,
    su.username as sender_name,
    s.receiver_id,
    ru.username as receiver_name,
    s.shipment_date,
    s.expected_delivery_date,
    ss.status,
    ss.status_timestamp,
    ss.courier_id,
    c.user_id as courier_user_id,
    c.vehicle_type,
    s.price
from shipments s
left join shipment_status ss on s.shipment_id = ss.shipment_id
left join users su on s.sender_id = su.user_id
left join users ru on s.receiver_id = ru.user_id
left join couriers c on ss.courier_id = c.courier_id
  );
  
[0m14:45:30.778291 [debug] [Thread-1 (]: SQL status: SELECT 10 in 0.008 seconds
[0m14:45:30.788327 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:45:30.788791 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m14:45:30.789719 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:45:30.792582 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:45:30.793049 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m14:45:30.794311 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:45:30.809833 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:45:30.810345 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:45:30.810708 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:45:30.812761 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m14:45:30.819043 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m14:45:30.823947 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:45:30.824329 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m14:45:30.828297 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.004 seconds
[0m14:45:30.830809 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m14:45:30.833121 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '502b1357-9fda-4efc-b783-3bc6e6169e06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130F9DFF50>]}
[0m14:45:30.833789 [info ] [Thread-1 (]: 1 of 5 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 10[0m in 0.18s]
[0m14:45:30.834892 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m14:45:30.835844 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:45:30.836251 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:45:30.836812 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:45:30.837892 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m14:45:30.837422 [info ] [Thread-1 (]: 5 of 5 START sql table model analytics.fct_shipments_slq ....................... [RUN]
[0m14:45:30.838871 [info ] [Thread-3 (]: 3 of 5 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m14:45:30.840576 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.fct_shipments_slq)
[0m14:45:30.839433 [info ] [Thread-4 (]: 4 of 5 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m14:45:30.841309 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m14:45:30.841829 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:45:30.840073 [info ] [Thread-2 (]: 2 of 5 START sql table model analytics.courier_performance ..................... [RUN]
[0m14:45:30.842497 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m14:45:30.842983 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m14:45:30.845916 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:45:30.846718 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m14:45:30.847264 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:45:30.850158 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:45:30.850854 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m14:45:30.854824 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:45:30.857506 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m14:45:30.858724 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:45:30.862959 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:45:30.864165 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:45:30.864724 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m14:45:30.865349 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m14:45:30.869536 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:45:30.872829 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m14:45:30.876329 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:45:30.877310 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:45:30.877995 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: BEGIN
[0m14:45:30.878646 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:45:30.879655 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:45:30.880210 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:45:30.880649 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:45:30.881015 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m14:45:30.881619 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m14:45:30.882191 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m14:45:30.883870 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:45:30.883337 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:45:30.882741 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:45:30.950118 [debug] [Thread-1 (]: SQL status: BEGIN in 0.071 seconds
[0m14:45:30.950804 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:45:30.951198 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_slq"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_slq__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with base as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
),

sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,
        -- trajanje isporuke u danima
        extract(epoch from (delivered_at - shipment_date))/86400 as delivery_duration_days,
        -- kasnjenje
        case 
            when delivered_at > expected_delivery_date then true
            else false
        end as late_delivery
    from base
)

select * from sla;
  );
  
[0m14:45:30.952206 [debug] [Thread-1 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 44: select * from sla;
                          ^

[0m14:45:30.952686 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: ROLLBACK
[0m14:45:30.953402 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: Close
[0m14:45:30.964316 [debug] [Thread-1 (]: Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:45:30.965065 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '502b1357-9fda-4efc-b783-3bc6e6169e06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130FD1BCD0>]}
[0m14:45:30.965738 [error] [Thread-1 (]: 5 of 5 ERROR creating sql table model analytics.fct_shipments_slq .............. [[31mERROR[0m in 0.12s]
[0m14:45:30.966877 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:45:30.967613 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_slq' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql.
[0m14:45:30.968952 [debug] [Thread-3 (]: SQL status: BEGIN in 0.085 seconds
[0m14:45:30.969406 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:45:30.969838 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m14:45:30.978502 [debug] [Thread-3 (]: SQL status: SELECT 9 in 0.008 seconds
[0m14:45:30.983632 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:45:30.984339 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m14:45:30.984990 [debug] [Thread-2 (]: SQL status: BEGIN in 0.102 seconds
[0m14:45:30.985387 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:45:30.985802 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:45:30.988716 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:45:30.989245 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when status = 'delivered' then 1 else 0 end) as delivered_shipments,
    avg(expected_delivery_date - shipment_date) as avg_delivery_days
from fct
group by courier_id;
  );
  
[0m14:45:30.989805 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m14:45:30.990809 [debug] [Thread-2 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 23: group by courier_id;
                            ^

[0m14:45:30.991273 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:45:30.991724 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m14:45:30.993349 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:45:30.994060 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:45:30.994557 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m14:45:30.994959 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:45:30.995362 [debug] [Thread-4 (]: SQL status: BEGIN in 0.113 seconds
[0m14:45:30.995969 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:45:30.997632 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

with base as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date::date as date,
    courier_id,
    sum(price) as total_revenue,
    count(shipment_id) as total_shipments
from base
group by shipment_date::date, courier_id;
  );
  
[0m14:45:30.998151 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m14:45:31.000676 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m14:45:31.001179 [debug] [Thread-4 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 25: group by shipment_date::date, courier_id;
                                                 ^

[0m14:45:31.001905 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:45:31.002346 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: ROLLBACK
[0m14:45:31.002862 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m14:45:31.003746 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m14:45:31.008279 [debug] [Thread-2 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  syntax error at or near ";"
  LINE 23: group by courier_id;
                              ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:45:31.008907 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '502b1357-9fda-4efc-b783-3bc6e6169e06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002131013F7F0>]}
[0m14:45:31.009621 [error] [Thread-2 (]: 2 of 5 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.16s]
[0m14:45:31.010564 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m14:45:31.011142 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  syntax error at or near ";"
  LINE 23: group by courier_id;
                              ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m14:45:31.013878 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.010 seconds
[0m14:45:31.015804 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m14:45:31.016585 [debug] [Thread-4 (]: Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 25: group by shipment_date::date, courier_id;
                                                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:45:31.017161 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '502b1357-9fda-4efc-b783-3bc6e6169e06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130FC1DF50>]}
[0m14:45:31.017710 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '502b1357-9fda-4efc-b783-3bc6e6169e06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002131012E450>]}
[0m14:45:31.018523 [error] [Thread-4 (]: 4 of 5 ERROR creating sql table model analytics.fct_shipments_revenue .......... [[31mERROR[0m in 0.17s]
[0m14:45:31.020133 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:45:31.019138 [info ] [Thread-3 (]: 3 of 5 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.18s]
[0m14:45:31.021186 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_revenue' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 25: group by shipment_date::date, courier_id;
                                                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql.
[0m14:45:31.021976 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:45:31.024145 [debug] [MainThread]: Using postgres connection "master"
[0m14:45:31.024701 [debug] [MainThread]: On master: BEGIN
[0m14:45:31.025033 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:45:31.087102 [debug] [MainThread]: SQL status: BEGIN in 0.062 seconds
[0m14:45:31.087646 [debug] [MainThread]: On master: COMMIT
[0m14:45:31.088002 [debug] [MainThread]: Using postgres connection "master"
[0m14:45:31.088333 [debug] [MainThread]: On master: COMMIT
[0m14:45:31.089105 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:45:31.089536 [debug] [MainThread]: On master: Close
[0m14:45:31.090106 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:45:31.090413 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m14:45:31.090660 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m14:45:31.090963 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_slq' was properly closed.
[0m14:45:31.091209 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m14:45:31.091445 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_revenue' was properly closed.
[0m14:45:31.091691 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m14:45:31.092060 [info ] [MainThread]: 
[0m14:45:31.092716 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 1.03 seconds (1.03s).
[0m14:45:31.094604 [debug] [MainThread]: Command end result
[0m14:45:31.122058 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:45:31.125547 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:45:31.133842 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m14:45:31.134398 [info ] [MainThread]: 
[0m14:45:31.135103 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m14:45:31.135736 [info ] [MainThread]: 
[0m14:45:31.136362 [error] [MainThread]: [31mFailure in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)[0m
[0m14:45:31.136958 [error] [MainThread]:   Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:45:31.137529 [info ] [MainThread]: 
[0m14:45:31.138083 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:45:31.138707 [info ] [MainThread]: 
[0m14:45:31.139332 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m14:45:31.140014 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  syntax error at or near ";"
  LINE 23: group by courier_id;
                              ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:45:31.140563 [info ] [MainThread]: 
[0m14:45:31.141154 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:45:31.141665 [info ] [MainThread]: 
[0m14:45:31.142212 [error] [MainThread]: [31mFailure in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)[0m
[0m14:45:31.142786 [error] [MainThread]:   Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 25: group by shipment_date::date, courier_id;
                                                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:45:31.143303 [info ] [MainThread]: 
[0m14:45:31.143837 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:45:31.144325 [info ] [MainThread]: 
[0m14:45:31.144976 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=5
[0m14:45:31.146715 [debug] [MainThread]: Command `dbt run` failed at 14:45:31.146570 after 3.13 seconds
[0m14:45:31.147145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130B973410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130DAE6C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000213100DF7D0>]}
[0m14:45:31.147559 [debug] [MainThread]: Flushing usage events
[0m14:45:32.247789 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:47:08.362173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2A200980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2ACACB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2B2E7D90>]}


============================== 14:47:08.370940 | 7bb128af-578b-4928-9a25-d3a428293717 ==============================
[0m14:47:08.370940 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:47:08.372336 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run --select marts.analytics', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:47:08.744653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7bb128af-578b-4928-9a25-d3a428293717', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2A668770>]}
[0m14:47:08.844282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7bb128af-578b-4928-9a25-d3a428293717', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2B18EE00>]}
[0m14:47:08.846799 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:47:09.516210 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:47:09.742065 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:47:09.743130 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\courier_performance.sql
[0m14:47:10.103354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7bb128af-578b-4928-9a25-d3a428293717', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2CCD5950>]}
[0m14:47:10.194084 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:47:10.199115 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:47:10.241207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7bb128af-578b-4928-9a25-d3a428293717', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2B17FD40>]}
[0m14:47:10.242050 [info ] [MainThread]: Found 13 models, 5 sources, 459 macros
[0m14:47:10.242920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7bb128af-578b-4928-9a25-d3a428293717', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2CE6E6D0>]}
[0m14:47:10.245714 [info ] [MainThread]: 
[0m14:47:10.246376 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:47:10.246980 [info ] [MainThread]: 
[0m14:47:10.248097 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:47:10.253714 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m14:47:10.509460 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m14:47:10.509903 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m14:47:10.510281 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:47:10.651384 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.141 seconds
[0m14:47:10.654083 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m14:47:10.657208 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m14:47:10.664653 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:47:10.665196 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m14:47:10.665548 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:47:10.729200 [debug] [ThreadPool]: SQL status: BEGIN in 0.064 seconds
[0m14:47:10.729689 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:47:10.730008 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m14:47:10.736798 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.006 seconds
[0m14:47:10.738382 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m14:47:10.739067 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m14:47:10.746152 [debug] [MainThread]: Using postgres connection "master"
[0m14:47:10.746555 [debug] [MainThread]: On master: BEGIN
[0m14:47:10.746848 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:47:10.809602 [debug] [MainThread]: SQL status: BEGIN in 0.063 seconds
[0m14:47:10.810107 [debug] [MainThread]: Using postgres connection "master"
[0m14:47:10.810492 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:47:10.817590 [debug] [MainThread]: SQL status: SELECT 5 in 0.007 seconds
[0m14:47:10.819608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7bb128af-578b-4928-9a25-d3a428293717', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2D09B040>]}
[0m14:47:10.820184 [debug] [MainThread]: On master: ROLLBACK
[0m14:47:10.820891 [debug] [MainThread]: Using postgres connection "master"
[0m14:47:10.821246 [debug] [MainThread]: On master: BEGIN
[0m14:47:10.822031 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m14:47:10.822417 [debug] [MainThread]: On master: COMMIT
[0m14:47:10.822759 [debug] [MainThread]: Using postgres connection "master"
[0m14:47:10.823034 [debug] [MainThread]: On master: COMMIT
[0m14:47:10.823547 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:47:10.823869 [debug] [MainThread]: On master: Close
[0m14:47:10.828146 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m14:47:10.828789 [info ] [Thread-1 (]: 1 of 5 START sql table model analytics.fct_shipments ........................... [RUN]
[0m14:47:10.829537 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m14:47:10.829912 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m14:47:10.838560 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:10.840550 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m14:47:10.880785 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:10.883145 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:10.883961 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m14:47:10.884636 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:47:10.948780 [debug] [Thread-1 (]: SQL status: BEGIN in 0.064 seconds
[0m14:47:10.949572 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:10.950153 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."analytics"."stg_shipments"
),

shipment_status as (
    select *
    from "delivery_analytics"."analytics"."stg_shipment_status"
),

users as (
    select *
    from "delivery_analytics"."analytics"."dim_users"
),

couriers as (
    select *
    from "delivery_analytics"."analytics"."dim_couriers"
)

select
    s.shipment_id,
    s.sender_id,
    su.username as sender_name,
    s.receiver_id,
    ru.username as receiver_name,
    s.shipment_date,
    s.expected_delivery_date,
    ss.status,
    ss.status_timestamp,
    ss.courier_id,
    c.user_id as courier_user_id,
    c.vehicle_type,
    s.price
from shipments s
left join shipment_status ss on s.shipment_id = ss.shipment_id
left join users su on s.sender_id = su.user_id
left join users ru on s.receiver_id = ru.user_id
left join couriers c on ss.courier_id = c.courier_id
  );
  
[0m14:47:10.960602 [debug] [Thread-1 (]: SQL status: SELECT 10 in 0.010 seconds
[0m14:47:10.970816 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:10.971251 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m14:47:10.972350 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:47:10.975268 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:10.975631 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m14:47:10.976640 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:47:10.993798 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:47:10.994377 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:10.994738 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:47:10.996093 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m14:47:11.004017 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m14:47:11.009085 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:11.009536 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m14:47:11.014006 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.004 seconds
[0m14:47:11.017531 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m14:47:11.020367 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7bb128af-578b-4928-9a25-d3a428293717', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2CC53590>]}
[0m14:47:11.021398 [info ] [Thread-1 (]: 1 of 5 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 10[0m in 0.19s]
[0m14:47:11.022535 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m14:47:11.023540 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:47:11.023988 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m14:47:11.025006 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:47:11.025616 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:47:11.024587 [info ] [Thread-1 (]: 5 of 5 START sql table model analytics.fct_shipments_slq ....................... [RUN]
[0m14:47:11.026237 [info ] [Thread-2 (]: 2 of 5 START sql table model analytics.courier_performance ..................... [RUN]
[0m14:47:11.028103 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.fct_shipments_slq)
[0m14:47:11.026760 [info ] [Thread-3 (]: 3 of 5 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m14:47:11.028990 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m14:47:11.029505 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:47:11.027383 [info ] [Thread-4 (]: 4 of 5 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m14:47:11.030208 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m14:47:11.030638 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m14:47:11.033548 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:47:11.034218 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m14:47:11.034682 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m14:47:11.037627 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m14:47:11.038410 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:47:11.042494 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:11.045241 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:47:11.046530 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:47:11.050351 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:47:11.051791 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m14:47:11.056230 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:11.056840 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m14:47:11.061106 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m14:47:11.061565 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:47:11.062282 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:47:11.065908 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:47:11.066493 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: BEGIN
[0m14:47:11.067373 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:47:11.068245 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:47:11.068709 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m14:47:11.069253 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:47:11.070029 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:11.070633 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m14:47:11.071354 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:47:11.073373 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:47:11.074075 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m14:47:11.074751 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:47:11.138533 [debug] [Thread-1 (]: SQL status: BEGIN in 0.071 seconds
[0m14:47:11.139094 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:47:11.139485 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_slq"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_slq__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with base as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
),

sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,
        -- trajanje isporuke u danima
        extract(epoch from (delivered_at - shipment_date))/86400 as delivery_duration_days,
        -- kasnjenje
        case 
            when delivered_at > expected_delivery_date then true
            else false
        end as late_delivery
    from base
)

select * from sla;
  );
  
[0m14:47:11.140369 [debug] [Thread-1 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 44: select * from sla;
                          ^

[0m14:47:11.140839 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: ROLLBACK
[0m14:47:11.141531 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: Close
[0m14:47:11.153074 [debug] [Thread-1 (]: Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:47:11.154049 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7bb128af-578b-4928-9a25-d3a428293717', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2D2909F0>]}
[0m14:47:11.155018 [error] [Thread-1 (]: 5 of 5 ERROR creating sql table model analytics.fct_shipments_slq .............. [[31mERROR[0m in 0.13s]
[0m14:47:11.156093 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:47:11.156789 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_slq' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql.
[0m14:47:11.159759 [debug] [Thread-2 (]: SQL status: BEGIN in 0.090 seconds
[0m14:47:11.160205 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:47:11.160588 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(shipment_id) as total_shipments,
    sum(case when not late_delivery then 1 else 0 end) as on_time_deliveries,
    sum(case when late_delivery then 1 else 0 end) as late_deliveries,
    avg(delivery_duration_days) as avg_delivery_days
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id;
  );
  
[0m14:47:11.161416 [debug] [Thread-2 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 19: group by courier_id;
                            ^

[0m14:47:11.161878 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m14:47:11.162604 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m14:47:11.168899 [debug] [Thread-2 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  syntax error at or near ";"
  LINE 19: group by courier_id;
                              ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:47:11.169688 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7bb128af-578b-4928-9a25-d3a428293717', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2CF1BF70>]}
[0m14:47:11.170681 [error] [Thread-2 (]: 2 of 5 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.14s]
[0m14:47:11.171601 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m14:47:11.172274 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  syntax error at or near ";"
  LINE 19: group by courier_id;
                              ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m14:47:11.175401 [debug] [Thread-3 (]: SQL status: BEGIN in 0.104 seconds
[0m14:47:11.175986 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:11.176416 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m14:47:11.183179 [debug] [Thread-3 (]: SQL status: SELECT 9 in 0.006 seconds
[0m14:47:11.188029 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:11.188646 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m14:47:11.189364 [debug] [Thread-4 (]: SQL status: BEGIN in 0.115 seconds
[0m14:47:11.189773 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:47:11.190157 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:47:11.193037 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:11.193531 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

with base as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date::date as date,
    courier_id,
    sum(price) as total_revenue,
    count(shipment_id) as total_shipments
from base
group by shipment_date::date, courier_id;
  );
  
[0m14:47:11.194053 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m14:47:11.195076 [debug] [Thread-4 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 25: group by shipment_date::date, courier_id;
                                                 ^

[0m14:47:11.195465 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:47:11.195889 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: ROLLBACK
[0m14:47:11.197687 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:47:11.198157 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:11.198654 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m14:47:11.199139 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:47:11.201006 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m14:47:11.204156 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m14:47:11.205447 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:11.206102 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m14:47:11.209950 [debug] [Thread-4 (]: Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 25: group by shipment_date::date, courier_id;
                                                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:47:11.210525 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7bb128af-578b-4928-9a25-d3a428293717', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2D301A50>]}
[0m14:47:11.211144 [error] [Thread-4 (]: 4 of 5 ERROR creating sql table model analytics.fct_shipments_revenue .......... [[31mERROR[0m in 0.18s]
[0m14:47:11.212100 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:47:11.212770 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_revenue' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 25: group by shipment_date::date, courier_id;
                                                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql.
[0m14:47:11.216134 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.009 seconds
[0m14:47:11.217943 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m14:47:11.218679 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7bb128af-578b-4928-9a25-d3a428293717', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2D22C9F0>]}
[0m14:47:11.219429 [info ] [Thread-3 (]: 3 of 5 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.19s]
[0m14:47:11.220488 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:47:11.222379 [debug] [MainThread]: Using postgres connection "master"
[0m14:47:11.222758 [debug] [MainThread]: On master: BEGIN
[0m14:47:11.223048 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:47:11.288683 [debug] [MainThread]: SQL status: BEGIN in 0.066 seconds
[0m14:47:11.289148 [debug] [MainThread]: On master: COMMIT
[0m14:47:11.289453 [debug] [MainThread]: Using postgres connection "master"
[0m14:47:11.289740 [debug] [MainThread]: On master: COMMIT
[0m14:47:11.290235 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:47:11.290557 [debug] [MainThread]: On master: Close
[0m14:47:11.291060 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:47:11.291345 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m14:47:11.291612 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m14:47:11.291986 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_slq' was properly closed.
[0m14:47:11.292321 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m14:47:11.292611 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m14:47:11.292870 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_revenue' was properly closed.
[0m14:47:11.293222 [info ] [MainThread]: 
[0m14:47:11.293838 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 1.05 seconds (1.05s).
[0m14:47:11.295661 [debug] [MainThread]: Command end result
[0m14:47:11.322627 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:47:11.326147 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:47:11.334035 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m14:47:11.334676 [info ] [MainThread]: 
[0m14:47:11.335361 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m14:47:11.336061 [info ] [MainThread]: 
[0m14:47:11.336721 [error] [MainThread]: [31mFailure in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)[0m
[0m14:47:11.337364 [error] [MainThread]:   Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:47:11.337873 [info ] [MainThread]: 
[0m14:47:11.338469 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:47:11.339058 [info ] [MainThread]: 
[0m14:47:11.339636 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m14:47:11.340258 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  syntax error at or near ";"
  LINE 19: group by courier_id;
                              ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:47:11.340813 [info ] [MainThread]: 
[0m14:47:11.341479 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:47:11.342070 [info ] [MainThread]: 
[0m14:47:11.342731 [error] [MainThread]: [31mFailure in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)[0m
[0m14:47:11.343342 [error] [MainThread]:   Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 25: group by shipment_date::date, courier_id;
                                                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:47:11.343947 [info ] [MainThread]: 
[0m14:47:11.344616 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:47:11.345122 [info ] [MainThread]: 
[0m14:47:11.345573 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=5
[0m14:47:11.347069 [debug] [MainThread]: Command `dbt run` failed at 14:47:11.346944 after 3.32 seconds
[0m14:47:11.347507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2D2CB950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2D2FFA10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024E2D2FF9B0>]}
[0m14:47:11.347906 [debug] [MainThread]: Flushing usage events
[0m14:47:12.459467 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:47:30.495456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001544D630980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001544E0ECB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001544E727D90>]}


============================== 14:47:30.501600 | f9230324-1487-48e3-8994-224429bee8d2 ==============================
[0m14:47:30.501600 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:47:30.503162 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'use_colors': 'True', 'invocation_command': 'dbt run --select marts.analytics', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:47:30.821337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f9230324-1487-48e3-8994-224429bee8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001544DA98770>]}
[0m14:47:30.914357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f9230324-1487-48e3-8994-224429bee8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001544E5CEE00>]}
[0m14:47:30.916434 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:47:31.481770 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:47:31.681843 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:47:31.682710 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\fct_shipments_revenue.sql
[0m14:47:31.987664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f9230324-1487-48e3-8994-224429bee8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015450029950>]}
[0m14:47:32.079083 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:47:32.084575 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:47:32.130930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f9230324-1487-48e3-8994-224429bee8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001544E5BFD40>]}
[0m14:47:32.131650 [info ] [MainThread]: Found 13 models, 5 sources, 459 macros
[0m14:47:32.132318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9230324-1487-48e3-8994-224429bee8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000154501B2970>]}
[0m14:47:32.134647 [info ] [MainThread]: 
[0m14:47:32.135380 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:47:32.136229 [info ] [MainThread]: 
[0m14:47:32.137230 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:47:32.142317 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m14:47:32.389226 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m14:47:32.389670 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m14:47:32.389963 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:47:32.526652 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.137 seconds
[0m14:47:32.528515 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m14:47:32.531394 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m14:47:32.538460 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:47:32.538890 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m14:47:32.539184 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:47:32.602121 [debug] [ThreadPool]: SQL status: BEGIN in 0.063 seconds
[0m14:47:32.602592 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:47:32.602910 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m14:47:32.609034 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.006 seconds
[0m14:47:32.610661 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m14:47:32.611371 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m14:47:32.619695 [debug] [MainThread]: Using postgres connection "master"
[0m14:47:32.620226 [debug] [MainThread]: On master: BEGIN
[0m14:47:32.620581 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:47:32.682876 [debug] [MainThread]: SQL status: BEGIN in 0.062 seconds
[0m14:47:32.683312 [debug] [MainThread]: Using postgres connection "master"
[0m14:47:32.683673 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:47:32.691934 [debug] [MainThread]: SQL status: SELECT 5 in 0.008 seconds
[0m14:47:32.693960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9230324-1487-48e3-8994-224429bee8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000154503FB040>]}
[0m14:47:32.694456 [debug] [MainThread]: On master: ROLLBACK
[0m14:47:32.695243 [debug] [MainThread]: Using postgres connection "master"
[0m14:47:32.695554 [debug] [MainThread]: On master: BEGIN
[0m14:47:32.696297 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m14:47:32.696612 [debug] [MainThread]: On master: COMMIT
[0m14:47:32.696932 [debug] [MainThread]: Using postgres connection "master"
[0m14:47:32.697223 [debug] [MainThread]: On master: COMMIT
[0m14:47:32.697707 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:47:32.698074 [debug] [MainThread]: On master: Close
[0m14:47:32.703397 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m14:47:32.704244 [info ] [Thread-1 (]: 1 of 5 START sql table model analytics.fct_shipments ........................... [RUN]
[0m14:47:32.705360 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m14:47:32.706118 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m14:47:32.713615 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:32.716706 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m14:47:32.755813 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:32.757756 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:32.758206 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m14:47:32.758545 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:47:32.819327 [debug] [Thread-1 (]: SQL status: BEGIN in 0.061 seconds
[0m14:47:32.819830 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:32.820430 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."analytics"."stg_shipments"
),

shipment_status as (
    select *
    from "delivery_analytics"."analytics"."stg_shipment_status"
),

users as (
    select *
    from "delivery_analytics"."analytics"."dim_users"
),

couriers as (
    select *
    from "delivery_analytics"."analytics"."dim_couriers"
)

select
    s.shipment_id,
    s.sender_id,
    su.username as sender_name,
    s.receiver_id,
    ru.username as receiver_name,
    s.shipment_date,
    s.expected_delivery_date,
    ss.status,
    ss.status_timestamp,
    ss.courier_id,
    c.user_id as courier_user_id,
    c.vehicle_type,
    s.price
from shipments s
left join shipment_status ss on s.shipment_id = ss.shipment_id
left join users su on s.sender_id = su.user_id
left join users ru on s.receiver_id = ru.user_id
left join couriers c on ss.courier_id = c.courier_id
  );
  
[0m14:47:32.827821 [debug] [Thread-1 (]: SQL status: SELECT 10 in 0.007 seconds
[0m14:47:32.838750 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:32.839398 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m14:47:32.840359 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m14:47:32.843503 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:32.843864 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m14:47:32.845002 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:47:32.863177 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:47:32.863808 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:32.864301 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:47:32.865808 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m14:47:32.874074 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m14:47:32.878830 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:47:32.879274 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m14:47:32.884413 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.005 seconds
[0m14:47:32.886820 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m14:47:32.889276 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9230324-1487-48e3-8994-224429bee8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001544FFA3590>]}
[0m14:47:32.890070 [info ] [Thread-1 (]: 1 of 5 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 10[0m in 0.18s]
[0m14:47:32.891313 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m14:47:32.892209 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:47:32.892615 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:47:32.892986 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:47:32.893372 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m14:47:32.893840 [info ] [Thread-1 (]: 5 of 5 START sql table model analytics.fct_shipments_slq ....................... [RUN]
[0m14:47:32.896546 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.fct_shipments_slq)
[0m14:47:32.894426 [info ] [Thread-3 (]: 3 of 5 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m14:47:32.897230 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:47:32.895176 [info ] [Thread-4 (]: 4 of 5 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m14:47:32.898067 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m14:47:32.902620 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m14:47:32.900810 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:47:32.902015 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m14:47:32.895962 [info ] [Thread-2 (]: 2 of 5 START sql table model analytics.courier_performance ..................... [RUN]
[0m14:47:32.907427 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:32.908218 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:47:32.908997 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m14:47:32.910115 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:47:32.914089 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:47:32.914549 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m14:47:32.917574 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:47:32.924553 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m14:47:32.927175 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m14:47:32.931214 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:32.931990 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:47:32.936842 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:47:32.939963 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:47:32.940646 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: BEGIN
[0m14:47:32.941176 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:47:32.942110 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m14:47:32.947083 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:47:32.947762 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:32.948440 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m14:47:32.949078 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m14:47:32.949638 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m14:47:32.950386 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:47:32.950924 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:47:32.953064 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:47:32.953795 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m14:47:32.954270 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:47:33.027131 [debug] [Thread-1 (]: SQL status: BEGIN in 0.086 seconds
[0m14:47:33.027765 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:47:33.028324 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_slq"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_slq__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with base as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
),

sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,
        -- trajanje isporuke u danima
        extract(epoch from (delivered_at - shipment_date))/86400 as delivery_duration_days,
        -- kasnjenje
        case 
            when delivered_at > expected_delivery_date then true
            else false
        end as late_delivery
    from base
)

select * from sla;
  );
  
[0m14:47:33.029365 [debug] [Thread-1 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 44: select * from sla;
                          ^

[0m14:47:33.029877 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: ROLLBACK
[0m14:47:33.030589 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: Close
[0m14:47:33.041438 [debug] [Thread-1 (]: Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:47:33.042301 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9230324-1487-48e3-8994-224429bee8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000154505F8AA0>]}
[0m14:47:33.042964 [debug] [Thread-3 (]: SQL status: BEGIN in 0.092 seconds
[0m14:47:33.044420 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:33.043802 [error] [Thread-1 (]: 5 of 5 ERROR creating sql table model analytics.fct_shipments_slq .............. [[31mERROR[0m in 0.15s]
[0m14:47:33.045067 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m14:47:33.045990 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:47:33.046967 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_slq' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql.
[0m14:47:33.053626 [debug] [Thread-3 (]: SQL status: SELECT 9 in 0.007 seconds
[0m14:47:33.057020 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:33.057466 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m14:47:33.058289 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m14:47:33.058667 [debug] [Thread-4 (]: SQL status: BEGIN in 0.108 seconds
[0m14:47:33.061559 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:33.062266 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:47:33.062857 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m14:47:33.063467 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    shipment_date,
    courier_id,
    sum(price) as total_revenue,
    count(shipment_id) as total_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by shipment_date, courier_id;
  );
  
[0m14:47:33.064730 [debug] [Thread-4 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 20: group by shipment_date, courier_id;
                                           ^

[0m14:47:33.065152 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: ROLLBACK
[0m14:47:33.065595 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:47:33.068079 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:47:33.068780 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m14:47:33.069329 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:33.069982 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:47:33.073498 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m14:47:33.076448 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m14:47:33.080186 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:47:33.081133 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m14:47:33.081725 [debug] [Thread-2 (]: SQL status: BEGIN in 0.127 seconds
[0m14:47:33.083405 [debug] [Thread-4 (]: Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 20: group by shipment_date, courier_id;
                                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:47:33.084108 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:47:33.084790 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9230324-1487-48e3-8994-224429bee8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001544FC73110>]}
[0m14:47:33.085383 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(shipment_id) as total_shipments,
    sum(case when not late_delivery then 1 else 0 end) as on_time_deliveries,
    sum(case when late_delivery then 1 else 0 end) as late_deliveries,
    avg(delivery_duration_days) as avg_delivery_days
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id;
  );
  
[0m14:47:33.086622 [error] [Thread-4 (]: 4 of 5 ERROR creating sql table model analytics.fct_shipments_revenue .......... [[31mERROR[0m in 0.18s]
[0m14:47:33.087720 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:47:33.088230 [debug] [Thread-2 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 19: group by courier_id;
                            ^

[0m14:47:33.088856 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_revenue' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 20: group by shipment_date, courier_id;
                                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql.
[0m14:47:33.089259 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m14:47:33.089702 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.006 seconds
[0m14:47:33.091441 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m14:47:33.091908 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m14:47:33.092694 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9230324-1487-48e3-8994-224429bee8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000154504FB650>]}
[0m14:47:33.094860 [info ] [Thread-3 (]: 3 of 5 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.19s]
[0m14:47:33.096247 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:47:33.101405 [debug] [Thread-2 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  syntax error at or near ";"
  LINE 19: group by courier_id;
                              ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:47:33.102262 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f9230324-1487-48e3-8994-224429bee8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000154505C7310>]}
[0m14:47:33.103437 [error] [Thread-2 (]: 2 of 5 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.19s]
[0m14:47:33.104722 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m14:47:33.105590 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  syntax error at or near ";"
  LINE 19: group by courier_id;
                              ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m14:47:33.107341 [debug] [MainThread]: Using postgres connection "master"
[0m14:47:33.107788 [debug] [MainThread]: On master: BEGIN
[0m14:47:33.108223 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:47:33.180745 [debug] [MainThread]: SQL status: BEGIN in 0.072 seconds
[0m14:47:33.181264 [debug] [MainThread]: On master: COMMIT
[0m14:47:33.181625 [debug] [MainThread]: Using postgres connection "master"
[0m14:47:33.181909 [debug] [MainThread]: On master: COMMIT
[0m14:47:33.182399 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:47:33.182921 [debug] [MainThread]: On master: Close
[0m14:47:33.183904 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:47:33.184195 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m14:47:33.184465 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m14:47:33.184813 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_slq' was properly closed.
[0m14:47:33.185077 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m14:47:33.185323 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_revenue' was properly closed.
[0m14:47:33.185587 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m14:47:33.186012 [info ] [MainThread]: 
[0m14:47:33.186546 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 1.05 seconds (1.05s).
[0m14:47:33.188240 [debug] [MainThread]: Command end result
[0m14:47:33.221154 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:47:33.224606 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:47:33.231585 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m14:47:33.231958 [info ] [MainThread]: 
[0m14:47:33.232596 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m14:47:33.233338 [info ] [MainThread]: 
[0m14:47:33.234011 [error] [MainThread]: [31mFailure in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)[0m
[0m14:47:33.234668 [error] [MainThread]:   Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:47:33.235202 [info ] [MainThread]: 
[0m14:47:33.235944 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:47:33.236504 [info ] [MainThread]: 
[0m14:47:33.237179 [error] [MainThread]: [31mFailure in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)[0m
[0m14:47:33.237826 [error] [MainThread]:   Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 20: group by shipment_date, courier_id;
                                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:47:33.238383 [info ] [MainThread]: 
[0m14:47:33.238979 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:47:33.239610 [info ] [MainThread]: 
[0m14:47:33.240240 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m14:47:33.240824 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  syntax error at or near ";"
  LINE 19: group by courier_id;
                              ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:47:33.241373 [info ] [MainThread]: 
[0m14:47:33.241961 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:47:33.242490 [info ] [MainThread]: 
[0m14:47:33.243038 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=5
[0m14:47:33.244646 [debug] [MainThread]: Command `dbt run` failed at 14:47:33.244512 after 2.97 seconds
[0m14:47:33.245075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001544E056B70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000154506D3E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000154506D38F0>]}
[0m14:47:33.245483 [debug] [MainThread]: Flushing usage events
[0m14:47:34.340578 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:48:03.843079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018076B14980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180775BCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018077C0BD90>]}


============================== 14:48:03.849372 | da51ae77-19b5-4e16-8580-b8ff0d7fd567 ==============================
[0m14:48:03.849372 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:48:03.850919 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt run --select marts.analytics', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:48:04.163952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'da51ae77-19b5-4e16-8580-b8ff0d7fd567', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018076F78770>]}
[0m14:48:04.250426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'da51ae77-19b5-4e16-8580-b8ff0d7fd567', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018077AAEE00>]}
[0m14:48:04.252266 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:48:04.859190 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:48:05.071290 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:48:05.071822 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:48:05.125016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'da51ae77-19b5-4e16-8580-b8ff0d7fd567', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018079508250>]}
[0m14:48:05.229621 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:48:05.238284 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:48:05.316708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'da51ae77-19b5-4e16-8580-b8ff0d7fd567', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018077A9FD40>]}
[0m14:48:05.317581 [info ] [MainThread]: Found 13 models, 5 sources, 459 macros
[0m14:48:05.318467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'da51ae77-19b5-4e16-8580-b8ff0d7fd567', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180790B58D0>]}
[0m14:48:05.321316 [info ] [MainThread]: 
[0m14:48:05.322086 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:48:05.322750 [info ] [MainThread]: 
[0m14:48:05.323673 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:48:05.328984 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m14:48:05.532760 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m14:48:05.533331 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m14:48:05.533839 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:48:05.716618 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.183 seconds
[0m14:48:05.718380 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m14:48:05.721588 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m14:48:05.729584 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:48:05.730128 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m14:48:05.730493 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:48:05.812552 [debug] [ThreadPool]: SQL status: BEGIN in 0.082 seconds
[0m14:48:05.813180 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:48:05.813593 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m14:48:05.821842 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.008 seconds
[0m14:48:05.823649 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m14:48:05.824444 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m14:48:05.833314 [debug] [MainThread]: Using postgres connection "master"
[0m14:48:05.833842 [debug] [MainThread]: On master: BEGIN
[0m14:48:05.834330 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:48:05.907632 [debug] [MainThread]: SQL status: BEGIN in 0.073 seconds
[0m14:48:05.908172 [debug] [MainThread]: Using postgres connection "master"
[0m14:48:05.908613 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:48:05.918391 [debug] [MainThread]: SQL status: SELECT 5 in 0.009 seconds
[0m14:48:05.920080 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'da51ae77-19b5-4e16-8580-b8ff0d7fd567', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018079950120>]}
[0m14:48:05.920650 [debug] [MainThread]: On master: ROLLBACK
[0m14:48:05.921389 [debug] [MainThread]: Using postgres connection "master"
[0m14:48:05.921873 [debug] [MainThread]: On master: BEGIN
[0m14:48:05.922870 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m14:48:05.923268 [debug] [MainThread]: On master: COMMIT
[0m14:48:05.923643 [debug] [MainThread]: Using postgres connection "master"
[0m14:48:05.923988 [debug] [MainThread]: On master: COMMIT
[0m14:48:05.924508 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:48:05.924895 [debug] [MainThread]: On master: Close
[0m14:48:05.931711 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m14:48:05.932750 [info ] [Thread-1 (]: 1 of 5 START sql table model analytics.fct_shipments ........................... [RUN]
[0m14:48:05.934009 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m14:48:05.934523 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m14:48:05.944182 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:48:05.947019 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m14:48:05.994807 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:48:05.997103 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:48:05.997655 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m14:48:05.998215 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:48:06.173810 [debug] [Thread-1 (]: SQL status: BEGIN in 0.175 seconds
[0m14:48:06.174444 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:48:06.174891 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."analytics"."stg_shipments"
),

shipment_status as (
    select *
    from "delivery_analytics"."analytics"."stg_shipment_status"
),

users as (
    select *
    from "delivery_analytics"."analytics"."dim_users"
),

couriers as (
    select *
    from "delivery_analytics"."analytics"."dim_couriers"
)

select
    s.shipment_id,
    s.sender_id,
    su.username as sender_name,
    s.receiver_id,
    ru.username as receiver_name,
    s.shipment_date,
    s.expected_delivery_date,
    ss.status,
    ss.status_timestamp,
    ss.courier_id,
    c.user_id as courier_user_id,
    c.vehicle_type,
    s.price
from shipments s
left join shipment_status ss on s.shipment_id = ss.shipment_id
left join users su on s.sender_id = su.user_id
left join users ru on s.receiver_id = ru.user_id
left join couriers c on ss.courier_id = c.courier_id
  );
  
[0m14:48:06.184018 [debug] [Thread-1 (]: SQL status: SELECT 10 in 0.009 seconds
[0m14:48:06.195634 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:48:06.196098 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m14:48:06.197219 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:48:06.200417 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:48:06.200845 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m14:48:06.202168 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:48:06.220868 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:48:06.221387 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:48:06.221795 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:48:06.254401 [debug] [Thread-1 (]: SQL status: COMMIT in 0.032 seconds
[0m14:48:06.261875 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m14:48:06.267761 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:48:06.268209 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m14:48:06.272944 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.004 seconds
[0m14:48:06.275664 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m14:48:06.278726 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da51ae77-19b5-4e16-8580-b8ff0d7fd567', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001807958F350>]}
[0m14:48:06.279598 [info ] [Thread-1 (]: 1 of 5 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 10[0m in 0.34s]
[0m14:48:06.280789 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m14:48:06.281846 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:48:06.282332 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m14:48:06.282817 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:48:06.283308 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:48:06.283961 [info ] [Thread-1 (]: 5 of 5 START sql table model analytics.fct_shipments_slq ....................... [RUN]
[0m14:48:06.284750 [info ] [Thread-2 (]: 2 of 5 START sql table model analytics.courier_performance ..................... [RUN]
[0m14:48:06.287227 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.fct_shipments_slq)
[0m14:48:06.285552 [info ] [Thread-4 (]: 4 of 5 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m14:48:06.288239 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m14:48:06.288855 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:48:06.286365 [info ] [Thread-3 (]: 3 of 5 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m14:48:06.289762 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m14:48:06.290362 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m14:48:06.294511 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:48:06.295456 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m14:48:06.296090 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:48:06.300250 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m14:48:06.301553 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m14:48:06.306075 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:48:06.310055 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:48:06.311404 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:48:06.316073 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:48:06.317795 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m14:48:06.327071 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:48:06.327638 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m14:48:06.399685 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m14:48:06.400929 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:48:06.401768 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:48:06.405750 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:48:06.406835 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: BEGIN
[0m14:48:06.407678 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:48:06.408243 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:48:06.408950 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:48:06.409601 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m14:48:06.410176 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m14:48:06.411022 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:48:06.411730 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:48:06.412447 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:48:06.413626 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m14:48:06.414327 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:48:06.495557 [debug] [Thread-1 (]: SQL status: BEGIN in 0.086 seconds
[0m14:48:06.496138 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:48:06.496682 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_slq"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_slq__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with base as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
),

sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,
        -- trajanje isporuke u danima
        extract(epoch from (delivered_at - shipment_date))/86400 as delivery_duration_days,
        -- kasnjenje
        case 
            when delivered_at > expected_delivery_date then true
            else false
        end as late_delivery
    from base
)

select * from sla;
  );
  
[0m14:48:06.497916 [debug] [Thread-1 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 44: select * from sla;
                          ^

[0m14:48:06.498459 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: ROLLBACK
[0m14:48:06.499313 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: Close
[0m14:48:06.513685 [debug] [Thread-1 (]: Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:48:06.514504 [debug] [Thread-2 (]: SQL status: BEGIN in 0.104 seconds
[0m14:48:06.515210 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da51ae77-19b5-4e16-8580-b8ff0d7fd567', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001807959F3E0>]}
[0m14:48:06.515889 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:48:06.517576 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(shipment_id) as total_shipments,
    sum(case when not late_delivery then 1 else 0 end) as on_time_deliveries,
    sum(case when late_delivery then 1 else 0 end) as late_deliveries,
    avg(delivery_duration_days) as avg_delivery_days
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id;
  );
  
[0m14:48:06.516864 [error] [Thread-1 (]: 5 of 5 ERROR creating sql table model analytics.fct_shipments_slq .............. [[31mERROR[0m in 0.23s]
[0m14:48:06.518719 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:48:06.519259 [debug] [Thread-2 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 19: group by courier_id;
                            ^

[0m14:48:06.520280 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_slq' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql.
[0m14:48:06.520995 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m14:48:06.522991 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m14:48:06.532158 [debug] [Thread-2 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  syntax error at or near ";"
  LINE 19: group by courier_id;
                              ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:48:06.533072 [debug] [Thread-3 (]: SQL status: BEGIN in 0.121 seconds
[0m14:48:06.533844 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da51ae77-19b5-4e16-8580-b8ff0d7fd567', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180791B9EF0>]}
[0m14:48:06.534664 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:48:06.536509 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m14:48:06.535787 [error] [Thread-2 (]: 2 of 5 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.25s]
[0m14:48:06.537651 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m14:48:06.538457 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  syntax error at or near ";"
  LINE 19: group by courier_id;
                              ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m14:48:06.544875 [debug] [Thread-4 (]: SQL status: BEGIN in 0.131 seconds
[0m14:48:06.545566 [debug] [Thread-3 (]: SQL status: SELECT 9 in 0.008 seconds
[0m14:48:06.546162 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:48:06.551494 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:48:06.552154 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    shipment_date,
    courier_id,
    sum(price) as total_revenue,
    count(shipment_id) as total_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by shipment_date, courier_id;
  );
  
[0m14:48:06.552687 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m14:48:06.553904 [debug] [Thread-4 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 20: group by shipment_date, courier_id;
                                           ^

[0m14:48:06.554371 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:48:06.554809 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: ROLLBACK
[0m14:48:06.558274 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:48:06.558932 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m14:48:06.559602 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m14:48:06.561247 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:48:06.563397 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:48:06.564213 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:48:06.564797 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:48:06.567221 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m14:48:06.570487 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m14:48:06.571439 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:48:06.574076 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m14:48:06.575178 [debug] [Thread-4 (]: Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 20: group by shipment_date, courier_id;
                                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:48:06.575764 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da51ae77-19b5-4e16-8580-b8ff0d7fd567', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018079A24550>]}
[0m14:48:06.576481 [error] [Thread-4 (]: 4 of 5 ERROR creating sql table model analytics.fct_shipments_revenue .......... [[31mERROR[0m in 0.29s]
[0m14:48:06.577425 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:48:06.578065 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_revenue' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 20: group by shipment_date, courier_id;
                                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql.
[0m14:48:06.579521 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.005 seconds
[0m14:48:06.581179 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m14:48:06.582173 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da51ae77-19b5-4e16-8580-b8ff0d7fd567', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180799AE7B0>]}
[0m14:48:06.583395 [info ] [Thread-3 (]: 3 of 5 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.29s]
[0m14:48:06.584400 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:48:06.586400 [debug] [MainThread]: Using postgres connection "master"
[0m14:48:06.586810 [debug] [MainThread]: On master: BEGIN
[0m14:48:06.587168 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:48:06.661379 [debug] [MainThread]: SQL status: BEGIN in 0.074 seconds
[0m14:48:06.661991 [debug] [MainThread]: On master: COMMIT
[0m14:48:06.662427 [debug] [MainThread]: Using postgres connection "master"
[0m14:48:06.662835 [debug] [MainThread]: On master: COMMIT
[0m14:48:06.663730 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:48:06.664336 [debug] [MainThread]: On master: Close
[0m14:48:06.665180 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:48:06.665620 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m14:48:06.665973 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m14:48:06.666305 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_slq' was properly closed.
[0m14:48:06.666600 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m14:48:06.666922 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_revenue' was properly closed.
[0m14:48:06.667246 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m14:48:06.667700 [info ] [MainThread]: 
[0m14:48:06.668393 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 1.34 seconds (1.34s).
[0m14:48:06.670397 [debug] [MainThread]: Command end result
[0m14:48:06.699776 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:48:06.704432 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:48:06.713009 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m14:48:06.713590 [info ] [MainThread]: 
[0m14:48:06.714429 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m14:48:06.715292 [info ] [MainThread]: 
[0m14:48:06.716011 [error] [MainThread]: [31mFailure in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)[0m
[0m14:48:06.716725 [error] [MainThread]:   Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:48:06.717331 [info ] [MainThread]: 
[0m14:48:06.718013 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:48:06.718633 [info ] [MainThread]: 
[0m14:48:06.719425 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m14:48:06.720682 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  syntax error at or near ";"
  LINE 19: group by courier_id;
                              ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:48:06.721547 [info ] [MainThread]: 
[0m14:48:06.722274 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:48:06.723201 [info ] [MainThread]: 
[0m14:48:06.724077 [error] [MainThread]: [31mFailure in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)[0m
[0m14:48:06.724975 [error] [MainThread]:   Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 20: group by shipment_date, courier_id;
                                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:48:06.726186 [info ] [MainThread]: 
[0m14:48:06.727161 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:48:06.727918 [info ] [MainThread]: 
[0m14:48:06.728651 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=5
[0m14:48:06.731532 [debug] [MainThread]: Command `dbt run` failed at 14:48:06.731362 after 3.09 seconds
[0m14:48:06.732066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018077B03590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180799CB5F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180799F3CB0>]}
[0m14:48:06.732582 [debug] [MainThread]: Flushing usage events
[0m14:48:07.890207 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:50:46.854349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FC6B0980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FD16CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FD7ABD90>]}


============================== 14:50:46.861608 | 962452bc-04ab-4bcf-bc5d-d0f894629020 ==============================
[0m14:50:46.861608 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:50:46.863499 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run --select marts.analytics', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:50:47.211757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '962452bc-04ab-4bcf-bc5d-d0f894629020', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FCB18770>]}
[0m14:50:47.325331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '962452bc-04ab-4bcf-bc5d-d0f894629020', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FD64EE00>]}
[0m14:50:47.328142 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:50:47.968899 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:50:48.194737 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m14:50:48.195503 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\schema.yml
[0m14:50:48.437845 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values`. Arguments to generic tests
should be nested under the `arguments` property.`
[0m14:50:48.438750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '962452bc-04ab-4bcf-bc5d-d0f894629020', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FF194B50>]}
[0m14:50:48.543986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '962452bc-04ab-4bcf-bc5d-d0f894629020', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FF0BF6B0>]}
[0m14:50:48.637925 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:50:48.641615 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:50:48.761857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '962452bc-04ab-4bcf-bc5d-d0f894629020', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FEEDC050>]}
[0m14:50:48.762458 [info ] [MainThread]: Found 13 models, 5 data tests, 5 sources, 459 macros
[0m14:50:48.763158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '962452bc-04ab-4bcf-bc5d-d0f894629020', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FF3C6680>]}
[0m14:50:48.765611 [info ] [MainThread]: 
[0m14:50:48.766089 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:50:48.766559 [info ] [MainThread]: 
[0m14:50:48.767389 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:50:48.771463 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m14:50:48.927406 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m14:50:48.928256 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m14:50:48.928882 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:50:49.071807 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.143 seconds
[0m14:50:49.073390 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m14:50:49.075968 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m14:50:49.081833 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:50:49.082312 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m14:50:49.082635 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:50:49.155979 [debug] [ThreadPool]: SQL status: BEGIN in 0.073 seconds
[0m14:50:49.156461 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:50:49.156784 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m14:50:49.165874 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.009 seconds
[0m14:50:49.167538 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m14:50:49.168071 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m14:50:49.174826 [debug] [MainThread]: Using postgres connection "master"
[0m14:50:49.175322 [debug] [MainThread]: On master: BEGIN
[0m14:50:49.175614 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:50:49.248512 [debug] [MainThread]: SQL status: BEGIN in 0.073 seconds
[0m14:50:49.249025 [debug] [MainThread]: Using postgres connection "master"
[0m14:50:49.249470 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:50:49.258082 [debug] [MainThread]: SQL status: SELECT 5 in 0.008 seconds
[0m14:50:49.260044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '962452bc-04ab-4bcf-bc5d-d0f894629020', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FF698290>]}
[0m14:50:49.260514 [debug] [MainThread]: On master: ROLLBACK
[0m14:50:49.261082 [debug] [MainThread]: Using postgres connection "master"
[0m14:50:49.261397 [debug] [MainThread]: On master: BEGIN
[0m14:50:49.262113 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m14:50:49.262533 [debug] [MainThread]: On master: COMMIT
[0m14:50:49.262905 [debug] [MainThread]: Using postgres connection "master"
[0m14:50:49.263253 [debug] [MainThread]: On master: COMMIT
[0m14:50:49.263774 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:50:49.264166 [debug] [MainThread]: On master: Close
[0m14:50:49.268840 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m14:50:49.269457 [info ] [Thread-1 (]: 1 of 5 START sql table model analytics.fct_shipments ........................... [RUN]
[0m14:50:49.270324 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m14:50:49.270858 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m14:50:49.279364 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:50:49.281382 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m14:50:49.322048 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:50:49.324120 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:50:49.324700 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m14:50:49.325039 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:50:49.391935 [debug] [Thread-1 (]: SQL status: BEGIN in 0.067 seconds
[0m14:50:49.392505 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:50:49.392969 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."analytics"."stg_shipments"
),

shipment_status as (
    select *
    from "delivery_analytics"."analytics"."stg_shipment_status"
),

users as (
    select *
    from "delivery_analytics"."analytics"."dim_users"
),

couriers as (
    select *
    from "delivery_analytics"."analytics"."dim_couriers"
)

select
    s.shipment_id,
    s.sender_id,
    su.username as sender_name,
    s.receiver_id,
    ru.username as receiver_name,
    s.shipment_date,
    s.expected_delivery_date,
    ss.status,
    ss.status_timestamp,
    ss.courier_id,
    c.user_id as courier_user_id,
    c.vehicle_type,
    s.price
from shipments s
left join shipment_status ss on s.shipment_id = ss.shipment_id
left join users su on s.sender_id = su.user_id
left join users ru on s.receiver_id = ru.user_id
left join couriers c on ss.courier_id = c.courier_id
  );
  
[0m14:50:49.403849 [debug] [Thread-1 (]: SQL status: SELECT 10 in 0.010 seconds
[0m14:50:49.415232 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:50:49.415810 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m14:50:49.416760 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:50:49.420028 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:50:49.420535 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m14:50:49.421806 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:50:49.437294 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:50:49.437744 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:50:49.438073 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:50:49.439613 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m14:50:49.446076 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m14:50:49.451180 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:50:49.451671 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m14:50:49.456848 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.005 seconds
[0m14:50:49.459300 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m14:50:49.461607 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '962452bc-04ab-4bcf-bc5d-d0f894629020', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FF68D180>]}
[0m14:50:49.462422 [info ] [Thread-1 (]: 1 of 5 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 10[0m in 0.19s]
[0m14:50:49.463519 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m14:50:49.464495 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:50:49.465003 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:50:49.465433 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:50:49.466392 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m14:50:49.465974 [info ] [Thread-1 (]: 5 of 5 START sql table model analytics.fct_shipments_slq ....................... [RUN]
[0m14:50:49.467123 [info ] [Thread-3 (]: 3 of 5 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m14:50:49.467783 [info ] [Thread-4 (]: 4 of 5 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m14:50:49.469000 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.fct_shipments_slq)
[0m14:50:49.468281 [info ] [Thread-2 (]: 2 of 5 START sql table model analytics.courier_performance ..................... [RUN]
[0m14:50:49.469832 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m14:50:49.470446 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m14:50:49.470988 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:50:49.471705 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m14:50:49.472252 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m14:50:49.472676 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:50:49.476104 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:50:49.476672 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m14:50:49.479264 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:50:49.482032 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:50:49.484958 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m14:50:49.486968 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:50:49.491429 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:50:49.492439 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m14:50:49.493035 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:50:49.493554 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m14:50:49.497099 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m14:50:49.500871 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:50:49.504979 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:50:49.505512 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:50:49.506277 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: BEGIN
[0m14:50:49.506763 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:49.507964 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:50:49.508595 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:50:49.509083 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m14:50:49.509504 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m14:50:49.509982 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:50:49.510511 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:50:49.510960 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:50:49.511818 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m14:50:49.512656 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:50:49.577292 [debug] [Thread-1 (]: SQL status: BEGIN in 0.070 seconds
[0m14:50:49.577868 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_slq"
[0m14:50:49.578328 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_slq"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_slq__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with base as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
),

sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,
        -- trajanje isporuke u danima
        extract(epoch from (delivered_at - shipment_date))/86400 as delivery_duration_days,
        -- kasnjenje
        case 
            when delivered_at > expected_delivery_date then true
            else false
        end as late_delivery
    from base
)

select * from sla;
  );
  
[0m14:50:49.579321 [debug] [Thread-1 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 44: select * from sla;
                          ^

[0m14:50:49.579776 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: ROLLBACK
[0m14:50:49.580473 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_slq: Close
[0m14:50:49.589853 [debug] [Thread-1 (]: Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:50:49.590531 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '962452bc-04ab-4bcf-bc5d-d0f894629020', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FF7C1450>]}
[0m14:50:49.591209 [error] [Thread-1 (]: 5 of 5 ERROR creating sql table model analytics.fct_shipments_slq .............. [[31mERROR[0m in 0.12s]
[0m14:50:49.592214 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_slq
[0m14:50:49.592947 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_slq' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql.
[0m14:50:49.594028 [debug] [Thread-2 (]: SQL status: BEGIN in 0.084 seconds
[0m14:50:49.594400 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:50:49.594757 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(shipment_id) as total_shipments,
    sum(case when not late_delivery then 1 else 0 end) as on_time_deliveries,
    sum(case when late_delivery then 1 else 0 end) as late_deliveries,
    avg(delivery_duration_days) as avg_delivery_days
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id;
  );
  
[0m14:50:49.595638 [debug] [Thread-2 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 19: group by courier_id;
                            ^

[0m14:50:49.596103 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m14:50:49.596825 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m14:50:49.604050 [debug] [Thread-2 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  syntax error at or near ";"
  LINE 19: group by courier_id;
                              ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:50:49.604775 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '962452bc-04ab-4bcf-bc5d-d0f894629020', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FEE1D550>]}
[0m14:50:49.605534 [error] [Thread-2 (]: 2 of 5 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.13s]
[0m14:50:49.606422 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m14:50:49.607087 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  syntax error at or near ";"
  LINE 19: group by courier_id;
                              ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m14:50:49.651594 [debug] [Thread-4 (]: SQL status: BEGIN in 0.141 seconds
[0m14:50:49.652222 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:50:49.652735 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    shipment_date,
    courier_id,
    sum(price) as total_revenue,
    count(shipment_id) as total_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by shipment_date, courier_id;
  );
  
[0m14:50:49.653715 [debug] [Thread-4 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 20: group by shipment_date, courier_id;
                                           ^

[0m14:50:49.654125 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: ROLLBACK
[0m14:50:49.654827 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m14:50:49.660816 [debug] [Thread-4 (]: Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 20: group by shipment_date, courier_id;
                                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:50:49.661446 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '962452bc-04ab-4bcf-bc5d-d0f894629020', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FF739FD0>]}
[0m14:50:49.662080 [error] [Thread-4 (]: 4 of 5 ERROR creating sql table model analytics.fct_shipments_revenue .......... [[31mERROR[0m in 0.19s]
[0m14:50:49.663024 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:50:49.663699 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_revenue' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 20: group by shipment_date, courier_id;
                                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql.
[0m14:50:49.719793 [debug] [Thread-3 (]: SQL status: BEGIN in 0.207 seconds
[0m14:50:49.720439 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:50:49.720843 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m14:50:49.726579 [debug] [Thread-3 (]: SQL status: SELECT 9 in 0.005 seconds
[0m14:50:49.730223 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:50:49.730760 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m14:50:49.731862 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:50:49.734610 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:50:49.735021 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m14:50:49.736361 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:50:49.738198 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:50:49.738589 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:50:49.738921 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:50:49.740436 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m14:50:49.742761 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m14:50:49.743493 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:50:49.743947 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m14:50:49.754777 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.010 seconds
[0m14:50:49.756474 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m14:50:49.757193 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '962452bc-04ab-4bcf-bc5d-d0f894629020', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FF795010>]}
[0m14:50:49.757820 [info ] [Thread-3 (]: 3 of 5 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.29s]
[0m14:50:49.758651 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:50:49.760133 [debug] [MainThread]: Using postgres connection "master"
[0m14:50:49.760567 [debug] [MainThread]: On master: BEGIN
[0m14:50:49.761006 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:50:49.878939 [debug] [MainThread]: SQL status: BEGIN in 0.118 seconds
[0m14:50:49.879473 [debug] [MainThread]: On master: COMMIT
[0m14:50:49.879814 [debug] [MainThread]: Using postgres connection "master"
[0m14:50:49.880105 [debug] [MainThread]: On master: COMMIT
[0m14:50:49.880666 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:50:49.881074 [debug] [MainThread]: On master: Close
[0m14:50:49.881699 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:50:49.882002 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m14:50:49.882265 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m14:50:49.882635 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_slq' was properly closed.
[0m14:50:49.883083 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m14:50:49.883501 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_revenue' was properly closed.
[0m14:50:49.883917 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m14:50:49.884472 [info ] [MainThread]: 
[0m14:50:49.885095 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 1.12 seconds (1.12s).
[0m14:50:49.886754 [debug] [MainThread]: Command end result
[0m14:50:49.922196 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:50:49.927706 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:50:49.938316 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m14:50:49.939026 [info ] [MainThread]: 
[0m14:50:49.939674 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m14:50:49.940249 [info ] [MainThread]: 
[0m14:50:49.940855 [error] [MainThread]: [31mFailure in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)[0m
[0m14:50:49.941605 [error] [MainThread]:   Database Error in model fct_shipments_slq (models\marts\analytics\fct_shipments_slq.sql)
  syntax error at or near ";"
  LINE 44: select * from sla;
                            ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:50:49.942170 [info ] [MainThread]: 
[0m14:50:49.942804 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_slq.sql
[0m14:50:49.943320 [info ] [MainThread]: 
[0m14:50:49.944030 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m14:50:49.944849 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  syntax error at or near ";"
  LINE 19: group by courier_id;
                              ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:50:49.945700 [info ] [MainThread]: 
[0m14:50:49.946819 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:50:49.951290 [info ] [MainThread]: 
[0m14:50:49.952782 [error] [MainThread]: [31mFailure in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)[0m
[0m14:50:49.954363 [error] [MainThread]:   Database Error in model fct_shipments_revenue (models\marts\analytics\fct_shipments_revenue.sql)
  syntax error at or near ";"
  LINE 20: group by shipment_date, courier_id;
                                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:50:49.954994 [info ] [MainThread]: 
[0m14:50:49.955666 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_revenue.sql
[0m14:50:49.956302 [info ] [MainThread]: 
[0m14:50:49.956933 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=5
[0m14:50:49.957837 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:50:49.959735 [debug] [MainThread]: Command `dbt run` failed at 14:50:49.959538 after 3.45 seconds
[0m14:50:49.960277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FF29C770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FC2CFA70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133FF113F50>]}
[0m14:50:49.960776 [debug] [MainThread]: Flushing usage events
[0m14:50:50.999070 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:53:58.633388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267AE9D0980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267AF47CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267AFACBD90>]}


============================== 14:53:58.639130 | 4be5890d-a9dc-4f0a-b4cb-04a66524913e ==============================
[0m14:53:58.639130 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:53:58.640236 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run --select marts.analytics', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:53:58.908345 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4be5890d-a9dc-4f0a-b4cb-04a66524913e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267AEE38770>]}
[0m14:53:58.986815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4be5890d-a9dc-4f0a-b4cb-04a66524913e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267AF96EE00>]}
[0m14:53:58.988548 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:53:59.475376 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:53:59.681191 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 2 files changed.
[0m14:53:59.682203 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\analytics\fct_shipments_sla.sql
[0m14:53:59.683564 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\analytics\fct_shipments_slq.sql
[0m14:53:59.684742 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\fct_shipments_revenue.sql
[0m14:53:59.685448 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\courier_performance.sql
[0m14:54:00.009852 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4be5890d-a9dc-4f0a-b4cb-04a66524913e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267B149DA50>]}
[0m14:54:00.121406 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:54:00.125351 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:54:00.230820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4be5890d-a9dc-4f0a-b4cb-04a66524913e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267AF95FD40>]}
[0m14:54:00.231408 [info ] [MainThread]: Found 13 models, 5 data tests, 5 sources, 459 macros
[0m14:54:00.232306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4be5890d-a9dc-4f0a-b4cb-04a66524913e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267B118ADD0>]}
[0m14:54:00.235519 [info ] [MainThread]: 
[0m14:54:00.236112 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:54:00.236732 [info ] [MainThread]: 
[0m14:54:00.237520 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:54:00.241728 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m14:54:00.399198 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m14:54:00.399665 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m14:54:00.400005 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:54:00.547146 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.147 seconds
[0m14:54:00.549442 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m14:54:00.552422 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m14:54:00.567412 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:54:00.568173 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m14:54:00.568811 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:54:00.643806 [debug] [ThreadPool]: SQL status: BEGIN in 0.075 seconds
[0m14:54:00.644307 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m14:54:00.644769 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m14:54:00.653560 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.008 seconds
[0m14:54:00.655182 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m14:54:00.655730 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m14:54:00.663489 [debug] [MainThread]: Using postgres connection "master"
[0m14:54:00.663932 [debug] [MainThread]: On master: BEGIN
[0m14:54:00.664203 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:54:00.739707 [debug] [MainThread]: SQL status: BEGIN in 0.075 seconds
[0m14:54:00.740293 [debug] [MainThread]: Using postgres connection "master"
[0m14:54:00.740643 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:54:00.750311 [debug] [MainThread]: SQL status: SELECT 5 in 0.009 seconds
[0m14:54:00.752281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4be5890d-a9dc-4f0a-b4cb-04a66524913e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267B1793EE0>]}
[0m14:54:00.752735 [debug] [MainThread]: On master: ROLLBACK
[0m14:54:00.753358 [debug] [MainThread]: Using postgres connection "master"
[0m14:54:00.753737 [debug] [MainThread]: On master: BEGIN
[0m14:54:00.754494 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m14:54:00.754873 [debug] [MainThread]: On master: COMMIT
[0m14:54:00.755275 [debug] [MainThread]: Using postgres connection "master"
[0m14:54:00.755729 [debug] [MainThread]: On master: COMMIT
[0m14:54:00.756280 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:54:00.756667 [debug] [MainThread]: On master: Close
[0m14:54:00.761386 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m14:54:00.762023 [info ] [Thread-1 (]: 1 of 5 START sql table model analytics.fct_shipments ........................... [RUN]
[0m14:54:00.762979 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m14:54:00.763475 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m14:54:00.771470 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:54:00.774380 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m14:54:00.814546 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:54:00.816684 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:54:00.817103 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m14:54:00.817445 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:54:00.881620 [debug] [Thread-1 (]: SQL status: BEGIN in 0.064 seconds
[0m14:54:00.882120 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:54:00.882513 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."analytics"."stg_shipments"
),

shipment_status as (
    select *
    from "delivery_analytics"."analytics"."stg_shipment_status"
),

users as (
    select *
    from "delivery_analytics"."analytics"."dim_users"
),

couriers as (
    select *
    from "delivery_analytics"."analytics"."dim_couriers"
)

select
    s.shipment_id,
    s.sender_id,
    su.username as sender_name,
    s.receiver_id,
    ru.username as receiver_name,
    s.shipment_date,
    s.expected_delivery_date,
    ss.status,
    ss.status_timestamp,
    ss.courier_id,
    c.user_id as courier_user_id,
    c.vehicle_type,
    s.price
from shipments s
left join shipment_status ss on s.shipment_id = ss.shipment_id
left join users su on s.sender_id = su.user_id
left join users ru on s.receiver_id = ru.user_id
left join couriers c on ss.courier_id = c.courier_id
  );
  
[0m14:54:00.893862 [debug] [Thread-1 (]: SQL status: SELECT 10 in 0.011 seconds
[0m14:54:00.905083 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:54:00.905528 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m14:54:00.906465 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:54:00.908873 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:54:00.909231 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m14:54:00.910593 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:54:00.926503 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:54:00.927167 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:54:00.927619 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:54:00.929549 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m14:54:00.936124 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m14:54:00.940754 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:54:00.941135 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m14:54:00.945897 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.004 seconds
[0m14:54:00.948415 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m14:54:00.950751 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4be5890d-a9dc-4f0a-b4cb-04a66524913e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267B19B8950>]}
[0m14:54:00.951639 [info ] [Thread-1 (]: 1 of 5 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 10[0m in 0.19s]
[0m14:54:00.952761 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m14:54:00.953651 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m14:54:00.954122 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:54:00.954543 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments_sla
[0m14:54:00.954983 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:54:00.955519 [info ] [Thread-2 (]: 2 of 5 START sql table model analytics.courier_performance ..................... [RUN]
[0m14:54:00.956087 [info ] [Thread-3 (]: 3 of 5 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m14:54:00.958068 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m14:54:00.956709 [info ] [Thread-1 (]: 5 of 5 START sql table model analytics.fct_shipments_sla ....................... [RUN]
[0m14:54:00.958795 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m14:54:00.959658 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m14:54:00.957280 [info ] [Thread-4 (]: 4 of 5 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m14:54:00.960491 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.fct_shipments_sla)
[0m14:54:00.960980 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m14:54:00.963950 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m14:54:00.965041 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m14:54:00.965774 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_sla
[0m14:54:00.969269 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:54:00.970071 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:54:00.972962 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m14:54:00.975225 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:54:00.976103 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m14:54:00.981950 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m14:54:00.983257 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m14:54:00.988106 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:54:00.988739 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:54:00.989384 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_sla
[0m14:54:00.993725 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:54:00.996789 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m14:54:00.997844 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:54:00.998521 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:54:00.999117 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m14:54:00.999668 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m14:54:01.000207 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:54:01.000670 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:54:01.002131 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m14:54:01.002934 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: BEGIN
[0m14:54:01.003771 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:54:01.004287 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:54:01.004935 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m14:54:01.005668 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:54:01.082426 [debug] [Thread-2 (]: SQL status: BEGIN in 0.082 seconds
[0m14:54:01.083156 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m14:54:01.083712 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    avg(delivered_at::date - shipment_date::date) as avg_delivery_days
from fct
group by courier_id
  );
  
[0m14:54:01.085765 [debug] [Thread-2 (]: Postgres adapter: Postgres error: column "late_delivery" does not exist
LINE 20:     sum(case when late_delivery = false then 1 else 0 end) a...
                           ^

[0m14:54:01.086509 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m14:54:01.087490 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m14:54:01.100185 [debug] [Thread-2 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 20:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:54:01.100783 [debug] [Thread-3 (]: SQL status: BEGIN in 0.100 seconds
[0m14:54:01.101364 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4be5890d-a9dc-4f0a-b4cb-04a66524913e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267B1901F40>]}
[0m14:54:01.101986 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:54:01.103469 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m14:54:01.102959 [error] [Thread-2 (]: 2 of 5 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.14s]
[0m14:54:01.104528 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m14:54:01.105417 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 20:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m14:54:01.112873 [debug] [Thread-3 (]: SQL status: SELECT 9 in 0.009 seconds
[0m14:54:01.117973 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:54:01.118816 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m14:54:01.119452 [debug] [Thread-1 (]: SQL status: BEGIN in 0.115 seconds
[0m14:54:01.119874 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:54:01.120250 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m14:54:01.123320 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:54:01.123876 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,
        (delivered_at::date - expected_delivery_date::date) as delivery_delay_days,
        case
            when delivered_at::date > expected_delivery_date::date then true
            else false
        end as late_delivery
    from "delivery_analytics"."analytics"."fct_shipments"
)

select *
from sla
  );
  
[0m14:54:01.124461 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m14:54:01.125676 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "delivered_at" does not exist
LINE 28:         delivered_at,
                 ^

[0m14:54:01.126124 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:54:01.126490 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: ROLLBACK
[0m14:54:01.128003 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:54:01.128489 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:54:01.128966 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: Close
[0m14:54:01.129301 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m14:54:01.130996 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m14:54:01.134178 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m14:54:01.135114 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m14:54:01.135528 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m14:54:01.139828 [debug] [Thread-1 (]: Database Error in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)
  column "delivered_at" does not exist
  LINE 28:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql
[0m14:54:01.140789 [debug] [Thread-4 (]: SQL status: BEGIN in 0.135 seconds
[0m14:54:01.141455 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4be5890d-a9dc-4f0a-b4cb-04a66524913e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267B1A58550>]}
[0m14:54:01.141977 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:54:01.143153 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    courier_id,
    sum(price) as total_revenue
from fct
group by shipment_date, courier_id
  );
  
[0m14:54:01.142759 [error] [Thread-1 (]: 5 of 5 ERROR creating sql table model analytics.fct_shipments_sla .............. [[31mERROR[0m in 0.18s]
[0m14:54:01.143949 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_sla
[0m14:54:01.144504 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_sla' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)
  column "delivered_at" does not exist
  LINE 28:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql.
[0m14:54:01.147134 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.011 seconds
[0m14:54:01.149201 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m14:54:01.150442 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4be5890d-a9dc-4f0a-b4cb-04a66524913e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267B134A1D0>]}
[0m14:54:01.151593 [info ] [Thread-3 (]: 3 of 5 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.19s]
[0m14:54:01.152784 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m14:54:01.159159 [debug] [Thread-4 (]: SQL status: SELECT 6 in 0.015 seconds
[0m14:54:01.163530 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:54:01.163979 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp" rename to "fct_shipments_revenue"
[0m14:54:01.167359 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m14:54:01.169001 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m14:54:01.169415 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:54:01.169849 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m14:54:01.171375 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m14:54:01.173618 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup"
[0m14:54:01.174382 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m14:54:01.174783 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup" cascade
[0m14:54:01.175502 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.000 seconds
[0m14:54:01.176773 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m14:54:01.177379 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4be5890d-a9dc-4f0a-b4cb-04a66524913e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267B1A0C590>]}
[0m14:54:01.177974 [info ] [Thread-4 (]: 4 of 5 OK created sql table model analytics.fct_shipments_revenue .............. [[32mSELECT 6[0m in 0.21s]
[0m14:54:01.178907 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m14:54:01.180574 [debug] [MainThread]: Using postgres connection "master"
[0m14:54:01.181112 [debug] [MainThread]: On master: BEGIN
[0m14:54:01.181466 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:54:01.262264 [debug] [MainThread]: SQL status: BEGIN in 0.081 seconds
[0m14:54:01.262725 [debug] [MainThread]: On master: COMMIT
[0m14:54:01.263039 [debug] [MainThread]: Using postgres connection "master"
[0m14:54:01.263319 [debug] [MainThread]: On master: COMMIT
[0m14:54:01.263807 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:54:01.264222 [debug] [MainThread]: On master: Close
[0m14:54:01.264818 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:54:01.265129 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m14:54:01.265475 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m14:54:01.265853 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_sla' was properly closed.
[0m14:54:01.266232 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m14:54:01.266543 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m14:54:01.266801 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_revenue' was properly closed.
[0m14:54:01.267200 [info ] [MainThread]: 
[0m14:54:01.267768 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 1.03 seconds (1.03s).
[0m14:54:01.269588 [debug] [MainThread]: Command end result
[0m14:54:01.302013 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:54:01.306231 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:54:01.313439 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m14:54:01.313894 [info ] [MainThread]: 
[0m14:54:01.314624 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m14:54:01.315397 [info ] [MainThread]: 
[0m14:54:01.316007 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m14:54:01.316769 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 20:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:54:01.317511 [info ] [MainThread]: 
[0m14:54:01.318240 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m14:54:01.318919 [info ] [MainThread]: 
[0m14:54:01.319524 [error] [MainThread]: [31mFailure in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)[0m
[0m14:54:01.320076 [error] [MainThread]:   Database Error in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)
  column "delivered_at" does not exist
  LINE 28:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql
[0m14:54:01.320769 [info ] [MainThread]: 
[0m14:54:01.321397 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql
[0m14:54:01.321914 [info ] [MainThread]: 
[0m14:54:01.322459 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=5
[0m14:54:01.323847 [debug] [MainThread]: Command `dbt run` failed at 14:54:01.323722 after 2.97 seconds
[0m14:54:01.324241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267AEC7BD70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267B1A96510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000267B13A4A70>]}
[0m14:54:01.324644 [debug] [MainThread]: Flushing usage events
[0m14:54:02.387225 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:25:27.833326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016781314980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016781DACB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000167823FBD90>]}


============================== 10:25:27.868507 | 1a1f3fe9-6a0e-4296-93bd-f40bf575030a ==============================
[0m10:25:27.868507 [info ] [MainThread]: Running with dbt=1.10.15
[0m10:25:27.869886 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt run --select marts.analytics', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m10:25:28.253558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1a1f3fe9-6a0e-4296-93bd-f40bf575030a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016781778770>]}
[0m10:25:28.371174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1a1f3fe9-6a0e-4296-93bd-f40bf575030a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001678229EE00>]}
[0m10:25:28.385498 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:25:28.971060 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m10:25:29.266703 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:25:29.267220 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:25:29.325406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1a1f3fe9-6a0e-4296-93bd-f40bf575030a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016783D05650>]}
[0m10:25:29.448551 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:25:29.453787 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:25:29.506765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1a1f3fe9-6a0e-4296-93bd-f40bf575030a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001678228FD40>]}
[0m10:25:29.507715 [info ] [MainThread]: Found 13 models, 5 data tests, 5 sources, 459 macros
[0m10:25:29.508573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1a1f3fe9-6a0e-4296-93bd-f40bf575030a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000167838C7D90>]}
[0m10:25:29.511291 [info ] [MainThread]: 
[0m10:25:29.512002 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:25:29.512748 [info ] [MainThread]: 
[0m10:25:29.513498 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:25:29.518848 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m10:25:29.773921 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m10:25:29.774465 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m10:25:29.774872 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:25:30.257182 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.482 seconds
[0m10:25:30.258923 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m10:25:30.262987 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m10:25:30.272741 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:25:30.273248 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m10:25:30.273725 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:25:30.345160 [debug] [ThreadPool]: SQL status: BEGIN in 0.071 seconds
[0m10:25:30.346140 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:25:30.346683 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m10:25:30.392855 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.046 seconds
[0m10:25:30.395007 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m10:25:30.396597 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m10:25:30.407710 [debug] [MainThread]: Using postgres connection "master"
[0m10:25:30.408477 [debug] [MainThread]: On master: BEGIN
[0m10:25:30.408997 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:25:30.474792 [debug] [MainThread]: SQL status: BEGIN in 0.066 seconds
[0m10:25:30.475423 [debug] [MainThread]: Using postgres connection "master"
[0m10:25:30.475985 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:25:30.513710 [debug] [MainThread]: SQL status: SELECT 5 in 0.037 seconds
[0m10:25:30.515858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1a1f3fe9-6a0e-4296-93bd-f40bf575030a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001678400CFC0>]}
[0m10:25:30.516760 [debug] [MainThread]: On master: ROLLBACK
[0m10:25:30.517797 [debug] [MainThread]: Using postgres connection "master"
[0m10:25:30.518429 [debug] [MainThread]: On master: BEGIN
[0m10:25:30.519408 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m10:25:30.520062 [debug] [MainThread]: On master: COMMIT
[0m10:25:30.520721 [debug] [MainThread]: Using postgres connection "master"
[0m10:25:30.521281 [debug] [MainThread]: On master: COMMIT
[0m10:25:30.522422 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m10:25:30.522937 [debug] [MainThread]: On master: Close
[0m10:25:30.530679 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m10:25:30.531416 [info ] [Thread-1 (]: 1 of 5 START sql table model analytics.fct_shipments ........................... [RUN]
[0m10:25:30.532589 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m10:25:30.533387 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m10:25:30.548920 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:25:30.552773 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m10:25:30.620009 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:25:30.622951 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:25:30.623792 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m10:25:30.624518 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:25:30.730368 [debug] [Thread-1 (]: SQL status: BEGIN in 0.106 seconds
[0m10:25:30.730990 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:25:30.731508 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."analytics"."stg_shipments"
),

shipment_status as (
    select *
    from "delivery_analytics"."analytics"."stg_shipment_status"
),

users as (
    select *
    from "delivery_analytics"."analytics"."dim_users"
),

couriers as (
    select *
    from "delivery_analytics"."analytics"."dim_couriers"
)

select
    s.shipment_id,
    s.sender_id,
    su.username as sender_name,
    s.receiver_id,
    ru.username as receiver_name,
    s.shipment_date,
    s.expected_delivery_date,
    ss.status,
    ss.status_timestamp,
    ss.courier_id,
    c.user_id as courier_user_id,
    c.vehicle_type,
    s.price
from shipments s
left join shipment_status ss on s.shipment_id = ss.shipment_id
left join users su on s.sender_id = su.user_id
left join users ru on s.receiver_id = ru.user_id
left join couriers c on ss.courier_id = c.courier_id
  );
  
[0m10:25:30.819406 [debug] [Thread-1 (]: SQL status: SELECT 10 in 0.087 seconds
[0m10:25:30.834310 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:25:30.835153 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m10:25:30.837465 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m10:25:30.841310 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:25:30.841829 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m10:25:30.843481 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:25:30.874538 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:25:30.875178 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:25:30.875627 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:25:30.877515 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m10:25:30.886249 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m10:25:30.898392 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:25:30.899136 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m10:25:30.914711 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.015 seconds
[0m10:25:30.918711 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m10:25:30.922756 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a1f3fe9-6a0e-4296-93bd-f40bf575030a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016783DD71D0>]}
[0m10:25:30.924000 [info ] [Thread-1 (]: 1 of 5 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 10[0m in 0.39s]
[0m10:25:30.925444 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m10:25:30.926589 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m10:25:30.927447 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:25:30.927989 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:25:30.928524 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m10:25:30.929198 [info ] [Thread-3 (]: 3 of 5 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m10:25:30.930079 [info ] [Thread-4 (]: 4 of 5 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m10:25:30.935234 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m10:25:30.931353 [info ] [Thread-1 (]: 5 of 5 START sql table model analytics.fct_shipments_sla ....................... [RUN]
[0m10:25:30.936246 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m10:25:30.933024 [info ] [Thread-2 (]: 2 of 5 START sql table model analytics.courier_performance ..................... [RUN]
[0m10:25:30.937126 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m10:25:30.937941 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.fct_shipments_sla)
[0m10:25:30.938556 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:25:30.939294 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m10:25:30.943245 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:25:30.944142 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:25:30.947439 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:25:30.948012 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m10:25:31.032160 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:25:31.036657 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m10:25:31.038411 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m10:25:31.042867 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:25:31.045507 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:25:31.046332 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m10:25:31.047111 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m10:25:31.061217 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:25:31.062153 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m10:25:31.068697 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:25:31.070538 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:25:31.075209 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m10:25:31.078836 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:25:31.082420 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:25:31.083339 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: BEGIN
[0m10:25:31.084309 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:25:31.085131 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:25:31.085891 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m10:25:31.086829 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:25:31.087869 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:25:31.088623 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m10:25:31.089327 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:25:31.134741 [debug] [Thread-3 (]: SQL status: BEGIN in 0.088 seconds
[0m10:25:31.135507 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:25:31.136263 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m10:25:31.152304 [debug] [Thread-3 (]: SQL status: SELECT 9 in 0.015 seconds
[0m10:25:31.157954 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:25:31.158726 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m10:25:31.160149 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:25:31.164838 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:25:31.165608 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m10:25:31.167178 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:25:31.169838 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m10:25:31.170351 [debug] [Thread-1 (]: SQL status: BEGIN in 0.085 seconds
[0m10:25:31.170840 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:25:31.171345 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:25:31.171853 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m10:25:31.172408 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,
        (delivered_at::date - expected_delivery_date::date) as delivery_delay_days,
        case
            when delivered_at::date > expected_delivery_date::date then true
            else false
        end as late_delivery
    from "delivery_analytics"."analytics"."fct_shipments"
)

select *
from sla
  );
  
[0m10:25:31.174107 [debug] [Thread-4 (]: SQL status: BEGIN in 0.087 seconds
[0m10:25:31.174728 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m10:25:31.175288 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:25:31.179102 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m10:25:31.180080 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "delivered_at" does not exist
LINE 28:         delivered_at,
                 ^

[0m10:25:31.180680 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    courier_id,
    sum(price) as total_revenue
from fct
group by shipment_date, courier_id
  );
  
[0m10:25:31.181961 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:25:31.182499 [debug] [Thread-2 (]: SQL status: BEGIN in 0.093 seconds
[0m10:25:31.182964 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: ROLLBACK
[0m10:25:31.183633 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m10:25:31.184338 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:25:31.185346 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    avg(delivered_at::date - shipment_date::date) as avg_delivery_days
from fct
group by courier_id
  );
  
[0m10:25:31.186019 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: Close
[0m10:25:31.188326 [debug] [Thread-2 (]: Postgres adapter: Postgres error: column "late_delivery" does not exist
LINE 20:     sum(case when late_delivery = false then 1 else 0 end) a...
                           ^

[0m10:25:31.189053 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m10:25:31.190106 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m10:25:31.296107 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.111 seconds
[0m10:25:31.299821 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m10:25:31.301602 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a1f3fe9-6a0e-4296-93bd-f40bf575030a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000167840F8AA0>]}
[0m10:25:31.303297 [info ] [Thread-3 (]: 3 of 5 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.37s]
[0m10:25:31.304748 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m10:25:31.310600 [debug] [Thread-4 (]: SQL status: SELECT 6 in 0.127 seconds
[0m10:25:31.315473 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:25:31.316074 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue" rename to "fct_shipments_revenue__dbt_backup"
[0m10:25:31.317730 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:25:31.322951 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:25:31.323537 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp" rename to "fct_shipments_revenue"
[0m10:25:31.325925 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m10:25:31.327894 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m10:25:31.328401 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:25:31.328853 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m10:25:31.421810 [debug] [Thread-4 (]: SQL status: COMMIT in 0.092 seconds
[0m10:25:31.428190 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup"
[0m10:25:31.429832 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:25:31.430676 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup" cascade
[0m10:25:31.447150 [debug] [Thread-2 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 20:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:25:31.447896 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.016 seconds
[0m10:25:31.448798 [debug] [Thread-1 (]: Database Error in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)
  column "delivered_at" does not exist
  LINE 28:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql
[0m10:25:31.449462 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a1f3fe9-6a0e-4296-93bd-f40bf575030a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000167FFCB4730>]}
[0m10:25:31.451436 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m10:25:31.452005 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a1f3fe9-6a0e-4296-93bd-f40bf575030a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000167841C69F0>]}
[0m10:25:31.454763 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1a1f3fe9-6a0e-4296-93bd-f40bf575030a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000167841FFF50>]}
[0m10:25:31.452870 [error] [Thread-2 (]: 2 of 5 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.51s]
[0m10:25:31.457012 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m10:25:31.453939 [error] [Thread-1 (]: 5 of 5 ERROR creating sql table model analytics.fct_shipments_sla .............. [[31mERROR[0m in 0.51s]
[0m10:25:31.458276 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:25:31.459456 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 20:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m10:25:31.455875 [info ] [Thread-4 (]: 4 of 5 OK created sql table model analytics.fct_shipments_revenue .............. [[32mSELECT 6[0m in 0.52s]
[0m10:25:31.461763 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_sla' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)
  column "delivered_at" does not exist
  LINE 28:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql.
[0m10:25:31.462639 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:25:31.465913 [debug] [MainThread]: Using postgres connection "master"
[0m10:25:31.466368 [debug] [MainThread]: On master: BEGIN
[0m10:25:31.466732 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:25:31.552445 [debug] [MainThread]: SQL status: BEGIN in 0.086 seconds
[0m10:25:31.553128 [debug] [MainThread]: On master: COMMIT
[0m10:25:31.553534 [debug] [MainThread]: Using postgres connection "master"
[0m10:25:31.554063 [debug] [MainThread]: On master: COMMIT
[0m10:25:31.554776 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:25:31.555185 [debug] [MainThread]: On master: Close
[0m10:25:31.556032 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:25:31.556561 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m10:25:31.557088 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m10:25:31.557578 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_sla' was properly closed.
[0m10:25:31.558028 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m10:25:31.558473 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_revenue' was properly closed.
[0m10:25:31.558916 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m10:25:31.559583 [info ] [MainThread]: 
[0m10:25:31.560680 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 2.05 seconds (2.05s).
[0m10:25:31.563612 [debug] [MainThread]: Command end result
[0m10:25:31.612256 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:25:31.619061 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:25:31.629945 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m10:25:31.630741 [info ] [MainThread]: 
[0m10:25:31.631518 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m10:25:31.632640 [info ] [MainThread]: 
[0m10:25:31.633598 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m10:25:31.634549 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 20:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:25:31.635261 [info ] [MainThread]: 
[0m10:25:31.636088 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:25:31.636807 [info ] [MainThread]: 
[0m10:25:31.637858 [error] [MainThread]: [31mFailure in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)[0m
[0m10:25:31.638732 [error] [MainThread]:   Database Error in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)
  column "delivered_at" does not exist
  LINE 28:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql
[0m10:25:31.639561 [info ] [MainThread]: 
[0m10:25:31.640399 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql
[0m10:25:31.641058 [info ] [MainThread]: 
[0m10:25:31.641888 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=5
[0m10:25:31.643826 [debug] [MainThread]: Command `dbt run` failed at 10:25:31.643556 after 4.04 seconds
[0m10:25:31.644687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016781653E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016781D713D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000167842CBDD0>]}
[0m10:25:31.645521 [debug] [MainThread]: Flushing usage events
[0m10:25:33.042436 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:31:55.120495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A04F8980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A0FACB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A15E7D90>]}


============================== 10:31:55.126700 | 0eec2cae-5096-481d-8bbe-f1e2caf71925 ==============================
[0m10:31:55.126700 [info ] [MainThread]: Running with dbt=1.10.15
[0m10:31:55.128156 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select marts.analytics', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m10:31:55.419053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0eec2cae-5096-481d-8bbe-f1e2caf71925', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A0958770>]}
[0m10:31:55.500774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0eec2cae-5096-481d-8bbe-f1e2caf71925', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A148EE00>]}
[0m10:31:55.502083 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:31:55.918667 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m10:31:56.115927 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:31:56.116878 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\fct_shipments_revenue.sql
[0m10:31:56.517913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0eec2cae-5096-481d-8bbe-f1e2caf71925', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A2FA5B50>]}
[0m10:31:56.646578 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:31:56.650722 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:31:56.755144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0eec2cae-5096-481d-8bbe-f1e2caf71925', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A147FD40>]}
[0m10:31:56.756155 [info ] [MainThread]: Found 13 models, 5 data tests, 5 sources, 459 macros
[0m10:31:56.757255 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0eec2cae-5096-481d-8bbe-f1e2caf71925', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A2FB6F90>]}
[0m10:31:56.761614 [info ] [MainThread]: 
[0m10:31:56.762527 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:31:56.764635 [info ] [MainThread]: 
[0m10:31:56.765761 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:31:56.772349 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m10:31:56.911883 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m10:31:56.912527 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m10:31:56.912904 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:31:57.050257 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.137 seconds
[0m10:31:57.052466 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m10:31:57.056224 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m10:31:57.064218 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:31:57.064762 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m10:31:57.065137 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:31:57.129824 [debug] [ThreadPool]: SQL status: BEGIN in 0.065 seconds
[0m10:31:57.130441 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:31:57.130872 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m10:31:57.139065 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.008 seconds
[0m10:31:57.140884 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m10:31:57.141980 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m10:31:57.151326 [debug] [MainThread]: Using postgres connection "master"
[0m10:31:57.151821 [debug] [MainThread]: On master: BEGIN
[0m10:31:57.152169 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:31:57.214543 [debug] [MainThread]: SQL status: BEGIN in 0.062 seconds
[0m10:31:57.215276 [debug] [MainThread]: Using postgres connection "master"
[0m10:31:57.216105 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:31:57.225976 [debug] [MainThread]: SQL status: SELECT 5 in 0.009 seconds
[0m10:31:57.228275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0eec2cae-5096-481d-8bbe-f1e2caf71925', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A32BB6C0>]}
[0m10:31:57.228845 [debug] [MainThread]: On master: ROLLBACK
[0m10:31:57.229608 [debug] [MainThread]: Using postgres connection "master"
[0m10:31:57.230016 [debug] [MainThread]: On master: BEGIN
[0m10:31:57.231300 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m10:31:57.231946 [debug] [MainThread]: On master: COMMIT
[0m10:31:57.232323 [debug] [MainThread]: Using postgres connection "master"
[0m10:31:57.232693 [debug] [MainThread]: On master: COMMIT
[0m10:31:57.233323 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:31:57.233758 [debug] [MainThread]: On master: Close
[0m10:31:57.239481 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m10:31:57.240072 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:31:57.240886 [info ] [Thread-1 (]: 1 of 5 START sql table model analytics.fct_shipments ........................... [RUN]
[0m10:31:57.242446 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m10:31:57.243019 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m10:31:57.241604 [info ] [Thread-2 (]: 2 of 5 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m10:31:57.252571 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:31:57.253677 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m10:31:57.254894 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:31:57.258925 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:31:57.260159 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m10:31:57.312250 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:31:57.313887 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:31:57.318131 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:31:57.319503 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:31:57.320192 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m10:31:57.320841 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:31:57.321683 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:31:57.322195 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m10:31:57.322687 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:31:57.387889 [debug] [Thread-1 (]: SQL status: BEGIN in 0.067 seconds
[0m10:31:57.388633 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:31:57.389196 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."analytics"."stg_shipments"
),

shipment_status as (
    select *
    from "delivery_analytics"."analytics"."stg_shipment_status"
),

users as (
    select *
    from "delivery_analytics"."analytics"."dim_users"
),

couriers as (
    select *
    from "delivery_analytics"."analytics"."dim_couriers"
)

select
    s.shipment_id,
    s.sender_id,
    su.username as sender_name,
    s.receiver_id,
    ru.username as receiver_name,
    s.shipment_date,
    s.expected_delivery_date,
    ss.status,
    ss.status_timestamp,
    ss.courier_id,
    c.user_id as courier_user_id,
    c.vehicle_type,
    s.price
from shipments s
left join shipment_status ss on s.shipment_id = ss.shipment_id
left join users su on s.sender_id = su.user_id
left join users ru on s.receiver_id = ru.user_id
left join couriers c on ss.courier_id = c.courier_id
  );
  
[0m10:31:57.395198 [debug] [Thread-2 (]: SQL status: BEGIN in 0.072 seconds
[0m10:31:57.395703 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:31:57.396196 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    s.shipment_id,
    ca.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    ca.delivered_at,
    s.price,

    case
        when ca.delivered_at is null then null
        when ca.delivered_at::date > s.expected_delivery_date then true
        else false
    end as late_delivery

from "delivery_analytics"."raw"."shipments" s
left join "delivery_analytics"."raw"."courier_assignments" ca
    on s.shipment_id = ca.shipment_id
  );
  
[0m10:31:57.404527 [debug] [Thread-1 (]: SQL status: SELECT 10 in 0.015 seconds
[0m10:31:57.417386 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:31:57.417928 [debug] [Thread-2 (]: SQL status: SELECT 6 in 0.021 seconds
[0m10:31:57.418410 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m10:31:57.421814 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:31:57.422444 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue" rename to "fct_shipments_revenue__dbt_backup"
[0m10:31:57.423018 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:31:57.423552 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:31:57.426804 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:31:57.430366 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:31:57.430992 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m10:31:57.431706 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp" rename to "fct_shipments_revenue"
[0m10:31:57.433396 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:31:57.433930 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:31:57.454135 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:31:57.456213 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m10:31:57.456853 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:31:57.457447 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:31:57.458003 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:31:57.458603 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m10:31:57.460550 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m10:31:57.468208 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m10:31:57.468884 [debug] [Thread-2 (]: SQL status: COMMIT in 0.009 seconds
[0m10:31:57.477844 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:31:57.480837 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup"
[0m10:31:57.481472 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m10:31:57.482528 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:31:57.483227 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup" cascade
[0m10:31:57.489151 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.006 seconds
[0m10:31:57.492397 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m10:31:57.492935 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.009 seconds
[0m10:31:57.494701 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m10:31:57.497413 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0eec2cae-5096-481d-8bbe-f1e2caf71925', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A34FCA10>]}
[0m10:31:57.498045 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0eec2cae-5096-481d-8bbe-f1e2caf71925', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A33B8470>]}
[0m10:31:57.499130 [info ] [Thread-1 (]: 1 of 5 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 10[0m in 0.25s]
[0m10:31:57.501539 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m10:31:57.500127 [info ] [Thread-2 (]: 2 of 5 OK created sql table model analytics.fct_shipments_revenue .............. [[32mSELECT 6[0m in 0.24s]
[0m10:31:57.503038 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:31:57.503732 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m10:31:57.504315 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m10:31:57.505048 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:31:57.505813 [info ] [Thread-3 (]: 3 of 5 START sql table model analytics.courier_performance ..................... [RUN]
[0m10:31:57.508576 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m10:31:57.506547 [info ] [Thread-4 (]: 4 of 5 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m10:31:57.509571 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m10:31:57.507437 [info ] [Thread-1 (]: 5 of 5 START sql table model analytics.fct_shipments_sla ....................... [RUN]
[0m10:31:57.510638 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m10:31:57.514967 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m10:31:57.515914 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.fct_shipments_sla)
[0m10:31:57.516665 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m10:31:57.517514 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:31:57.522845 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:31:57.526232 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:31:57.528038 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m10:31:57.532786 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m10:31:57.534118 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m10:31:57.534664 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:31:57.539080 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:31:57.544095 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:31:57.545575 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:31:57.546165 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m10:31:57.546905 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m10:31:57.548376 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:31:57.549301 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:31:57.550343 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: BEGIN
[0m10:31:57.551154 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m10:31:57.551924 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:31:57.552486 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:31:57.664664 [debug] [Thread-4 (]: SQL status: BEGIN in 0.112 seconds
[0m10:31:57.665500 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:31:57.666235 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m10:31:57.682946 [debug] [Thread-4 (]: SQL status: SELECT 9 in 0.016 seconds
[0m10:31:57.688712 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:31:57.689454 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m10:31:57.690452 [debug] [Thread-3 (]: SQL status: BEGIN in 0.144 seconds
[0m10:31:57.691043 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:31:57.691616 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:31:57.696374 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:31:57.697064 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    avg(delivered_at::date - shipment_date::date) as avg_delivery_days
from fct
group by courier_id
  );
  
[0m10:31:57.697822 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m10:31:57.699158 [debug] [Thread-1 (]: SQL status: BEGIN in 0.147 seconds
[0m10:31:57.699822 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:31:57.700563 [debug] [Thread-3 (]: Postgres adapter: Postgres error: column "late_delivery" does not exist
LINE 20:     sum(case when late_delivery = false then 1 else 0 end) a...
                           ^

[0m10:31:57.701283 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m10:31:57.701997 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,
        (delivered_at::date - expected_delivery_date::date) as delivery_delay_days,
        case
            when delivered_at::date > expected_delivery_date::date then true
            else false
        end as late_delivery
    from "delivery_analytics"."analytics"."fct_shipments"
)

select *
from sla
  );
  
[0m10:31:57.702911 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m10:31:57.705390 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m10:31:57.706223 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:31:57.706866 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m10:31:57.707376 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m10:31:57.707940 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "delivered_at" does not exist
LINE 28:         delivered_at,
                 ^

[0m10:31:57.710230 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: ROLLBACK
[0m10:31:57.710859 [debug] [Thread-4 (]: SQL status: COMMIT in 0.002 seconds
[0m10:31:57.720855 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m10:31:57.721711 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: Close
[0m10:31:57.722911 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:31:57.723944 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m10:31:57.734548 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.009 seconds
[0m10:31:57.736936 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m10:31:57.738030 [debug] [Thread-1 (]: Database Error in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)
  column "delivered_at" does not exist
  LINE 28:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql
[0m10:31:57.738837 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0eec2cae-5096-481d-8bbe-f1e2caf71925', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A35A9090>]}
[0m10:31:57.742154 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0eec2cae-5096-481d-8bbe-f1e2caf71925', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A34EBAD0>]}
[0m10:31:57.744344 [debug] [Thread-3 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 20:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:31:57.743350 [info ] [Thread-4 (]: 4 of 5 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.23s]
[0m10:31:57.746079 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0eec2cae-5096-481d-8bbe-f1e2caf71925', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A354D630>]}
[0m10:31:57.745454 [error] [Thread-1 (]: 5 of 5 ERROR creating sql table model analytics.fct_shipments_sla .............. [[31mERROR[0m in 0.23s]
[0m10:31:57.747206 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m10:31:57.748852 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:31:57.748044 [error] [Thread-3 (]: 3 of 5 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.24s]
[0m10:31:57.750172 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_sla' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)
  column "delivered_at" does not exist
  LINE 28:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql.
[0m10:31:57.751058 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m10:31:57.753101 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 20:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m10:31:57.755643 [debug] [MainThread]: Using postgres connection "master"
[0m10:31:57.756066 [debug] [MainThread]: On master: BEGIN
[0m10:31:57.756433 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:31:57.822586 [debug] [MainThread]: SQL status: BEGIN in 0.066 seconds
[0m10:31:57.823386 [debug] [MainThread]: On master: COMMIT
[0m10:31:57.823966 [debug] [MainThread]: Using postgres connection "master"
[0m10:31:57.824359 [debug] [MainThread]: On master: COMMIT
[0m10:31:57.825030 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:31:57.825654 [debug] [MainThread]: On master: Close
[0m10:31:57.826384 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:31:57.826833 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m10:31:57.827225 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m10:31:57.827658 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_sla' was properly closed.
[0m10:31:57.827988 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_revenue' was properly closed.
[0m10:31:57.828369 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m10:31:57.828684 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m10:31:57.829165 [info ] [MainThread]: 
[0m10:31:57.829982 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 1.06 seconds (1.06s).
[0m10:31:57.832052 [debug] [MainThread]: Command end result
[0m10:31:57.865411 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:31:57.869029 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:31:57.877949 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m10:31:57.878526 [info ] [MainThread]: 
[0m10:31:57.879380 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m10:31:57.880186 [info ] [MainThread]: 
[0m10:31:57.881011 [error] [MainThread]: [31mFailure in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)[0m
[0m10:31:57.881856 [error] [MainThread]:   Database Error in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)
  column "delivered_at" does not exist
  LINE 28:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql
[0m10:31:57.882585 [info ] [MainThread]: 
[0m10:31:57.883458 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql
[0m10:31:57.884197 [info ] [MainThread]: 
[0m10:31:57.885064 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m10:31:57.885946 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 20:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:31:57.886647 [info ] [MainThread]: 
[0m10:31:57.887370 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:31:57.888074 [info ] [MainThread]: 
[0m10:31:57.888835 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=5
[0m10:31:57.890265 [debug] [MainThread]: Command `dbt run` failed at 10:31:57.890104 after 3.07 seconds
[0m10:31:57.890794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A01915B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A304FB30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8A14E2150>]}
[0m10:31:57.891263 [debug] [MainThread]: Flushing usage events
[0m10:31:59.090454 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:32:14.824642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BFF664980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B801FCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B8084BD90>]}


============================== 10:32:14.830602 | b288fe23-ddc9-466a-a6bd-8c69dd2604a1 ==============================
[0m10:32:14.830602 [info ] [MainThread]: Running with dbt=1.10.15
[0m10:32:14.831806 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run --select fct_shipments', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m10:32:15.070641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b288fe23-ddc9-466a-a6bd-8c69dd2604a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BFFAC8770>]}
[0m10:32:15.153488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b288fe23-ddc9-466a-a6bd-8c69dd2604a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B806EEE00>]}
[0m10:32:15.155044 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:32:15.562296 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m10:32:15.751470 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:32:15.752184 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:32:15.811006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b288fe23-ddc9-466a-a6bd-8c69dd2604a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B821F5650>]}
[0m10:32:15.915505 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:32:15.920476 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:32:15.953643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b288fe23-ddc9-466a-a6bd-8c69dd2604a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B806DFD40>]}
[0m10:32:15.954308 [info ] [MainThread]: Found 13 models, 5 data tests, 5 sources, 459 macros
[0m10:32:15.955046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b288fe23-ddc9-466a-a6bd-8c69dd2604a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B81DB7D90>]}
[0m10:32:15.957364 [info ] [MainThread]: 
[0m10:32:15.958087 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:32:15.958807 [info ] [MainThread]: 
[0m10:32:15.959773 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:32:15.961016 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m10:32:16.097535 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m10:32:16.098084 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m10:32:16.098460 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:32:16.227538 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.129 seconds
[0m10:32:16.229456 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m10:32:16.236697 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m10:32:16.243922 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:32:16.244492 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m10:32:16.244893 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:32:16.338400 [debug] [ThreadPool]: SQL status: BEGIN in 0.093 seconds
[0m10:32:16.339171 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:32:16.339683 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m10:32:16.347367 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.007 seconds
[0m10:32:16.349628 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m10:32:16.350366 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m10:32:16.360488 [debug] [MainThread]: Using postgres connection "master"
[0m10:32:16.361037 [debug] [MainThread]: On master: BEGIN
[0m10:32:16.361645 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:32:16.423652 [debug] [MainThread]: SQL status: BEGIN in 0.062 seconds
[0m10:32:16.424218 [debug] [MainThread]: Using postgres connection "master"
[0m10:32:16.424733 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:32:16.435967 [debug] [MainThread]: SQL status: SELECT 5 in 0.011 seconds
[0m10:32:16.438077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b288fe23-ddc9-466a-a6bd-8c69dd2604a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B824FCFC0>]}
[0m10:32:16.438643 [debug] [MainThread]: On master: ROLLBACK
[0m10:32:16.439398 [debug] [MainThread]: Using postgres connection "master"
[0m10:32:16.439802 [debug] [MainThread]: On master: BEGIN
[0m10:32:16.440780 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m10:32:16.441196 [debug] [MainThread]: On master: COMMIT
[0m10:32:16.441575 [debug] [MainThread]: Using postgres connection "master"
[0m10:32:16.441969 [debug] [MainThread]: On master: COMMIT
[0m10:32:16.442599 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:32:16.443026 [debug] [MainThread]: On master: Close
[0m10:32:16.449450 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m10:32:16.450326 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics.fct_shipments ........................... [RUN]
[0m10:32:16.451451 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m10:32:16.451945 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m10:32:16.462135 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:32:16.464580 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m10:32:16.516989 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:32:16.519340 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:32:16.519901 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m10:32:16.520403 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:32:16.588779 [debug] [Thread-1 (]: SQL status: BEGIN in 0.068 seconds
[0m10:32:16.589607 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:32:16.590306 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."analytics"."stg_shipments"
),

shipment_status as (
    select *
    from "delivery_analytics"."analytics"."stg_shipment_status"
),

users as (
    select *
    from "delivery_analytics"."analytics"."dim_users"
),

couriers as (
    select *
    from "delivery_analytics"."analytics"."dim_couriers"
)

select
    s.shipment_id,
    s.sender_id,
    su.username as sender_name,
    s.receiver_id,
    ru.username as receiver_name,
    s.shipment_date,
    s.expected_delivery_date,
    ss.status,
    ss.status_timestamp,
    ss.courier_id,
    c.user_id as courier_user_id,
    c.vehicle_type,
    s.price
from shipments s
left join shipment_status ss on s.shipment_id = ss.shipment_id
left join users su on s.sender_id = su.user_id
left join users ru on s.receiver_id = ru.user_id
left join couriers c on ss.courier_id = c.courier_id
  );
  
[0m10:32:16.600446 [debug] [Thread-1 (]: SQL status: SELECT 10 in 0.009 seconds
[0m10:32:16.613027 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:32:16.613586 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m10:32:16.614965 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:32:16.618703 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:32:16.619174 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m10:32:16.620622 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:32:16.639990 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:32:16.640500 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:32:16.640929 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:32:16.642667 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m10:32:16.650101 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m10:32:16.659665 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:32:16.660180 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m10:32:16.667642 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.007 seconds
[0m10:32:16.671130 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m10:32:16.673621 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b288fe23-ddc9-466a-a6bd-8c69dd2604a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B822C34D0>]}
[0m10:32:16.674448 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 10[0m in 0.22s]
[0m10:32:16.675683 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m10:32:16.677813 [debug] [MainThread]: Using postgres connection "master"
[0m10:32:16.678265 [debug] [MainThread]: On master: BEGIN
[0m10:32:16.678700 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:32:16.743132 [debug] [MainThread]: SQL status: BEGIN in 0.064 seconds
[0m10:32:16.743832 [debug] [MainThread]: On master: COMMIT
[0m10:32:16.744693 [debug] [MainThread]: Using postgres connection "master"
[0m10:32:16.745181 [debug] [MainThread]: On master: COMMIT
[0m10:32:16.745874 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:32:16.746342 [debug] [MainThread]: On master: Close
[0m10:32:16.746985 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:32:16.747383 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m10:32:16.747703 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m10:32:16.748219 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments' was properly closed.
[0m10:32:16.748709 [info ] [MainThread]: 
[0m10:32:16.749441 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.79 seconds (0.79s).
[0m10:32:16.750966 [debug] [MainThread]: Command end result
[0m10:32:16.855988 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:32:16.860283 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:32:16.869321 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m10:32:16.869835 [info ] [MainThread]: 
[0m10:32:16.870697 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:32:16.871492 [info ] [MainThread]: 
[0m10:32:16.872183 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m10:32:16.873583 [debug] [MainThread]: Command `dbt run` succeeded at 10:32:16.873419 after 2.26 seconds
[0m10:32:16.874074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B8230E830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025BFD279270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B81DBF1B0>]}
[0m10:32:16.874552 [debug] [MainThread]: Flushing usage events
[0m10:32:17.928051 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:32:51.809153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C639A68980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C63A4FCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C63AB4BD90>]}


============================== 10:32:51.846471 | 16f997c0-627c-4ab6-8389-f9794b20aca5 ==============================
[0m10:32:51.846471 [info ] [MainThread]: Running with dbt=1.10.15
[0m10:32:51.847859 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select courier_performance', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m10:32:52.098475 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '16f997c0-627c-4ab6-8389-f9794b20aca5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C639EC8770>]}
[0m10:32:52.180867 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '16f997c0-627c-4ab6-8389-f9794b20aca5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C63A9EEE00>]}
[0m10:32:52.182340 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:32:52.607086 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m10:32:52.828893 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:32:52.829927 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\courier_performance.sql
[0m10:32:53.208646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '16f997c0-627c-4ab6-8389-f9794b20aca5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C63C4C5B50>]}
[0m10:32:53.328844 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:32:53.332670 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:32:53.441704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '16f997c0-627c-4ab6-8389-f9794b20aca5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C63A9DFD40>]}
[0m10:32:53.442431 [info ] [MainThread]: Found 13 models, 5 data tests, 5 sources, 459 macros
[0m10:32:53.443267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '16f997c0-627c-4ab6-8389-f9794b20aca5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C63C4D6F90>]}
[0m10:32:53.445805 [info ] [MainThread]: 
[0m10:32:53.446719 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:32:53.447510 [info ] [MainThread]: 
[0m10:32:53.448492 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:32:53.449915 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m10:32:53.578585 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m10:32:53.579125 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m10:32:53.579514 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:32:53.722318 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.143 seconds
[0m10:32:53.724413 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m10:32:53.732132 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m10:32:53.742107 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:32:53.742781 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m10:32:53.743195 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:32:53.842555 [debug] [ThreadPool]: SQL status: BEGIN in 0.099 seconds
[0m10:32:53.843222 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:32:53.843627 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m10:32:53.853148 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.009 seconds
[0m10:32:53.855498 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m10:32:53.856512 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m10:32:53.866290 [debug] [MainThread]: Using postgres connection "master"
[0m10:32:53.866780 [debug] [MainThread]: On master: BEGIN
[0m10:32:53.867128 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:32:53.932114 [debug] [MainThread]: SQL status: BEGIN in 0.065 seconds
[0m10:32:53.932757 [debug] [MainThread]: Using postgres connection "master"
[0m10:32:53.933337 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:32:53.945801 [debug] [MainThread]: SQL status: SELECT 5 in 0.012 seconds
[0m10:32:53.948210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '16f997c0-627c-4ab6-8389-f9794b20aca5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C63C7DBEE0>]}
[0m10:32:53.948789 [debug] [MainThread]: On master: ROLLBACK
[0m10:32:53.949549 [debug] [MainThread]: Using postgres connection "master"
[0m10:32:53.949960 [debug] [MainThread]: On master: BEGIN
[0m10:32:53.950921 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m10:32:53.951329 [debug] [MainThread]: On master: COMMIT
[0m10:32:53.951695 [debug] [MainThread]: Using postgres connection "master"
[0m10:32:53.952219 [debug] [MainThread]: On master: COMMIT
[0m10:32:53.953008 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:32:53.953440 [debug] [MainThread]: On master: Close
[0m10:32:53.959192 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m10:32:53.960026 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics.courier_performance ..................... [RUN]
[0m10:32:53.961395 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m10:32:53.962117 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m10:32:53.972328 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m10:32:53.974695 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m10:32:54.029649 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m10:32:54.031827 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:32:54.032889 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m10:32:54.033693 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:32:54.116906 [debug] [Thread-1 (]: SQL status: BEGIN in 0.083 seconds
[0m10:32:54.117677 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:32:54.118150 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id
  );
  
[0m10:32:54.120677 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "late_delivery" does not exist
LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                           ^

[0m10:32:54.121170 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m10:32:54.121977 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m10:32:54.131440 [debug] [Thread-1 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:32:54.134089 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '16f997c0-627c-4ab6-8389-f9794b20aca5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C63CA10590>]}
[0m10:32:54.134947 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.17s]
[0m10:32:54.136262 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m10:32:54.137049 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m10:32:54.139814 [debug] [MainThread]: Using postgres connection "master"
[0m10:32:54.140257 [debug] [MainThread]: On master: BEGIN
[0m10:32:54.140612 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:32:54.219889 [debug] [MainThread]: SQL status: BEGIN in 0.079 seconds
[0m10:32:54.220578 [debug] [MainThread]: On master: COMMIT
[0m10:32:54.220979 [debug] [MainThread]: Using postgres connection "master"
[0m10:32:54.221323 [debug] [MainThread]: On master: COMMIT
[0m10:32:54.221958 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:32:54.222471 [debug] [MainThread]: On master: Close
[0m10:32:54.223145 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:32:54.223733 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m10:32:54.224298 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m10:32:54.224769 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m10:32:54.225367 [info ] [MainThread]: 
[0m10:32:54.226229 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.78 seconds (0.78s).
[0m10:32:54.227638 [debug] [MainThread]: Command end result
[0m10:32:54.260066 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:32:54.263857 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:32:54.305153 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m10:32:54.305713 [info ] [MainThread]: 
[0m10:32:54.306507 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m10:32:54.307286 [info ] [MainThread]: 
[0m10:32:54.308192 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m10:32:54.309154 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:32:54.309840 [info ] [MainThread]: 
[0m10:32:54.310766 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:32:54.311416 [info ] [MainThread]: 
[0m10:32:54.312191 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m10:32:54.313707 [debug] [MainThread]: Command `dbt run` failed at 10:32:54.313547 after 2.71 seconds
[0m10:32:54.314209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C63C453330>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C639E576B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C639E54AF0>]}
[0m10:32:54.314715 [debug] [MainThread]: Flushing usage events
[0m10:32:55.437627 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:32:59.130819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED71E98980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED7293CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED72F8BD90>]}


============================== 10:32:59.166925 | 1e4e1214-8dd3-493c-87ea-a10882b8ae6f ==============================
[0m10:32:59.166925 [info ] [MainThread]: Running with dbt=1.10.15
[0m10:32:59.167815 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'use_colors': 'True', 'invocation_command': 'dbt run --select courier_performance', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m10:32:59.359403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1e4e1214-8dd3-493c-87ea-a10882b8ae6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED722F8770>]}
[0m10:32:59.441078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1e4e1214-8dd3-493c-87ea-a10882b8ae6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED72E2EE00>]}
[0m10:32:59.442546 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:32:59.808575 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m10:32:59.994082 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:32:59.994698 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:33:00.051720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1e4e1214-8dd3-493c-87ea-a10882b8ae6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED748D5650>]}
[0m10:33:00.146243 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:33:00.149502 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:33:00.175648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1e4e1214-8dd3-493c-87ea-a10882b8ae6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED72E1FD40>]}
[0m10:33:00.176288 [info ] [MainThread]: Found 13 models, 5 data tests, 5 sources, 459 macros
[0m10:33:00.176862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1e4e1214-8dd3-493c-87ea-a10882b8ae6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED740E7D90>]}
[0m10:33:00.178789 [info ] [MainThread]: 
[0m10:33:00.179342 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:33:00.179733 [info ] [MainThread]: 
[0m10:33:00.180330 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:33:00.181403 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m10:33:00.277147 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m10:33:00.277596 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m10:33:00.277910 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:33:00.410578 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.133 seconds
[0m10:33:00.412019 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m10:33:00.419110 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m10:33:00.424895 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:33:00.425337 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m10:33:00.425765 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:33:00.485691 [debug] [ThreadPool]: SQL status: BEGIN in 0.060 seconds
[0m10:33:00.486204 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:33:00.486544 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m10:33:00.496234 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.009 seconds
[0m10:33:00.498046 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m10:33:00.498803 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m10:33:00.506919 [debug] [MainThread]: Using postgres connection "master"
[0m10:33:00.507365 [debug] [MainThread]: On master: BEGIN
[0m10:33:00.507682 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:33:00.568490 [debug] [MainThread]: SQL status: BEGIN in 0.061 seconds
[0m10:33:00.568995 [debug] [MainThread]: Using postgres connection "master"
[0m10:33:00.569405 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:33:00.580697 [debug] [MainThread]: SQL status: SELECT 5 in 0.011 seconds
[0m10:33:00.582825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1e4e1214-8dd3-493c-87ea-a10882b8ae6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED74BDCEF0>]}
[0m10:33:00.583562 [debug] [MainThread]: On master: ROLLBACK
[0m10:33:00.584222 [debug] [MainThread]: Using postgres connection "master"
[0m10:33:00.584570 [debug] [MainThread]: On master: BEGIN
[0m10:33:00.585423 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m10:33:00.585798 [debug] [MainThread]: On master: COMMIT
[0m10:33:00.586141 [debug] [MainThread]: Using postgres connection "master"
[0m10:33:00.586452 [debug] [MainThread]: On master: COMMIT
[0m10:33:00.586922 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:33:00.587310 [debug] [MainThread]: On master: Close
[0m10:33:00.592427 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m10:33:00.593087 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics.courier_performance ..................... [RUN]
[0m10:33:00.593857 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m10:33:00.594416 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m10:33:00.603107 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m10:33:00.604955 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m10:33:00.648639 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m10:33:00.650492 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:33:00.650983 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m10:33:00.651325 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:33:00.744627 [debug] [Thread-1 (]: SQL status: BEGIN in 0.093 seconds
[0m10:33:00.745111 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:33:00.745473 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id
  );
  
[0m10:33:00.747458 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "late_delivery" does not exist
LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                           ^

[0m10:33:00.747849 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m10:33:00.748939 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m10:33:00.759166 [debug] [Thread-1 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:33:00.762154 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e4e1214-8dd3-493c-87ea-a10882b8ae6f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED749A7C50>]}
[0m10:33:00.763251 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.17s]
[0m10:33:00.764896 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m10:33:00.765688 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m10:33:00.767815 [debug] [MainThread]: Using postgres connection "master"
[0m10:33:00.768212 [debug] [MainThread]: On master: BEGIN
[0m10:33:00.768500 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:33:00.956712 [debug] [MainThread]: SQL status: BEGIN in 0.188 seconds
[0m10:33:00.957338 [debug] [MainThread]: On master: COMMIT
[0m10:33:00.957646 [debug] [MainThread]: Using postgres connection "master"
[0m10:33:00.957914 [debug] [MainThread]: On master: COMMIT
[0m10:33:00.958455 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:33:00.958848 [debug] [MainThread]: On master: Close
[0m10:33:00.959377 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:33:00.959680 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m10:33:00.959925 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m10:33:00.960273 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m10:33:00.960591 [info ] [MainThread]: 
[0m10:33:00.961168 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.78 seconds (0.78s).
[0m10:33:00.962175 [debug] [MainThread]: Command end result
[0m10:33:01.055835 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:33:01.058718 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:33:01.066022 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m10:33:01.066405 [info ] [MainThread]: 
[0m10:33:01.066981 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m10:33:01.067562 [info ] [MainThread]: 
[0m10:33:01.068152 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m10:33:01.068778 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:33:01.069404 [info ] [MainThread]: 
[0m10:33:01.069991 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:33:01.070484 [info ] [MainThread]: 
[0m10:33:01.071029 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m10:33:01.072295 [debug] [MainThread]: Command `dbt run` failed at 10:33:01.072131 after 2.09 seconds
[0m10:33:01.072685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED749EB540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED7459B9D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED7459BE30>]}
[0m10:33:01.073067 [debug] [MainThread]: Flushing usage events
[0m10:33:02.042881 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:35:25.946632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BD8330980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BD8DDCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BD942BD90>]}


============================== 10:35:25.983827 | 5c117747-64d8-4c4e-baa4-f60d5a0c55fb ==============================
[0m10:35:25.983827 [info ] [MainThread]: Running with dbt=1.10.15
[0m10:35:25.985302 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run --select courier_performance', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m10:35:26.256642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5c117747-64d8-4c4e-baa4-f60d5a0c55fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BD8798770>]}
[0m10:35:26.341322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5c117747-64d8-4c4e-baa4-f60d5a0c55fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BD92CEE00>]}
[0m10:35:26.342886 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:35:26.766202 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m10:35:26.970087 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:35:26.998712 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:35:27.062984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5c117747-64d8-4c4e-baa4-f60d5a0c55fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BDADF5650>]}
[0m10:35:27.178356 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:35:27.184394 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:35:27.217169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5c117747-64d8-4c4e-baa4-f60d5a0c55fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BD92BFD40>]}
[0m10:35:27.218245 [info ] [MainThread]: Found 13 models, 5 data tests, 5 sources, 459 macros
[0m10:35:27.219328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5c117747-64d8-4c4e-baa4-f60d5a0c55fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BDA9B7D90>]}
[0m10:35:27.222012 [info ] [MainThread]: 
[0m10:35:27.222826 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:35:27.223554 [info ] [MainThread]: 
[0m10:35:27.224590 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:35:27.225892 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m10:35:27.354068 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m10:35:27.354610 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m10:35:27.355019 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:35:27.485967 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.131 seconds
[0m10:35:27.487807 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m10:35:27.496306 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m10:35:27.503474 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:35:27.504083 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m10:35:27.504698 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:35:27.565724 [debug] [ThreadPool]: SQL status: BEGIN in 0.061 seconds
[0m10:35:27.566304 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:35:27.566715 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m10:35:27.574338 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.007 seconds
[0m10:35:27.576843 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m10:35:27.577545 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m10:35:27.586881 [debug] [MainThread]: Using postgres connection "master"
[0m10:35:27.587352 [debug] [MainThread]: On master: BEGIN
[0m10:35:27.587762 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:35:27.649425 [debug] [MainThread]: SQL status: BEGIN in 0.062 seconds
[0m10:35:27.650039 [debug] [MainThread]: Using postgres connection "master"
[0m10:35:27.650493 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:35:27.659838 [debug] [MainThread]: SQL status: SELECT 5 in 0.009 seconds
[0m10:35:27.661940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5c117747-64d8-4c4e-baa4-f60d5a0c55fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BDB0FCEF0>]}
[0m10:35:27.662510 [debug] [MainThread]: On master: ROLLBACK
[0m10:35:27.663263 [debug] [MainThread]: Using postgres connection "master"
[0m10:35:27.663850 [debug] [MainThread]: On master: BEGIN
[0m10:35:27.664946 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m10:35:27.665364 [debug] [MainThread]: On master: COMMIT
[0m10:35:27.665758 [debug] [MainThread]: Using postgres connection "master"
[0m10:35:27.666126 [debug] [MainThread]: On master: COMMIT
[0m10:35:27.666764 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:35:27.667187 [debug] [MainThread]: On master: Close
[0m10:35:27.673855 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m10:35:27.674833 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics.courier_performance ..................... [RUN]
[0m10:35:27.676138 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m10:35:27.676798 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m10:35:27.685883 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m10:35:27.689229 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m10:35:27.744453 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m10:35:27.747681 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:35:27.748401 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m10:35:27.749088 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:35:27.816043 [debug] [Thread-1 (]: SQL status: BEGIN in 0.067 seconds
[0m10:35:27.816882 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:35:27.817635 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id
  );
  
[0m10:35:27.819978 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "late_delivery" does not exist
LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                           ^

[0m10:35:27.820605 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m10:35:27.821947 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m10:35:27.832033 [debug] [Thread-1 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:35:27.834485 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5c117747-64d8-4c4e-baa4-f60d5a0c55fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BDAEC7C50>]}
[0m10:35:27.835342 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.16s]
[0m10:35:27.836685 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m10:35:27.837636 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m10:35:27.840304 [debug] [MainThread]: Using postgres connection "master"
[0m10:35:27.840731 [debug] [MainThread]: On master: BEGIN
[0m10:35:27.841094 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:35:27.902974 [debug] [MainThread]: SQL status: BEGIN in 0.062 seconds
[0m10:35:27.903614 [debug] [MainThread]: On master: COMMIT
[0m10:35:27.904098 [debug] [MainThread]: Using postgres connection "master"
[0m10:35:27.904518 [debug] [MainThread]: On master: COMMIT
[0m10:35:27.905258 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:35:27.905861 [debug] [MainThread]: On master: Close
[0m10:35:27.906533 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:35:27.906934 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m10:35:27.907342 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m10:35:27.907681 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m10:35:27.908063 [info ] [MainThread]: 
[0m10:35:27.908756 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.68 seconds (0.68s).
[0m10:35:27.910083 [debug] [MainThread]: Command end result
[0m10:35:28.015920 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:35:28.019923 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:35:28.028929 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m10:35:28.029428 [info ] [MainThread]: 
[0m10:35:28.030159 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m10:35:28.030880 [info ] [MainThread]: 
[0m10:35:28.031692 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m10:35:28.032451 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:35:28.033200 [info ] [MainThread]: 
[0m10:35:28.034140 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:35:28.034886 [info ] [MainThread]: 
[0m10:35:28.035651 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m10:35:28.037532 [debug] [MainThread]: Command `dbt run` failed at 10:35:28.037279 after 2.37 seconds
[0m10:35:28.038304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BDAF0B540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BDAABB9D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028BDAABBE30>]}
[0m10:35:28.039055 [debug] [MainThread]: Flushing usage events
[0m10:35:29.157988 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:35:36.498888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272ED2E8980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272EDD8CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272EE3D7D90>]}


============================== 10:35:36.506356 | 1e6bffc2-87d5-4ec7-b3f6-e6159607d7d9 ==============================
[0m10:35:36.506356 [info ] [MainThread]: Running with dbt=1.10.15
[0m10:35:36.507539 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run --select marts.analytics', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m10:35:36.747960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1e6bffc2-87d5-4ec7-b3f6-e6159607d7d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272ED748770>]}
[0m10:35:36.829807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1e6bffc2-87d5-4ec7-b3f6-e6159607d7d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272EE27EE00>]}
[0m10:35:36.831358 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:35:37.243046 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m10:35:37.443132 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:35:37.443871 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:35:37.503585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1e6bffc2-87d5-4ec7-b3f6-e6159607d7d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272EFD19650>]}
[0m10:35:37.628382 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:35:37.633895 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:35:37.676052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1e6bffc2-87d5-4ec7-b3f6-e6159607d7d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272EE26FD40>]}
[0m10:35:37.677032 [info ] [MainThread]: Found 13 models, 5 data tests, 5 sources, 459 macros
[0m10:35:37.678566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1e6bffc2-87d5-4ec7-b3f6-e6159607d7d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272EF537D90>]}
[0m10:35:37.682249 [info ] [MainThread]: 
[0m10:35:37.683030 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:35:37.683830 [info ] [MainThread]: 
[0m10:35:37.684878 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:35:37.691803 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m10:35:37.836053 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m10:35:37.836668 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m10:35:37.837218 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:35:37.959394 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.122 seconds
[0m10:35:37.961203 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m10:35:37.964390 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m10:35:37.971960 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:35:37.972463 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m10:35:37.972819 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:35:38.034464 [debug] [ThreadPool]: SQL status: BEGIN in 0.061 seconds
[0m10:35:38.035115 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:35:38.035565 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m10:35:38.043782 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.008 seconds
[0m10:35:38.046459 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m10:35:38.047209 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m10:35:38.057344 [debug] [MainThread]: Using postgres connection "master"
[0m10:35:38.057894 [debug] [MainThread]: On master: BEGIN
[0m10:35:38.058291 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:35:38.118634 [debug] [MainThread]: SQL status: BEGIN in 0.060 seconds
[0m10:35:38.119299 [debug] [MainThread]: Using postgres connection "master"
[0m10:35:38.120203 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:35:38.129874 [debug] [MainThread]: SQL status: SELECT 5 in 0.009 seconds
[0m10:35:38.131579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1e6bffc2-87d5-4ec7-b3f6-e6159607d7d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272F0020FC0>]}
[0m10:35:38.132147 [debug] [MainThread]: On master: ROLLBACK
[0m10:35:38.132862 [debug] [MainThread]: Using postgres connection "master"
[0m10:35:38.133271 [debug] [MainThread]: On master: BEGIN
[0m10:35:38.134299 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m10:35:38.134701 [debug] [MainThread]: On master: COMMIT
[0m10:35:38.135091 [debug] [MainThread]: Using postgres connection "master"
[0m10:35:38.135485 [debug] [MainThread]: On master: COMMIT
[0m10:35:38.136145 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:35:38.136558 [debug] [MainThread]: On master: Close
[0m10:35:38.143375 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m10:35:38.144309 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:35:38.145029 [info ] [Thread-1 (]: 1 of 5 START sql table model analytics.fct_shipments ........................... [RUN]
[0m10:35:38.146733 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m10:35:38.145676 [info ] [Thread-2 (]: 2 of 5 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m10:35:38.147482 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m10:35:38.148303 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m10:35:38.158597 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:35:38.159491 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:35:38.163289 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:35:38.165998 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m10:35:38.166732 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:35:38.226690 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:35:38.233069 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:35:38.236776 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:35:38.237765 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:35:38.238466 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m10:35:38.239013 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m10:35:38.239680 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:35:38.240394 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:35:38.325576 [debug] [Thread-2 (]: SQL status: BEGIN in 0.085 seconds
[0m10:35:38.326173 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:35:38.326660 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    s.shipment_id,
    ca.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    ca.delivered_at,
    s.price,

    case
        when ca.delivered_at is null then null
        when ca.delivered_at::date > s.expected_delivery_date then true
        else false
    end as late_delivery

from "delivery_analytics"."raw"."shipments" s
left join "delivery_analytics"."raw"."courier_assignments" ca
    on s.shipment_id = ca.shipment_id
  );
  
[0m10:35:38.335935 [debug] [Thread-1 (]: SQL status: BEGIN in 0.096 seconds
[0m10:35:38.336451 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:35:38.336959 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."analytics"."stg_shipments"
),

shipment_status as (
    select *
    from "delivery_analytics"."analytics"."stg_shipment_status"
),

users as (
    select *
    from "delivery_analytics"."analytics"."dim_users"
),

couriers as (
    select *
    from "delivery_analytics"."analytics"."dim_couriers"
)

select
    s.shipment_id,
    s.sender_id,
    su.username as sender_name,
    s.receiver_id,
    ru.username as receiver_name,
    s.shipment_date,
    s.expected_delivery_date,
    ss.status,
    ss.status_timestamp,
    ss.courier_id,
    c.user_id as courier_user_id,
    c.vehicle_type,
    s.price
from shipments s
left join shipment_status ss on s.shipment_id = ss.shipment_id
left join users su on s.sender_id = su.user_id
left join users ru on s.receiver_id = ru.user_id
left join couriers c on ss.courier_id = c.courier_id
  );
  
[0m10:35:38.337578 [debug] [Thread-2 (]: SQL status: SELECT 6 in 0.010 seconds
[0m10:35:38.349121 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:35:38.349671 [debug] [Thread-1 (]: SQL status: SELECT 10 in 0.012 seconds
[0m10:35:38.350486 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue" rename to "fct_shipments_revenue__dbt_backup"
[0m10:35:38.357568 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:35:38.358490 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m10:35:38.359211 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:35:38.359889 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:35:38.362909 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:35:38.366602 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:35:38.367193 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp" rename to "fct_shipments_revenue"
[0m10:35:38.367790 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m10:35:38.369531 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:35:38.370095 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m10:35:38.459264 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:35:38.461739 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m10:35:38.462625 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:35:38.463270 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:35:38.464007 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:35:38.464588 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m10:35:38.496729 [debug] [Thread-1 (]: SQL status: COMMIT in 0.031 seconds
[0m10:35:38.497819 [debug] [Thread-2 (]: SQL status: COMMIT in 0.032 seconds
[0m10:35:38.506612 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m10:35:38.510283 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup"
[0m10:35:38.517019 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:35:38.518705 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:35:38.519580 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m10:35:38.520247 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup" cascade
[0m10:35:38.527876 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.007 seconds
[0m10:35:38.528402 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.008 seconds
[0m10:35:38.532136 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m10:35:38.533956 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m10:35:38.537404 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e6bffc2-87d5-4ec7-b3f6-e6159607d7d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272EFDE7B90>]}
[0m10:35:38.537927 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e6bffc2-87d5-4ec7-b3f6-e6159607d7d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272EFE2B960>]}
[0m10:35:38.539029 [info ] [Thread-2 (]: 2 of 5 OK created sql table model analytics.fct_shipments_revenue .............. [[32mSELECT 6[0m in 0.39s]
[0m10:35:38.540910 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:35:38.539993 [info ] [Thread-1 (]: 1 of 5 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 10[0m in 0.39s]
[0m10:35:38.542036 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m10:35:38.543263 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m10:35:38.543893 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:35:38.544585 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m10:35:38.545353 [info ] [Thread-4 (]: 4 of 5 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m10:35:38.548317 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m10:35:38.546176 [info ] [Thread-2 (]: 5 of 5 START sql table model analytics.fct_shipments_sla ....................... [RUN]
[0m10:35:38.549116 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m10:35:38.549818 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments_revenue, now model.dbt_delivery_analytics.fct_shipments_sla)
[0m10:35:38.547218 [info ] [Thread-3 (]: 3 of 5 START sql table model analytics.courier_performance ..................... [RUN]
[0m10:35:38.553502 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:35:38.554434 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:35:38.555468 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m10:35:38.558819 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:35:38.559539 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m10:35:38.563607 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m10:35:38.564820 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m10:35:38.569264 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:35:38.570892 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m10:35:38.571659 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:35:38.576720 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m10:35:38.581041 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:35:38.582234 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:35:38.582814 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m10:35:38.583575 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:35:38.585243 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:35:38.586073 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m10:35:38.587182 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:35:38.587888 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m10:35:38.588524 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: BEGIN
[0m10:35:38.589378 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m10:35:38.667572 [debug] [Thread-4 (]: SQL status: BEGIN in 0.084 seconds
[0m10:35:38.668307 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:35:38.668967 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m10:35:38.673326 [debug] [Thread-3 (]: SQL status: BEGIN in 0.085 seconds
[0m10:35:38.673960 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:35:38.674444 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id
  );
  
[0m10:35:38.674997 [debug] [Thread-2 (]: SQL status: BEGIN in 0.086 seconds
[0m10:35:38.675635 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:35:38.676108 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,
        (delivered_at::date - expected_delivery_date::date) as delivery_delay_days,
        case
            when delivered_at::date > expected_delivery_date::date then true
            else false
        end as late_delivery
    from "delivery_analytics"."analytics"."fct_shipments"
)

select *
from sla
  );
  
[0m10:35:38.677131 [debug] [Thread-3 (]: Postgres adapter: Postgres error: column "late_delivery" does not exist
LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                           ^

[0m10:35:38.677717 [debug] [Thread-2 (]: Postgres adapter: Postgres error: column "delivered_at" does not exist
LINE 28:         delivered_at,
                 ^

[0m10:35:38.678172 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m10:35:38.678742 [debug] [Thread-4 (]: SQL status: SELECT 9 in 0.009 seconds
[0m10:35:38.679318 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: ROLLBACK
[0m10:35:38.683446 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:35:38.684134 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m10:35:38.684908 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m10:35:38.685708 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: Close
[0m10:35:38.689196 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m10:35:38.693678 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:35:38.694480 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m10:35:38.696134 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:35:38.698359 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m10:35:38.699081 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:35:38.699674 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m10:35:38.701950 [debug] [Thread-4 (]: SQL status: COMMIT in 0.002 seconds
[0m10:35:38.705814 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m10:35:38.707195 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:35:38.708361 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m10:35:38.714940 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.006 seconds
[0m10:35:38.720410 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m10:35:38.721668 [debug] [Thread-3 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:35:38.722770 [debug] [Thread-2 (]: Database Error in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)
  column "delivered_at" does not exist
  LINE 28:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql
[0m10:35:38.723745 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e6bffc2-87d5-4ec7-b3f6-e6159607d7d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272EF9A5950>]}
[0m10:35:38.724412 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e6bffc2-87d5-4ec7-b3f6-e6159607d7d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272EFE57B60>]}
[0m10:35:38.725107 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1e6bffc2-87d5-4ec7-b3f6-e6159607d7d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272F00B6DD0>]}
[0m10:35:38.726222 [info ] [Thread-4 (]: 4 of 5 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.18s]
[0m10:35:38.728860 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m10:35:38.727038 [error] [Thread-3 (]: 3 of 5 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.17s]
[0m10:35:38.728032 [error] [Thread-2 (]: 5 of 5 ERROR creating sql table model analytics.fct_shipments_sla .............. [[31mERROR[0m in 0.18s]
[0m10:35:38.730189 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m10:35:38.731006 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:35:38.731928 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m10:35:38.733999 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_sla' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)
  column "delivered_at" does not exist
  LINE 28:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql.
[0m10:35:38.736314 [debug] [MainThread]: Using postgres connection "master"
[0m10:35:38.736858 [debug] [MainThread]: On master: BEGIN
[0m10:35:38.737254 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:35:38.801149 [debug] [MainThread]: SQL status: BEGIN in 0.064 seconds
[0m10:35:38.801884 [debug] [MainThread]: On master: COMMIT
[0m10:35:38.802320 [debug] [MainThread]: Using postgres connection "master"
[0m10:35:38.802790 [debug] [MainThread]: On master: COMMIT
[0m10:35:38.803502 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:35:38.804030 [debug] [MainThread]: On master: Close
[0m10:35:38.804669 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:35:38.805096 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m10:35:38.805423 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m10:35:38.805885 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments' was properly closed.
[0m10:35:38.806336 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_sla' was properly closed.
[0m10:35:38.806757 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m10:35:38.807084 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m10:35:38.807585 [info ] [MainThread]: 
[0m10:35:38.808553 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 1.12 seconds (1.12s).
[0m10:35:38.810669 [debug] [MainThread]: Command end result
[0m10:35:38.840899 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:35:38.844925 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:35:38.854262 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m10:35:38.855045 [info ] [MainThread]: 
[0m10:35:38.855832 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m10:35:38.856687 [info ] [MainThread]: 
[0m10:35:38.857657 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m10:35:38.858457 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:35:38.859139 [info ] [MainThread]: 
[0m10:35:38.859973 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:35:38.860724 [info ] [MainThread]: 
[0m10:35:38.861612 [error] [MainThread]: [31mFailure in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)[0m
[0m10:35:38.862709 [error] [MainThread]:   Database Error in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)
  column "delivered_at" does not exist
  LINE 28:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql
[0m10:35:38.863602 [info ] [MainThread]: 
[0m10:35:38.864562 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql
[0m10:35:38.865337 [info ] [MainThread]: 
[0m10:35:38.866099 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=5
[0m10:35:38.868042 [debug] [MainThread]: Command `dbt run` failed at 10:35:38.867794 after 2.58 seconds
[0m10:35:38.868854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272ED6D7290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272EDDAC230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000272EDDAC6B0>]}
[0m10:35:38.869685 [debug] [MainThread]: Flushing usage events
[0m10:35:40.045863 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:39:02.212800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F26D4980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F317CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F37CBD90>]}


============================== 10:39:02.219130 | 9e1ce1b1-be32-4a7b-9bfb-3eeeb863a1f6 ==============================
[0m10:39:02.219130 [info ] [MainThread]: Running with dbt=1.10.15
[0m10:39:02.221139 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt run --select marts.analytics', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m10:39:02.485787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9e1ce1b1-be32-4a7b-9bfb-3eeeb863a1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F2B38770>]}
[0m10:39:02.578377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9e1ce1b1-be32-4a7b-9bfb-3eeeb863a1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F366EE00>]}
[0m10:39:02.580185 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:39:03.034467 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m10:39:03.258042 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:39:03.259372 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\fct_shipments_sla.sql
[0m10:39:03.645650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9e1ce1b1-be32-4a7b-9bfb-3eeeb863a1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F50F1B50>]}
[0m10:39:03.771618 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:39:03.775966 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:39:03.884198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9e1ce1b1-be32-4a7b-9bfb-3eeeb863a1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F365FD40>]}
[0m10:39:03.884942 [info ] [MainThread]: Found 13 models, 5 data tests, 5 sources, 459 macros
[0m10:39:03.885719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e1ce1b1-be32-4a7b-9bfb-3eeeb863a1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F5102F90>]}
[0m10:39:03.888308 [info ] [MainThread]: 
[0m10:39:03.889120 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:39:03.889915 [info ] [MainThread]: 
[0m10:39:03.890966 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:39:03.897318 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m10:39:04.034817 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m10:39:04.035374 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m10:39:04.035754 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:39:04.170280 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.134 seconds
[0m10:39:04.172031 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m10:39:04.175153 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m10:39:04.184809 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:39:04.185342 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m10:39:04.185707 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:39:04.259736 [debug] [ThreadPool]: SQL status: BEGIN in 0.074 seconds
[0m10:39:04.260754 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:39:04.261506 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m10:39:04.269315 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.007 seconds
[0m10:39:04.271557 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m10:39:04.272237 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m10:39:04.283597 [debug] [MainThread]: Using postgres connection "master"
[0m10:39:04.284334 [debug] [MainThread]: On master: BEGIN
[0m10:39:04.284959 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:39:04.349307 [debug] [MainThread]: SQL status: BEGIN in 0.064 seconds
[0m10:39:04.349945 [debug] [MainThread]: Using postgres connection "master"
[0m10:39:04.350418 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:39:04.360053 [debug] [MainThread]: SQL status: SELECT 5 in 0.009 seconds
[0m10:39:04.363177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e1ce1b1-be32-4a7b-9bfb-3eeeb863a1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F540FEE0>]}
[0m10:39:04.363794 [debug] [MainThread]: On master: ROLLBACK
[0m10:39:04.364605 [debug] [MainThread]: Using postgres connection "master"
[0m10:39:04.365089 [debug] [MainThread]: On master: BEGIN
[0m10:39:04.366060 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m10:39:04.366472 [debug] [MainThread]: On master: COMMIT
[0m10:39:04.366878 [debug] [MainThread]: Using postgres connection "master"
[0m10:39:04.367242 [debug] [MainThread]: On master: COMMIT
[0m10:39:04.367860 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:39:04.368278 [debug] [MainThread]: On master: Close
[0m10:39:04.374333 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m10:39:04.375110 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:39:04.375828 [info ] [Thread-1 (]: 1 of 5 START sql table model analytics.fct_shipments ........................... [RUN]
[0m10:39:04.378007 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m10:39:04.376682 [info ] [Thread-2 (]: 2 of 5 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m10:39:04.378806 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m10:39:04.379509 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m10:39:04.390183 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:39:04.390958 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:39:04.395828 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:39:04.398688 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:39:04.399380 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m10:39:04.478987 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:39:04.484411 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:39:04.487107 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:39:04.487880 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:39:04.488484 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m10:39:04.489061 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m10:39:04.489625 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:39:04.490174 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:39:04.564841 [debug] [Thread-1 (]: SQL status: BEGIN in 0.075 seconds
[0m10:39:04.565537 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:39:04.566039 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."analytics"."stg_shipments"
),

shipment_status as (
    select *
    from "delivery_analytics"."analytics"."stg_shipment_status"
),

users as (
    select *
    from "delivery_analytics"."analytics"."dim_users"
),

couriers as (
    select *
    from "delivery_analytics"."analytics"."dim_couriers"
)

select
    s.shipment_id,
    s.sender_id,
    su.username as sender_name,
    s.receiver_id,
    ru.username as receiver_name,
    s.shipment_date,
    s.expected_delivery_date,
    ss.status,
    ss.status_timestamp,
    ss.courier_id,
    c.user_id as courier_user_id,
    c.vehicle_type,
    s.price
from shipments s
left join shipment_status ss on s.shipment_id = ss.shipment_id
left join users su on s.sender_id = su.user_id
left join users ru on s.receiver_id = ru.user_id
left join couriers c on ss.courier_id = c.courier_id
  );
  
[0m10:39:04.583233 [debug] [Thread-1 (]: SQL status: SELECT 10 in 0.017 seconds
[0m10:39:04.595947 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:39:04.596537 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m10:39:04.597795 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:39:04.601252 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:39:04.601765 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m10:39:04.603270 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:39:04.619021 [debug] [Thread-2 (]: SQL status: BEGIN in 0.129 seconds
[0m10:39:04.625451 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:39:04.626086 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:39:04.626968 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:39:04.627744 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    s.shipment_id,
    ca.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    ca.delivered_at,
    s.price,

    case
        when ca.delivered_at is null then null
        when ca.delivered_at::date > s.expected_delivery_date then true
        else false
    end as late_delivery

from "delivery_analytics"."raw"."shipments" s
left join "delivery_analytics"."raw"."courier_assignments" ca
    on s.shipment_id = ca.shipment_id
  );
  
[0m10:39:04.628417 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:39:04.630549 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m10:39:04.638628 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m10:39:04.639226 [debug] [Thread-2 (]: SQL status: SELECT 6 in 0.010 seconds
[0m10:39:04.646269 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:39:04.650036 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:39:04.650828 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m10:39:04.651519 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue" rename to "fct_shipments_revenue__dbt_backup"
[0m10:39:04.652942 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:39:04.655984 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:39:04.656477 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp" rename to "fct_shipments_revenue"
[0m10:39:04.657355 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.005 seconds
[0m10:39:04.658027 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:39:04.661823 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m10:39:04.663788 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m10:39:04.664537 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:39:04.667538 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m10:39:04.668693 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e1ce1b1-be32-4a7b-9bfb-3eeeb863a1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F567C290>]}
[0m10:39:04.670171 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m10:39:04.669619 [info ] [Thread-1 (]: 1 of 5 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 10[0m in 0.29s]
[0m10:39:04.673359 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup"
[0m10:39:04.674486 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m10:39:04.675536 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:39:04.676745 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup" cascade
[0m10:39:04.677706 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m10:39:04.678305 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m10:39:04.678872 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:39:04.679551 [info ] [Thread-4 (]: 4 of 5 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m10:39:04.681988 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m10:39:04.680336 [info ] [Thread-3 (]: 3 of 5 START sql table model analytics.courier_performance ..................... [RUN]
[0m10:39:04.682747 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m10:39:04.683410 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.006 seconds
[0m10:39:04.684401 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m10:39:04.681213 [info ] [Thread-1 (]: 5 of 5 START sql table model analytics.fct_shipments_sla ....................... [RUN]
[0m10:39:04.687955 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:39:04.690070 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m10:39:04.690705 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m10:39:04.691818 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.fct_shipments_sla)
[0m10:39:04.697859 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m10:39:04.698816 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e1ce1b1-be32-4a7b-9bfb-3eeeb863a1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F5578D60>]}
[0m10:39:04.699361 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:39:04.701525 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m10:39:04.700869 [info ] [Thread-2 (]: 2 of 5 OK created sql table model analytics.fct_shipments_revenue .............. [[32mSELECT 6[0m in 0.32s]
[0m10:39:04.706195 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:39:04.712389 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:39:04.713458 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:39:04.714812 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m10:39:04.719523 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m10:39:04.720991 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:39:04.721639 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:39:04.722382 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m10:39:04.726634 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:39:04.727631 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:39:04.728517 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:39:04.729609 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m10:39:04.730158 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m10:39:04.731316 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:39:04.731866 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: BEGIN
[0m10:39:04.732365 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:39:04.807866 [debug] [Thread-4 (]: SQL status: BEGIN in 0.080 seconds
[0m10:39:04.808585 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:39:04.809074 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m10:39:04.817163 [debug] [Thread-3 (]: SQL status: BEGIN in 0.087 seconds
[0m10:39:04.817824 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:39:04.818379 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id
  );
  
[0m10:39:04.819268 [debug] [Thread-1 (]: SQL status: BEGIN in 0.087 seconds
[0m10:39:04.819760 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:39:04.820326 [debug] [Thread-4 (]: SQL status: SELECT 9 in 0.011 seconds
[0m10:39:04.820914 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,

        case
            when delivered_at is null then null
            else (delivered_at::date - expected_delivery_date::date)
        end as delivery_delay_days,

        case
            when delivered_at is null then null
            when delivered_at::date > expected_delivery_date::date then true
            else false
        end as late_delivery

    from "delivery_analytics"."analytics"."fct_shipments"
)

select *
from sla
  );
  
[0m10:39:04.821582 [debug] [Thread-3 (]: Postgres adapter: Postgres error: column "late_delivery" does not exist
LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                           ^

[0m10:39:04.825695 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:39:04.826519 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: ROLLBACK
[0m10:39:04.827273 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m10:39:04.828155 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "delivered_at" does not exist
LINE 28:         delivered_at,
                 ^

[0m10:39:04.828865 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m10:39:04.829391 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: ROLLBACK
[0m10:39:04.829936 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m10:39:04.835105 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:39:04.835862 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: Close
[0m10:39:04.836499 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m10:39:04.838640 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:39:04.840842 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m10:39:04.841542 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:39:04.842050 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m10:39:04.844346 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m10:39:04.851896 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m10:39:04.853048 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:39:04.853876 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m10:39:04.861457 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.007 seconds
[0m10:39:04.862743 [debug] [Thread-3 (]: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:39:04.864705 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m10:39:04.865744 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e1ce1b1-be32-4a7b-9bfb-3eeeb863a1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F571DDB0>]}
[0m10:39:04.869704 [debug] [Thread-1 (]: Database Error in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)
  column "delivered_at" does not exist
  LINE 28:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql
[0m10:39:04.871329 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e1ce1b1-be32-4a7b-9bfb-3eeeb863a1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F554ACD0>]}
[0m10:39:04.872024 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e1ce1b1-be32-4a7b-9bfb-3eeeb863a1f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F554ABD0>]}
[0m10:39:04.870643 [error] [Thread-3 (]: 3 of 5 ERROR creating sql table model analytics.courier_performance ............ [[31mERROR[0m in 0.18s]
[0m10:39:04.873268 [info ] [Thread-4 (]: 4 of 5 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.19s]
[0m10:39:04.875138 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m10:39:04.876145 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m10:39:04.874100 [error] [Thread-1 (]: 5 of 5 ERROR creating sql table model analytics.fct_shipments_sla .............. [[31mERROR[0m in 0.18s]
[0m10:39:04.877268 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.courier_performance' to be skipped because of status 'error'.  Reason: Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql.
[0m10:39:04.878322 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:39:04.880087 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments_sla' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)
  column "delivered_at" does not exist
  LINE 28:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql.
[0m10:39:04.882367 [debug] [MainThread]: Using postgres connection "master"
[0m10:39:04.882890 [debug] [MainThread]: On master: BEGIN
[0m10:39:04.883290 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:39:04.949417 [debug] [MainThread]: SQL status: BEGIN in 0.066 seconds
[0m10:39:04.950096 [debug] [MainThread]: On master: COMMIT
[0m10:39:04.950706 [debug] [MainThread]: Using postgres connection "master"
[0m10:39:04.951282 [debug] [MainThread]: On master: COMMIT
[0m10:39:04.951974 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:39:04.952658 [debug] [MainThread]: On master: Close
[0m10:39:04.953510 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:39:04.953897 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m10:39:04.954262 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m10:39:04.954743 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_sla' was properly closed.
[0m10:39:04.955247 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_revenue' was properly closed.
[0m10:39:04.955600 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m10:39:04.956009 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m10:39:04.956546 [info ] [MainThread]: 
[0m10:39:04.957356 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 1.07 seconds (1.07s).
[0m10:39:04.959504 [debug] [MainThread]: Command end result
[0m10:39:04.992443 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:39:04.997346 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:39:05.006441 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m10:39:05.007036 [info ] [MainThread]: 
[0m10:39:05.007875 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m10:39:05.008676 [info ] [MainThread]: 
[0m10:39:05.009436 [error] [MainThread]: [31mFailure in model courier_performance (models\marts\analytics\courier_performance.sql)[0m
[0m10:39:05.010450 [error] [MainThread]:   Database Error in model courier_performance (models\marts\analytics\courier_performance.sql)
  column "late_delivery" does not exist
  LINE 15:     sum(case when late_delivery = false then 1 else 0 end) a...
                             ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:39:05.011371 [info ] [MainThread]: 
[0m10:39:05.012268 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\courier_performance.sql
[0m10:39:05.013013 [info ] [MainThread]: 
[0m10:39:05.013859 [error] [MainThread]: [31mFailure in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)[0m
[0m10:39:05.014764 [error] [MainThread]:   Database Error in model fct_shipments_sla (models\marts\analytics\fct_shipments_sla.sql)
  column "delivered_at" does not exist
  LINE 28:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql
[0m10:39:05.015472 [info ] [MainThread]: 
[0m10:39:05.016216 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\fct_shipments_sla.sql
[0m10:39:05.016914 [info ] [MainThread]: 
[0m10:39:05.017746 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=5
[0m10:39:05.019328 [debug] [MainThread]: Command `dbt run` failed at 10:39:05.019159 after 3.05 seconds
[0m10:39:05.019846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F4DA3E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F319EF30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B5F3668FB0>]}
[0m10:39:05.020366 [debug] [MainThread]: Flushing usage events
[0m10:39:06.246014 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:40:35.069636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D93A74980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D9451CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D94B6BD90>]}


============================== 10:40:35.106561 | 062476c9-f9ca-4b07-9bee-b5a759248d53 ==============================
[0m10:40:35.106561 [info ] [MainThread]: Running with dbt=1.10.15
[0m10:40:35.108311 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt run --select marts.analytics', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m10:40:35.373325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '062476c9-f9ca-4b07-9bee-b5a759248d53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D93ED8770>]}
[0m10:40:35.463937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '062476c9-f9ca-4b07-9bee-b5a759248d53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D94A0EE00>]}
[0m10:40:35.466075 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:40:35.927100 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m10:40:36.160284 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:40:36.161937 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\fct_shipments.sql
[0m10:40:36.724976 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values`. Arguments to generic tests
should be nested under the `arguments` property.`
[0m10:40:36.726529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '062476c9-f9ca-4b07-9bee-b5a759248d53', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D964B5C50>]}
[0m10:40:36.939504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '062476c9-f9ca-4b07-9bee-b5a759248d53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D94BC3020>]}
[0m10:40:37.099652 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:40:37.103651 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:40:37.135617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '062476c9-f9ca-4b07-9bee-b5a759248d53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D961C4830>]}
[0m10:40:37.136383 [info ] [MainThread]: Found 13 models, 5 data tests, 5 sources, 459 macros
[0m10:40:37.137177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '062476c9-f9ca-4b07-9bee-b5a759248d53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D9659FEE0>]}
[0m10:40:37.140689 [info ] [MainThread]: 
[0m10:40:37.141431 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:40:37.142185 [info ] [MainThread]: 
[0m10:40:37.143298 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:40:37.149929 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m10:40:37.284518 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m10:40:37.285109 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m10:40:37.285784 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:40:37.393631 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.108 seconds
[0m10:40:37.395332 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m10:40:37.399029 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m10:40:37.408295 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:40:37.408889 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m10:40:37.409848 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:40:37.478426 [debug] [ThreadPool]: SQL status: BEGIN in 0.069 seconds
[0m10:40:37.478993 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:40:37.479420 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m10:40:37.488167 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.008 seconds
[0m10:40:37.490144 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m10:40:37.490890 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m10:40:37.500529 [debug] [MainThread]: Using postgres connection "master"
[0m10:40:37.501055 [debug] [MainThread]: On master: BEGIN
[0m10:40:37.501425 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:40:37.567192 [debug] [MainThread]: SQL status: BEGIN in 0.066 seconds
[0m10:40:37.567750 [debug] [MainThread]: Using postgres connection "master"
[0m10:40:37.568203 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:40:37.578932 [debug] [MainThread]: SQL status: SELECT 5 in 0.010 seconds
[0m10:40:37.582370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '062476c9-f9ca-4b07-9bee-b5a759248d53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D96A1C110>]}
[0m10:40:37.583027 [debug] [MainThread]: On master: ROLLBACK
[0m10:40:37.583826 [debug] [MainThread]: Using postgres connection "master"
[0m10:40:37.584241 [debug] [MainThread]: On master: BEGIN
[0m10:40:37.585175 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m10:40:37.585581 [debug] [MainThread]: On master: COMMIT
[0m10:40:37.585954 [debug] [MainThread]: Using postgres connection "master"
[0m10:40:37.586333 [debug] [MainThread]: On master: COMMIT
[0m10:40:37.586942 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:40:37.587368 [debug] [MainThread]: On master: Close
[0m10:40:37.593202 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m10:40:37.593837 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:40:37.594585 [info ] [Thread-1 (]: 1 of 5 START sql table model analytics.fct_shipments ........................... [RUN]
[0m10:40:37.596611 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m10:40:37.595434 [info ] [Thread-2 (]: 2 of 5 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m10:40:37.597547 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m10:40:37.598327 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m10:40:37.608025 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:37.608692 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:40:37.613173 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:37.615638 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:40:37.616202 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m10:40:37.670886 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:37.675300 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:37.678021 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:37.678585 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:37.679250 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m10:40:37.679816 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m10:40:37.680723 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:40:37.681658 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:40:37.758525 [debug] [Thread-2 (]: SQL status: BEGIN in 0.077 seconds
[0m10:40:37.759142 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:37.759708 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    s.shipment_id,
    ca.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    ca.delivered_at,
    s.price,

    case
        when ca.delivered_at is null then null
        when ca.delivered_at::date > s.expected_delivery_date then true
        else false
    end as late_delivery

from "delivery_analytics"."raw"."shipments" s
left join "delivery_analytics"."raw"."courier_assignments" ca
    on s.shipment_id = ca.shipment_id
  );
  
[0m10:40:37.760581 [debug] [Thread-1 (]: SQL status: BEGIN in 0.080 seconds
[0m10:40:37.761048 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:37.761525 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."raw"."shipments"
),

assignments as (
    select
        shipment_id,
        courier_id,
        delivered_at
    from "delivery_analytics"."raw"."courier_assignments"
)

select
    s.shipment_id,
    a.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    a.delivered_at,
    s.price,

    case
        when a.delivered_at is null then null
        when a.delivered_at::date > s.expected_delivery_date::date then true
        else false
    end as late_delivery

from shipments s
left join assignments a
    on s.shipment_id = a.shipment_id
  );
  
[0m10:40:37.773682 [debug] [Thread-2 (]: SQL status: SELECT 6 in 0.013 seconds
[0m10:40:37.774377 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.012 seconds
[0m10:40:37.789281 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:37.798362 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:37.799155 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue" rename to "fct_shipments_revenue__dbt_backup"
[0m10:40:37.799800 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m10:40:37.801332 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:40:37.801885 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:40:37.805231 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:37.809026 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:37.809659 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp" rename to "fct_shipments_revenue"
[0m10:40:37.810402 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m10:40:37.812024 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:40:37.812528 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:40:37.834135 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m10:40:37.836524 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:40:37.837231 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:37.837902 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:37.838525 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m10:40:37.839121 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:40:37.841503 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m10:40:37.842100 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m10:40:37.851238 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup"
[0m10:40:37.854584 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m10:40:37.861634 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:37.862788 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:37.863538 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup" cascade
[0m10:40:37.864264 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m10:40:37.871113 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.006 seconds
[0m10:40:37.871700 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.007 seconds
[0m10:40:37.874906 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m10:40:37.876669 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m10:40:37.880212 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '062476c9-f9ca-4b07-9bee-b5a759248d53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D969ED390>]}
[0m10:40:37.880979 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '062476c9-f9ca-4b07-9bee-b5a759248d53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D93E54C30>]}
[0m10:40:37.882051 [info ] [Thread-1 (]: 1 of 5 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 6[0m in 0.28s]
[0m10:40:37.884073 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m10:40:37.882967 [info ] [Thread-2 (]: 2 of 5 OK created sql table model analytics.fct_shipments_revenue .............. [[32mSELECT 6[0m in 0.28s]
[0m10:40:37.885345 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:40:37.885894 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:40:37.886424 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m10:40:37.887017 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m10:40:37.887923 [info ] [Thread-1 (]: 5 of 5 START sql table model analytics.fct_shipments_sla ....................... [RUN]
[0m10:40:37.890141 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.fct_shipments_sla)
[0m10:40:37.888609 [info ] [Thread-4 (]: 4 of 5 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m10:40:37.890880 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:40:37.891758 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m10:40:37.889507 [info ] [Thread-3 (]: 3 of 5 START sql table model analytics.courier_performance ..................... [RUN]
[0m10:40:37.895916 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:40:37.896591 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m10:40:37.897575 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m10:40:37.901299 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:40:37.901934 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m10:40:37.905878 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m10:40:37.907272 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:40:37.911296 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:40:37.912561 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m10:40:37.913297 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m10:40:37.918936 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m10:40:37.923502 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:40:37.924889 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:40:37.925469 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: BEGIN
[0m10:40:37.926085 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:40:37.927511 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:40:37.928125 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:40:37.928637 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m10:40:37.929228 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m10:40:37.929765 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m10:40:37.930595 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:40:38.003197 [debug] [Thread-1 (]: SQL status: BEGIN in 0.077 seconds
[0m10:40:38.003937 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:40:38.004468 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,

        case
            when delivered_at is null then null
            else (delivered_at::date - expected_delivery_date::date)
        end as delivery_delay_days,

        case
            when delivered_at is null then null
            when delivered_at::date > expected_delivery_date::date then true
            else false
        end as late_delivery

    from "delivery_analytics"."analytics"."fct_shipments"
)

select *
from sla
  );
  
[0m10:40:38.014095 [debug] [Thread-3 (]: SQL status: BEGIN in 0.084 seconds
[0m10:40:38.014854 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:40:38.015444 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.010 seconds
[0m10:40:38.016017 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id
  );
  
[0m10:40:38.020166 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:40:38.021037 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp" rename to "fct_shipments_sla"
[0m10:40:38.021946 [debug] [Thread-4 (]: SQL status: BEGIN in 0.091 seconds
[0m10:40:38.022475 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:40:38.022960 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:40:38.023561 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m10:40:38.025386 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m10:40:38.026279 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:40:38.026861 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m10:40:38.027503 [debug] [Thread-4 (]: Postgres adapter: Postgres error: column "status" does not exist
LINE 19:     status,
             ^

[0m10:40:38.028158 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: ROLLBACK
[0m10:40:38.029081 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m10:40:38.029634 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m10:40:38.030266 [debug] [Thread-3 (]: SQL status: SELECT 3 in 0.009 seconds
[0m10:40:38.033158 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup"
[0m10:40:38.038712 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:40:38.039906 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:40:38.040591 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance__dbt_tmp" rename to "courier_performance"
[0m10:40:38.041510 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup" cascade
[0m10:40:38.043510 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m10:40:38.045410 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: Close
[0m10:40:38.045962 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m10:40:38.048169 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m10:40:38.048944 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '062476c9-f9ca-4b07-9bee-b5a759248d53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D96952350>]}
[0m10:40:38.049515 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:40:38.051223 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m10:40:38.050676 [info ] [Thread-1 (]: 5 of 5 OK created sql table model analytics.fct_shipments_sla .................. [[32mSELECT 6[0m in 0.16s]
[0m10:40:38.052664 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:40:38.053503 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m10:40:38.056326 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."courier_performance__dbt_backup"
[0m10:40:38.057272 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:40:38.057758 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
drop table if exists "delivery_analytics"."analytics"."courier_performance__dbt_backup" cascade
[0m10:40:38.058776 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.000 seconds
[0m10:40:38.061043 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m10:40:38.062135 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '062476c9-f9ca-4b07-9bee-b5a759248d53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D96B2B3F0>]}
[0m10:40:38.063148 [info ] [Thread-3 (]: 3 of 5 OK created sql table model analytics.courier_performance ................ [[32mSELECT 3[0m in 0.16s]
[0m10:40:38.064760 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m10:40:38.070183 [debug] [Thread-4 (]: Database Error in model daily_shipments_status (models\marts\analytics\daily_shipments_status.sql)
  column "status" does not exist
  LINE 19:     status,
               ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\daily_shipments_status.sql
[0m10:40:38.070909 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '062476c9-f9ca-4b07-9bee-b5a759248d53', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D96BAF230>]}
[0m10:40:38.071886 [error] [Thread-4 (]: 4 of 5 ERROR creating sql table model analytics.daily_shipments_status ......... [[31mERROR[0m in 0.18s]
[0m10:40:38.073202 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m10:40:38.073983 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.daily_shipments_status' to be skipped because of status 'error'.  Reason: Database Error in model daily_shipments_status (models\marts\analytics\daily_shipments_status.sql)
  column "status" does not exist
  LINE 19:     status,
               ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\daily_shipments_status.sql.
[0m10:40:38.077306 [debug] [MainThread]: Using postgres connection "master"
[0m10:40:38.077741 [debug] [MainThread]: On master: BEGIN
[0m10:40:38.078155 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:40:38.143092 [debug] [MainThread]: SQL status: BEGIN in 0.065 seconds
[0m10:40:38.143748 [debug] [MainThread]: On master: COMMIT
[0m10:40:38.144273 [debug] [MainThread]: Using postgres connection "master"
[0m10:40:38.144640 [debug] [MainThread]: On master: COMMIT
[0m10:40:38.145331 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:40:38.146088 [debug] [MainThread]: On master: Close
[0m10:40:38.147009 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:40:38.147413 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m10:40:38.147771 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m10:40:38.148256 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_sla' was properly closed.
[0m10:40:38.148759 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_revenue' was properly closed.
[0m10:40:38.149265 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m10:40:38.149775 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m10:40:38.150261 [info ] [MainThread]: 
[0m10:40:38.151018 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 1.01 seconds (1.01s).
[0m10:40:38.153221 [debug] [MainThread]: Command end result
[0m10:40:38.182607 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:40:38.186618 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:40:38.196739 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m10:40:38.197533 [info ] [MainThread]: 
[0m10:40:38.198593 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m10:40:38.199550 [info ] [MainThread]: 
[0m10:40:38.200388 [error] [MainThread]: [31mFailure in model daily_shipments_status (models\marts\analytics\daily_shipments_status.sql)[0m
[0m10:40:38.201251 [error] [MainThread]:   Database Error in model daily_shipments_status (models\marts\analytics\daily_shipments_status.sql)
  column "status" does not exist
  LINE 19:     status,
               ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\daily_shipments_status.sql
[0m10:40:38.201975 [info ] [MainThread]: 
[0m10:40:38.202834 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\daily_shipments_status.sql
[0m10:40:38.203624 [info ] [MainThread]: 
[0m10:40:38.204440 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=5
[0m10:40:38.205439 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m10:40:38.206955 [debug] [MainThread]: Command `dbt run` failed at 10:40:38.206800 after 3.35 seconds
[0m10:40:38.207567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D944EBB30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D96C23B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028D96C23D70>]}
[0m10:40:38.208035 [debug] [MainThread]: Flushing usage events
[0m10:40:39.464066 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:40:55.391292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A7018980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A7ACCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A8107D90>]}


============================== 10:40:55.402620 | e831bf36-10d1-4665-8a18-6c43c1abeed8 ==============================
[0m10:40:55.402620 [info ] [MainThread]: Running with dbt=1.10.15
[0m10:40:55.404859 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run --select marts.analytics', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m10:40:55.772313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e831bf36-10d1-4665-8a18-6c43c1abeed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A7478770>]}
[0m10:40:55.906222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e831bf36-10d1-4665-8a18-6c43c1abeed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A7FAEE00>]}
[0m10:40:55.908898 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:40:56.556993 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m10:40:56.848224 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:40:56.848914 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:40:56.948467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e831bf36-10d1-4665-8a18-6c43c1abeed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A9A4D650>]}
[0m10:40:57.122799 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:40:57.128050 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:40:57.174856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e831bf36-10d1-4665-8a18-6c43c1abeed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A7F9FD40>]}
[0m10:40:57.175753 [info ] [MainThread]: Found 13 models, 5 data tests, 5 sources, 459 macros
[0m10:40:57.176807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e831bf36-10d1-4665-8a18-6c43c1abeed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A82975B0>]}
[0m10:40:57.180287 [info ] [MainThread]: 
[0m10:40:57.181070 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:40:57.182029 [info ] [MainThread]: 
[0m10:40:57.183518 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:40:57.192877 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m10:40:57.373536 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m10:40:57.374254 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m10:40:57.374820 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:40:57.548136 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.173 seconds
[0m10:40:57.550853 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m10:40:57.555396 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m10:40:57.566465 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:40:57.567564 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m10:40:57.568481 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:40:57.665051 [debug] [ThreadPool]: SQL status: BEGIN in 0.096 seconds
[0m10:40:57.665834 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:40:57.666647 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m10:40:57.682749 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.015 seconds
[0m10:40:57.686794 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m10:40:57.688274 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m10:40:57.701071 [debug] [MainThread]: Using postgres connection "master"
[0m10:40:57.701951 [debug] [MainThread]: On master: BEGIN
[0m10:40:57.702505 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:40:57.835658 [debug] [MainThread]: SQL status: BEGIN in 0.133 seconds
[0m10:40:57.836456 [debug] [MainThread]: Using postgres connection "master"
[0m10:40:57.837195 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:40:57.850913 [debug] [MainThread]: SQL status: SELECT 5 in 0.013 seconds
[0m10:40:57.853907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e831bf36-10d1-4665-8a18-6c43c1abeed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A9D30EF0>]}
[0m10:40:57.854701 [debug] [MainThread]: On master: ROLLBACK
[0m10:40:57.855800 [debug] [MainThread]: Using postgres connection "master"
[0m10:40:57.856436 [debug] [MainThread]: On master: BEGIN
[0m10:40:57.857773 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m10:40:57.858395 [debug] [MainThread]: On master: COMMIT
[0m10:40:57.858932 [debug] [MainThread]: Using postgres connection "master"
[0m10:40:57.859419 [debug] [MainThread]: On master: COMMIT
[0m10:40:57.860230 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:40:57.860818 [debug] [MainThread]: On master: Close
[0m10:40:57.869679 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:40:57.870723 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m10:40:57.871868 [info ] [Thread-2 (]: 2 of 5 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m10:40:57.874233 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m10:40:57.872913 [info ] [Thread-1 (]: 1 of 5 START sql table model analytics.fct_shipments ........................... [RUN]
[0m10:40:57.875224 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:40:57.876274 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m10:40:57.890644 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:57.891669 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m10:40:57.897833 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:57.899858 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:40:57.967602 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m10:40:57.992846 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:57.992199 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:57.996761 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:57.997561 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:57.998363 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m10:40:57.999316 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m10:40:58.000460 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:40:58.001444 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:40:58.112241 [debug] [Thread-1 (]: SQL status: BEGIN in 0.111 seconds
[0m10:40:58.113029 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:58.113723 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."raw"."shipments"
),

assignments as (
    select
        shipment_id,
        courier_id,
        delivered_at
    from "delivery_analytics"."raw"."courier_assignments"
)

select
    s.shipment_id,
    a.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    a.delivered_at,
    s.price,

    case
        when a.delivered_at is null then null
        when a.delivered_at::date > s.expected_delivery_date::date then true
        else false
    end as late_delivery

from shipments s
left join assignments a
    on s.shipment_id = a.shipment_id
  );
  
[0m10:40:58.121128 [debug] [Thread-2 (]: SQL status: BEGIN in 0.121 seconds
[0m10:40:58.121927 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:58.122660 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    s.shipment_id,
    ca.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    ca.delivered_at,
    s.price,

    case
        when ca.delivered_at is null then null
        when ca.delivered_at::date > s.expected_delivery_date then true
        else false
    end as late_delivery

from "delivery_analytics"."raw"."shipments" s
left join "delivery_analytics"."raw"."courier_assignments" ca
    on s.shipment_id = ca.shipment_id
  );
  
[0m10:40:58.130875 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.016 seconds
[0m10:40:58.147970 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:58.148914 [debug] [Thread-2 (]: SQL status: SELECT 6 in 0.025 seconds
[0m10:40:58.150473 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m10:40:58.160239 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:58.161264 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue" rename to "fct_shipments_revenue__dbt_backup"
[0m10:40:58.162223 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:40:58.163048 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:40:58.168254 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:58.172889 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:58.173762 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m10:40:58.174788 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp" rename to "fct_shipments_revenue"
[0m10:40:58.177680 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m10:40:58.178465 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m10:40:58.297297 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:40:58.301159 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m10:40:58.302223 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:58.303120 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:58.303968 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:40:58.304721 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m10:40:58.307329 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m10:40:58.318328 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m10:40:58.319148 [debug] [Thread-2 (]: SQL status: COMMIT in 0.014 seconds
[0m10:40:58.328174 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:40:58.333088 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup"
[0m10:40:58.334151 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m10:40:58.335560 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:40:58.336513 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup" cascade
[0m10:40:58.345543 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.008 seconds
[0m10:40:58.346373 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.010 seconds
[0m10:40:58.351264 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m10:40:58.353996 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m10:40:58.358439 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e831bf36-10d1-4665-8a18-6c43c1abeed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A9AF3A10>]}
[0m10:40:58.359142 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e831bf36-10d1-4665-8a18-6c43c1abeed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A9B3F330>]}
[0m10:40:58.360699 [info ] [Thread-2 (]: 2 of 5 OK created sql table model analytics.fct_shipments_revenue .............. [[32mSELECT 6[0m in 0.48s]
[0m10:40:58.363502 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:40:58.362141 [info ] [Thread-1 (]: 1 of 5 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 6[0m in 0.48s]
[0m10:40:58.365375 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m10:40:58.367588 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:40:58.368790 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m10:40:58.370045 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m10:40:58.371105 [info ] [Thread-2 (]: 5 of 5 START sql table model analytics.fct_shipments_sla ....................... [RUN]
[0m10:40:58.372114 [info ] [Thread-4 (]: 4 of 5 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m10:40:58.374660 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments_revenue, now model.dbt_delivery_analytics.fct_shipments_sla)
[0m10:40:58.376141 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m10:40:58.373454 [info ] [Thread-3 (]: 3 of 5 START sql table model analytics.courier_performance ..................... [RUN]
[0m10:40:58.377341 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:40:58.378175 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m10:40:58.379248 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m10:40:58.385328 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:40:58.389929 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:40:58.390857 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m10:40:58.396541 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m10:40:58.398490 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:40:58.399533 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m10:40:58.408322 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:40:58.414377 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:40:58.416593 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m10:40:58.423717 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m10:40:58.425761 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:40:58.426627 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m10:40:58.427480 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:40:58.428338 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:40:58.429381 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: BEGIN
[0m10:40:58.430745 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m10:40:58.431636 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:40:58.432887 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m10:40:58.433927 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m10:40:58.535340 [debug] [Thread-4 (]: SQL status: BEGIN in 0.107 seconds
[0m10:40:58.536178 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:40:58.536840 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date,
    status,
    count(*) as shipments_count
from fct
group by shipment_date, status
order by shipment_date, status
  );
  
[0m10:40:58.538996 [debug] [Thread-4 (]: Postgres adapter: Postgres error: column "status" does not exist
LINE 19:     status,
             ^

[0m10:40:58.539750 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: ROLLBACK
[0m10:40:58.541085 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m10:40:58.546874 [debug] [Thread-2 (]: SQL status: BEGIN in 0.116 seconds
[0m10:40:58.547866 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:40:58.548704 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,

        case
            when delivered_at is null then null
            else (delivered_at::date - expected_delivery_date::date)
        end as delivery_delay_days,

        case
            when delivered_at is null then null
            when delivered_at::date > expected_delivery_date::date then true
            else false
        end as late_delivery

    from "delivery_analytics"."analytics"."fct_shipments"
)

select *
from sla
  );
  
[0m10:40:58.552292 [debug] [Thread-3 (]: SQL status: BEGIN in 0.118 seconds
[0m10:40:58.553261 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:40:58.554044 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id
  );
  
[0m10:40:58.560380 [debug] [Thread-4 (]: Database Error in model daily_shipments_status (models\marts\analytics\daily_shipments_status.sql)
  column "status" does not exist
  LINE 19:     status,
               ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\daily_shipments_status.sql
[0m10:40:58.561492 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e831bf36-10d1-4665-8a18-6c43c1abeed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A9F4E5D0>]}
[0m10:40:58.563543 [debug] [Thread-2 (]: SQL status: SELECT 6 in 0.013 seconds
[0m10:40:58.562762 [error] [Thread-4 (]: 4 of 5 ERROR creating sql table model analytics.daily_shipments_status ......... [[31mERROR[0m in 0.19s]
[0m10:40:58.570575 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:40:58.571532 [debug] [Thread-3 (]: SQL status: SELECT 3 in 0.017 seconds
[0m10:40:58.573042 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m10:40:58.574081 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla" rename to "fct_shipments_sla__dbt_backup"
[0m10:40:58.580390 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:40:58.581859 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.daily_shipments_status' to be skipped because of status 'error'.  Reason: Database Error in model daily_shipments_status (models\marts\analytics\daily_shipments_status.sql)
  column "status" does not exist
  LINE 19:     status,
               ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\daily_shipments_status.sql.
[0m10:40:58.583161 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance" rename to "courier_performance__dbt_backup"
[0m10:40:58.584119 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:40:58.590962 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:40:58.591744 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m10:40:58.592571 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp" rename to "fct_shipments_sla"
[0m10:40:58.597502 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:40:58.598647 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance__dbt_tmp" rename to "courier_performance"
[0m10:40:58.600242 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m10:40:58.601352 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m10:40:58.604288 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m10:40:58.607075 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m10:40:58.607984 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:40:58.608975 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:40:58.609872 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m10:40:58.610746 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m10:40:58.614021 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m10:40:58.614880 [debug] [Thread-3 (]: SQL status: COMMIT in 0.003 seconds
[0m10:40:58.619266 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup"
[0m10:40:58.623324 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."courier_performance__dbt_backup"
[0m10:40:58.624896 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:40:58.626511 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:40:58.627442 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup" cascade
[0m10:40:58.628373 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
drop table if exists "delivery_analytics"."analytics"."courier_performance__dbt_backup" cascade
[0m10:40:58.640318 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.011 seconds
[0m10:40:58.641357 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.012 seconds
[0m10:40:58.644048 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m10:40:58.646549 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: Close
[0m10:40:58.647883 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e831bf36-10d1-4665-8a18-6c43c1abeed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A96BF450>]}
[0m10:40:58.648969 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e831bf36-10d1-4665-8a18-6c43c1abeed8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0AA000150>]}
[0m10:40:58.650628 [info ] [Thread-3 (]: 3 of 5 OK created sql table model analytics.courier_performance ................ [[32mSELECT 3[0m in 0.27s]
[0m10:40:58.653543 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m10:40:58.652040 [info ] [Thread-2 (]: 5 of 5 OK created sql table model analytics.fct_shipments_sla .................. [[32mSELECT 6[0m in 0.27s]
[0m10:40:58.655449 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:40:58.658460 [debug] [MainThread]: Using postgres connection "master"
[0m10:40:58.659190 [debug] [MainThread]: On master: BEGIN
[0m10:40:58.659775 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:40:58.746327 [debug] [MainThread]: SQL status: BEGIN in 0.086 seconds
[0m10:40:58.747223 [debug] [MainThread]: On master: COMMIT
[0m10:40:58.747783 [debug] [MainThread]: Using postgres connection "master"
[0m10:40:58.748387 [debug] [MainThread]: On master: COMMIT
[0m10:40:58.749631 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:40:58.750376 [debug] [MainThread]: On master: Close
[0m10:40:58.751450 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:40:58.752068 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m10:40:58.752558 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m10:40:58.753073 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_sla' was properly closed.
[0m10:40:58.753523 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments' was properly closed.
[0m10:40:58.754048 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m10:40:58.754481 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m10:40:58.755143 [info ] [MainThread]: 
[0m10:40:58.756236 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 1.57 seconds (1.57s).
[0m10:40:58.759043 [debug] [MainThread]: Command end result
[0m10:40:58.807527 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:40:58.812898 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:40:58.829032 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m10:40:58.829731 [info ] [MainThread]: 
[0m10:40:58.830810 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m10:40:58.831994 [info ] [MainThread]: 
[0m10:40:58.833332 [error] [MainThread]: [31mFailure in model daily_shipments_status (models\marts\analytics\daily_shipments_status.sql)[0m
[0m10:40:58.834511 [error] [MainThread]:   Database Error in model daily_shipments_status (models\marts\analytics\daily_shipments_status.sql)
  column "status" does not exist
  LINE 19:     status,
               ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\daily_shipments_status.sql
[0m10:40:58.835570 [info ] [MainThread]: 
[0m10:40:58.836899 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\daily_shipments_status.sql
[0m10:40:58.838170 [info ] [MainThread]: 
[0m10:40:58.839231 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=5
[0m10:40:58.842111 [debug] [MainThread]: Command `dbt run` failed at 10:40:58.841737 after 3.79 seconds
[0m10:40:58.843079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A6C34A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A8151550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0A9673C50>]}
[0m10:40:58.843822 [debug] [MainThread]: Flushing usage events
[0m10:41:00.314566 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:44:03.134535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF0F2E0980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF0FD8CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF103D7D90>]}


============================== 10:44:03.174381 | 5d50b152-b0bd-4597-a770-b3ffb10514f4 ==============================
[0m10:44:03.174381 [info ] [MainThread]: Running with dbt=1.10.15
[0m10:44:03.176063 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select marts.analytics', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m10:44:03.532819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5d50b152-b0bd-4597-a770-b3ffb10514f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF0F748770>]}
[0m10:44:03.658597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5d50b152-b0bd-4597-a770-b3ffb10514f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF1027EE00>]}
[0m10:44:03.660517 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m10:44:04.327505 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m10:44:04.641978 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:44:04.643294 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\daily_shipments_status.sql
[0m10:44:05.219841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5d50b152-b0bd-4597-a770-b3ffb10514f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF11CED850>]}
[0m10:44:05.388341 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:44:05.394686 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:44:05.527534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5d50b152-b0bd-4597-a770-b3ffb10514f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF1026FD40>]}
[0m10:44:05.528526 [info ] [MainThread]: Found 13 models, 5 data tests, 5 sources, 459 macros
[0m10:44:05.529715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5d50b152-b0bd-4597-a770-b3ffb10514f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF11CC6DD0>]}
[0m10:44:05.533315 [info ] [MainThread]: 
[0m10:44:05.534301 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m10:44:05.535305 [info ] [MainThread]: 
[0m10:44:05.536704 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m10:44:05.546397 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m10:44:05.735727 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m10:44:05.736452 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m10:44:05.737009 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:44:05.938373 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.201 seconds
[0m10:44:05.941252 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m10:44:05.945610 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m10:44:05.957412 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:44:05.958337 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m10:44:05.958856 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:44:06.065367 [debug] [ThreadPool]: SQL status: BEGIN in 0.106 seconds
[0m10:44:06.066454 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m10:44:06.067172 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m10:44:06.083183 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.015 seconds
[0m10:44:06.085968 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m10:44:06.086983 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m10:44:06.100803 [debug] [MainThread]: Using postgres connection "master"
[0m10:44:06.101523 [debug] [MainThread]: On master: BEGIN
[0m10:44:06.102100 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:44:06.209329 [debug] [MainThread]: SQL status: BEGIN in 0.107 seconds
[0m10:44:06.210069 [debug] [MainThread]: Using postgres connection "master"
[0m10:44:06.210749 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m10:44:06.231351 [debug] [MainThread]: SQL status: SELECT 5 in 0.020 seconds
[0m10:44:06.235248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5d50b152-b0bd-4597-a770-b3ffb10514f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF11FDBEE0>]}
[0m10:44:06.236135 [debug] [MainThread]: On master: ROLLBACK
[0m10:44:06.237323 [debug] [MainThread]: Using postgres connection "master"
[0m10:44:06.238340 [debug] [MainThread]: On master: BEGIN
[0m10:44:06.239862 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m10:44:06.240570 [debug] [MainThread]: On master: COMMIT
[0m10:44:06.241236 [debug] [MainThread]: Using postgres connection "master"
[0m10:44:06.241764 [debug] [MainThread]: On master: COMMIT
[0m10:44:06.242780 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:44:06.243580 [debug] [MainThread]: On master: Close
[0m10:44:06.252117 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m10:44:06.253107 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m10:44:06.254146 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:44:06.255559 [info ] [Thread-1 (]: 1 of 5 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m10:44:06.257027 [info ] [Thread-2 (]: 2 of 5 START sql table model analytics.fct_shipments ........................... [RUN]
[0m10:44:06.260015 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m10:44:06.261194 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m10:44:06.258599 [info ] [Thread-3 (]: 3 of 5 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m10:44:06.262234 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m10:44:06.263087 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m10:44:06.264160 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m10:44:06.278002 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:44:06.284464 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:44:06.285451 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:44:06.293629 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:44:06.295763 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m10:44:06.296699 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m10:44:06.403668 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:44:06.405018 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:44:06.406396 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m10:44:06.413107 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:44:06.417101 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:44:06.418075 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:44:06.419164 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:44:06.420317 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m10:44:06.449093 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m10:44:06.450356 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m10:44:06.451275 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m10:44:06.452272 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:44:06.453239 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m10:44:06.609371 [debug] [Thread-2 (]: SQL status: BEGIN in 0.158 seconds
[0m10:44:06.610264 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:44:06.611024 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."raw"."shipments"
),

assignments as (
    select
        shipment_id,
        courier_id,
        delivered_at
    from "delivery_analytics"."raw"."courier_assignments"
)

select
    s.shipment_id,
    a.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    a.delivered_at,
    s.price,

    case
        when a.delivered_at is null then null
        when a.delivered_at::date > s.expected_delivery_date::date then true
        else false
    end as late_delivery

from shipments s
left join assignments a
    on s.shipment_id = a.shipment_id
  );
  
[0m10:44:06.613607 [debug] [Thread-1 (]: SQL status: BEGIN in 0.161 seconds
[0m10:44:06.614367 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:44:06.615027 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    select
    status_timestamp::date as shipment_date,
    status,
    count(*) as shipments_count
from "delivery_analytics"."analytics"."stg_shipment_status"
group by
    status_timestamp::date,
    status
order by
    shipment_date,
    status
  );
  
[0m10:44:06.621013 [debug] [Thread-3 (]: SQL status: BEGIN in 0.168 seconds
[0m10:44:06.621924 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:44:06.622725 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    s.shipment_id,
    ca.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    ca.delivered_at,
    s.price,

    case
        when ca.delivered_at is null then null
        when ca.delivered_at::date > s.expected_delivery_date then true
        else false
    end as late_delivery

from "delivery_analytics"."raw"."shipments" s
left join "delivery_analytics"."raw"."courier_assignments" ca
    on s.shipment_id = ca.shipment_id
  );
  
[0m10:44:06.627507 [debug] [Thread-2 (]: SQL status: SELECT 6 in 0.016 seconds
[0m10:44:06.645452 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:44:06.646326 [debug] [Thread-1 (]: SQL status: SELECT 5 in 0.031 seconds
[0m10:44:06.647161 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m10:44:06.647981 [debug] [Thread-3 (]: SQL status: SELECT 6 in 0.024 seconds
[0m10:44:06.652989 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:44:06.658933 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:44:06.659876 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m10:44:06.660784 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.007 seconds
[0m10:44:06.661812 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue" rename to "fct_shipments_revenue__dbt_backup"
[0m10:44:06.667196 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:44:06.668056 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m10:44:06.669101 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m10:44:06.674041 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:44:06.674899 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.006 seconds
[0m10:44:06.676061 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m10:44:06.680820 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:44:06.681823 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.006 seconds
[0m10:44:06.682900 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp" rename to "fct_shipments_revenue"
[0m10:44:06.697171 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.014 seconds
[0m10:44:06.714882 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:44:06.715900 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m10:44:06.718723 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m10:44:06.719634 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:44:06.723501 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m10:44:06.724451 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:44:06.725461 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m10:44:06.726465 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:44:06.727488 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m10:44:06.728742 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m10:44:06.730526 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m10:44:06.741840 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m10:44:06.742736 [debug] [Thread-1 (]: SQL status: COMMIT in 0.013 seconds
[0m10:44:06.752347 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m10:44:06.753132 [debug] [Thread-3 (]: SQL status: COMMIT in 0.023 seconds
[0m10:44:06.758063 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m10:44:06.759007 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m10:44:06.763000 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup"
[0m10:44:06.764507 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m10:44:06.766268 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m10:44:06.767279 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m10:44:06.768181 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup" cascade
[0m10:44:06.773122 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.008 seconds
[0m10:44:06.777639 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m10:44:06.778533 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.009 seconds
[0m10:44:06.779337 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.010 seconds
[0m10:44:06.782471 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m10:44:06.787392 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m10:44:06.790205 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d50b152-b0bd-4597-a770-b3ffb10514f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF121FD190>]}
[0m10:44:06.791120 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d50b152-b0bd-4597-a770-b3ffb10514f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF1214C7E0>]}
[0m10:44:06.791879 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d50b152-b0bd-4597-a770-b3ffb10514f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF1214CCB0>]}
[0m10:44:06.793152 [info ] [Thread-2 (]: 2 of 5 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 6[0m in 0.52s]
[0m10:44:06.797558 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m10:44:06.794506 [info ] [Thread-3 (]: 3 of 5 OK created sql table model analytics.fct_shipments_revenue .............. [[32mSELECT 6[0m in 0.53s]
[0m10:44:06.796031 [info ] [Thread-1 (]: 1 of 5 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 5[0m in 0.53s]
[0m10:44:06.800319 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m10:44:06.801159 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:44:06.801987 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m10:44:06.803196 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m10:44:06.804751 [info ] [Thread-2 (]: 5 of 5 START sql table model analytics.fct_shipments_sla ....................... [RUN]
[0m10:44:06.805940 [info ] [Thread-4 (]: 4 of 5 START sql table model analytics.courier_performance ..................... [RUN]
[0m10:44:06.807819 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.fct_shipments_sla)
[0m10:44:06.809241 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_performance'
[0m10:44:06.810170 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:44:06.811109 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m10:44:06.816084 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:44:06.820816 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m10:44:06.825659 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:44:06.826510 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m10:44:06.832006 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:44:06.846769 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m10:44:06.850760 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:44:06.851670 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:44:06.852513 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m10:44:06.853582 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: BEGIN
[0m10:44:06.854532 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:44:06.855775 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m10:44:06.961954 [debug] [Thread-2 (]: SQL status: BEGIN in 0.106 seconds
[0m10:44:06.962863 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:44:06.963807 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,

        case
            when delivered_at is null then null
            else (delivered_at::date - expected_delivery_date::date)
        end as delivery_delay_days,

        case
            when delivered_at is null then null
            when delivered_at::date > expected_delivery_date::date then true
            else false
        end as late_delivery

    from "delivery_analytics"."analytics"."fct_shipments"
)

select *
from sla
  );
  
[0m10:44:06.966996 [debug] [Thread-4 (]: SQL status: BEGIN in 0.113 seconds
[0m10:44:06.967817 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:44:06.968452 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id
  );
  
[0m10:44:06.978489 [debug] [Thread-2 (]: SQL status: SELECT 6 in 0.014 seconds
[0m10:44:06.983742 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:44:06.984529 [debug] [Thread-4 (]: SQL status: SELECT 3 in 0.015 seconds
[0m10:44:06.985378 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla" rename to "fct_shipments_sla__dbt_backup"
[0m10:44:06.991237 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:44:06.992258 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance" rename to "courier_performance__dbt_backup"
[0m10:44:06.993010 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:44:06.998002 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:44:06.998791 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m10:44:06.999565 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp" rename to "fct_shipments_sla"
[0m10:44:07.003998 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:44:07.005194 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance__dbt_tmp" rename to "courier_performance"
[0m10:44:07.006615 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m10:44:07.007416 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m10:44:07.010281 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m10:44:07.013167 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m10:44:07.014086 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:44:07.015019 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:44:07.015852 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m10:44:07.016881 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m10:44:07.019400 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m10:44:07.024128 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup"
[0m10:44:07.024923 [debug] [Thread-4 (]: SQL status: COMMIT in 0.007 seconds
[0m10:44:07.026383 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m10:44:07.030015 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."courier_performance__dbt_backup"
[0m10:44:07.030933 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup" cascade
[0m10:44:07.032564 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m10:44:07.033747 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
drop table if exists "delivery_analytics"."analytics"."courier_performance__dbt_backup" cascade
[0m10:44:07.042838 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.008 seconds
[0m10:44:07.043748 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.010 seconds
[0m10:44:07.046348 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m10:44:07.048934 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: Close
[0m10:44:07.050255 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d50b152-b0bd-4597-a770-b3ffb10514f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF120F2ED0>]}
[0m10:44:07.052455 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d50b152-b0bd-4597-a770-b3ffb10514f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF1236A580>]}
[0m10:44:07.051496 [info ] [Thread-4 (]: 4 of 5 OK created sql table model analytics.courier_performance ................ [[32mSELECT 3[0m in 0.24s]
[0m10:44:07.055451 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m10:44:07.054166 [info ] [Thread-2 (]: 5 of 5 OK created sql table model analytics.fct_shipments_sla .................. [[32mSELECT 6[0m in 0.24s]
[0m10:44:07.057584 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_sla
[0m10:44:07.060647 [debug] [MainThread]: Using postgres connection "master"
[0m10:44:07.061367 [debug] [MainThread]: On master: BEGIN
[0m10:44:07.061944 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m10:44:07.151061 [debug] [MainThread]: SQL status: BEGIN in 0.089 seconds
[0m10:44:07.152045 [debug] [MainThread]: On master: COMMIT
[0m10:44:07.152639 [debug] [MainThread]: Using postgres connection "master"
[0m10:44:07.153164 [debug] [MainThread]: On master: COMMIT
[0m10:44:07.154247 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m10:44:07.155245 [debug] [MainThread]: On master: Close
[0m10:44:07.156275 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:44:07.156877 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m10:44:07.157353 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m10:44:07.157863 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m10:44:07.158319 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_sla' was properly closed.
[0m10:44:07.158840 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_revenue' was properly closed.
[0m10:44:07.159272 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m10:44:07.159939 [info ] [MainThread]: 
[0m10:44:07.161154 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 1.62 seconds (1.62s).
[0m10:44:07.163891 [debug] [MainThread]: Command end result
[0m10:44:07.210547 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m10:44:07.215892 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m10:44:07.229130 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m10:44:07.230004 [info ] [MainThread]: 
[0m10:44:07.231306 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:44:07.232348 [info ] [MainThread]: 
[0m10:44:07.233523 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m10:44:07.235554 [debug] [MainThread]: Command `dbt run` succeeded at 10:44:07.235324 after 4.40 seconds
[0m10:44:07.236236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF0F699B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF11BDA5D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CF10218830>]}
[0m10:44:07.237133 [debug] [MainThread]: Flushing usage events
[0m10:44:08.682488 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:11:41.233653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4B1F0980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4BC9CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4C2EBD90>]}


============================== 13:11:41.239672 | 2a10f594-1669-464c-affb-f70c792e2702 ==============================
[0m13:11:41.239672 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:11:41.241117 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select marts.analytics', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:11:41.504662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2a10f594-1669-464c-affb-f70c792e2702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4B658770>]}
[0m13:11:41.589944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2a10f594-1669-464c-affb-f70c792e2702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4C18EE00>]}
[0m13:11:41.591853 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:11:42.176270 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:11:42.441229 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m13:11:42.442025 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\analytics\shipments_by_hour_day.sql
[0m13:11:42.843258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2a10f594-1669-464c-affb-f70c792e2702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4DC05B50>]}
[0m13:11:43.055105 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:11:43.059915 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:11:43.095891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2a10f594-1669-464c-affb-f70c792e2702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4C17FD40>]}
[0m13:11:43.097098 [info ] [MainThread]: Found 14 models, 5 data tests, 5 sources, 459 macros
[0m13:11:43.098011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a10f594-1669-464c-affb-f70c792e2702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A481CB150>]}
[0m13:11:43.100786 [info ] [MainThread]: 
[0m13:11:43.102176 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:11:43.102940 [info ] [MainThread]: 
[0m13:11:43.103873 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m13:11:43.109596 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m13:11:43.304344 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m13:11:43.304888 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m13:11:43.305314 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:11:43.453190 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.148 seconds
[0m13:11:43.455007 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m13:11:43.458502 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m13:11:43.467738 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m13:11:43.468310 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m13:11:43.468782 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:11:43.539356 [debug] [ThreadPool]: SQL status: BEGIN in 0.070 seconds
[0m13:11:43.540182 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m13:11:43.540607 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m13:11:43.552204 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.011 seconds
[0m13:11:43.555181 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m13:11:43.556040 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m13:11:43.567290 [debug] [MainThread]: Using postgres connection "master"
[0m13:11:43.567853 [debug] [MainThread]: On master: BEGIN
[0m13:11:43.568260 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:11:43.693049 [debug] [MainThread]: SQL status: BEGIN in 0.125 seconds
[0m13:11:43.693646 [debug] [MainThread]: Using postgres connection "master"
[0m13:11:43.694121 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m13:11:43.706926 [debug] [MainThread]: SQL status: SELECT 5 in 0.012 seconds
[0m13:11:43.709432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a10f594-1669-464c-affb-f70c792e2702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4E029160>]}
[0m13:11:43.710078 [debug] [MainThread]: On master: ROLLBACK
[0m13:11:43.711063 [debug] [MainThread]: Using postgres connection "master"
[0m13:11:43.711700 [debug] [MainThread]: On master: BEGIN
[0m13:11:43.713018 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m13:11:43.713586 [debug] [MainThread]: On master: COMMIT
[0m13:11:43.714096 [debug] [MainThread]: Using postgres connection "master"
[0m13:11:43.714496 [debug] [MainThread]: On master: COMMIT
[0m13:11:43.715182 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:11:43.715643 [debug] [MainThread]: On master: Close
[0m13:11:43.721899 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:11:43.722679 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m13:11:43.723414 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:11:43.724046 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m13:11:43.724801 [info ] [Thread-3 (]: 3 of 6 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m13:11:43.725446 [info ] [Thread-2 (]: 2 of 6 START sql table model analytics.fct_shipments ........................... [RUN]
[0m13:11:43.728060 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m13:11:43.726340 [info ] [Thread-4 (]: 4 of 6 START sql table model analytics.shipments_by_hour_day ................... [RUN]
[0m13:11:43.729124 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m13:11:43.727197 [info ] [Thread-1 (]: 1 of 6 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m13:11:43.729989 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:11:43.730957 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.shipments_by_hour_day'
[0m13:11:43.731681 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m13:11:43.732414 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m13:11:43.742604 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:11:43.743312 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:11:43.748591 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m13:11:43.749387 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m13:11:43.753011 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:11:43.757096 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:11:43.758739 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:11:43.817705 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m13:11:43.820907 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:11:43.821474 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:11:43.822981 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m13:11:43.852504 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:11:43.856695 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:11:43.860732 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m13:11:43.862314 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:11:43.863560 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m13:11:43.864283 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:11:43.865743 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:11:43.866530 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:11:43.867378 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m13:11:43.867894 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: BEGIN
[0m13:11:43.868528 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:11:43.869124 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:11:43.869748 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:11:43.870318 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m13:11:43.871348 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:11:43.949363 [debug] [Thread-3 (]: SQL status: BEGIN in 0.085 seconds
[0m13:11:43.950139 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:11:43.950790 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    s.shipment_id,
    ca.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    ca.delivered_at,
    s.price,

    case
        when ca.delivered_at is null then null
        when ca.delivered_at::date > s.expected_delivery_date then true
        else false
    end as late_delivery

from "delivery_analytics"."raw"."shipments" s
left join "delivery_analytics"."raw"."courier_assignments" ca
    on s.shipment_id = ca.shipment_id
  );
  
[0m13:11:43.957886 [debug] [Thread-1 (]: SQL status: BEGIN in 0.089 seconds
[0m13:11:43.958460 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:11:43.958943 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    select
    status_timestamp::date as shipment_date,
    status,
    count(*) as shipments_count
from "delivery_analytics"."analytics"."stg_shipment_status"
group by
    status_timestamp::date,
    status
order by
    shipment_date,
    status
  );
  
[0m13:11:43.964115 [debug] [Thread-4 (]: SQL status: BEGIN in 0.094 seconds
[0m13:11:43.964735 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:11:43.965219 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */

  
    

  create  table "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select
        shipment_id,
        shipment_date,
        extract(hour from shipment_date) as shipment_hour,
        extract(dow from shipment_date) as day_of_week,
        delivered_at,
        expected_delivery_date
    from "delivery_analytics"."raw"."shipments"
)

select
    day_of_week,
    shipment_hour,
    count(distinct shipment_id) as total_shipments,
    sum(case when delivered_at is not null and delivered_at::date <= expected_delivery_date::date then 1 else 0 end) as on_time_deliveries,
    sum(case when delivered_at::date > expected_delivery_date::date then 1 else 0 end) as late_deliveries
from fct
group by day_of_week, shipment_hour
order by day_of_week, shipment_hour
  );
  
[0m13:11:43.966809 [debug] [Thread-3 (]: SQL status: SELECT 6 in 0.015 seconds
[0m13:11:43.967338 [debug] [Thread-2 (]: SQL status: BEGIN in 0.096 seconds
[0m13:11:43.973267 [debug] [Thread-4 (]: Postgres adapter: Postgres error: column "delivered_at" does not exist
LINE 18:         delivered_at,
                 ^

[0m13:11:43.981082 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:11:43.981668 [debug] [Thread-1 (]: SQL status: SELECT 5 in 0.022 seconds
[0m13:11:43.982201 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:11:43.982883 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: ROLLBACK
[0m13:11:43.983521 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue" rename to "fct_shipments_revenue__dbt_backup"
[0m13:11:43.987568 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:11:43.988998 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."raw"."shipments"
),

assignments as (
    select
        shipment_id,
        courier_id,
        delivered_at
    from "delivery_analytics"."raw"."courier_assignments"
)

select
    s.shipment_id,
    a.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    a.delivered_at,
    s.price,

    case
        when a.delivered_at is null then null
        when a.delivered_at::date > s.expected_delivery_date::date then true
        else false
    end as late_delivery

from shipments s
left join assignments a
    on s.shipment_id = a.shipment_id
  );
  
[0m13:11:43.990105 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m13:11:43.990979 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: Close
[0m13:11:43.991592 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m13:11:43.995626 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:11:43.997693 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m13:11:43.998399 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp" rename to "fct_shipments_revenue"
[0m13:11:44.002132 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:11:44.002906 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m13:11:44.003731 [debug] [Thread-2 (]: SQL status: SELECT 6 in 0.011 seconds
[0m13:11:44.004704 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m13:11:44.005379 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:11:44.009415 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:11:44.031691 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m13:11:44.033939 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m13:11:44.034710 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m13:11:44.035896 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:11:44.036887 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:11:44.037575 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m13:11:44.038191 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m13:11:44.038805 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:11:44.042768 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:11:44.043286 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m13:11:44.043828 [debug] [Thread-3 (]: SQL status: COMMIT in 0.004 seconds
[0m13:11:44.044479 [debug] [Thread-1 (]: SQL status: COMMIT in 0.005 seconds
[0m13:11:44.051069 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.006 seconds
[0m13:11:44.059504 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup"
[0m13:11:44.062578 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m13:11:44.065005 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m13:11:44.072100 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:11:44.073216 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:11:44.074218 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:11:44.075264 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup" cascade
[0m13:11:44.076472 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m13:11:44.077472 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m13:11:44.080048 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m13:11:44.083462 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m13:11:44.084455 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:11:44.085128 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m13:11:44.085842 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.007 seconds
[0m13:11:44.086404 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.008 seconds
[0m13:11:44.089695 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m13:11:44.091898 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m13:11:44.092528 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.007 seconds
[0m13:11:44.096952 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m13:11:44.099207 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a10f594-1669-464c-affb-f70c792e2702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4E1207D0>]}
[0m13:11:44.099835 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a10f594-1669-464c-affb-f70c792e2702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4E0E0C00>]}
[0m13:11:44.101310 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a10f594-1669-464c-affb-f70c792e2702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4DD05270>]}
[0m13:11:44.100826 [info ] [Thread-1 (]: 1 of 6 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 5[0m in 0.36s]
[0m13:11:44.102714 [info ] [Thread-3 (]: 3 of 6 OK created sql table model analytics.fct_shipments_revenue .............. [[32mSELECT 6[0m in 0.37s]
[0m13:11:44.105130 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m13:11:44.103683 [info ] [Thread-2 (]: 2 of 6 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 6[0m in 0.37s]
[0m13:11:44.109774 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:11:44.110794 [debug] [Thread-4 (]: Database Error in model shipments_by_hour_day (models\marts\analytics\shipments_by_hour_day.sql)
  column "delivered_at" does not exist
  LINE 18:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\shipments_by_hour_day.sql
[0m13:11:44.112266 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m13:11:44.113486 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a10f594-1669-464c-affb-f70c792e2702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4DDBE850>]}
[0m13:11:44.115583 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m13:11:44.116346 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:11:44.114909 [error] [Thread-4 (]: 4 of 6 ERROR creating sql table model analytics.shipments_by_hour_day .......... [[31mERROR[0m in 0.38s]
[0m13:11:44.117201 [info ] [Thread-1 (]: 5 of 6 START sql table model analytics.courier_performance ..................... [RUN]
[0m13:11:44.119099 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:11:44.117960 [info ] [Thread-3 (]: 6 of 6 START sql table model analytics.fct_shipments_sla ....................... [RUN]
[0m13:11:44.119941 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.daily_shipments_status, now model.dbt_delivery_analytics.courier_performance)
[0m13:11:44.120939 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.shipments_by_hour_day' to be skipped because of status 'error'.  Reason: Database Error in model shipments_by_hour_day (models\marts\analytics\shipments_by_hour_day.sql)
  column "delivered_at" does not exist
  LINE 18:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\shipments_by_hour_day.sql.
[0m13:11:44.121597 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments_revenue, now model.dbt_delivery_analytics.fct_shipments_sla)
[0m13:11:44.122250 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m13:11:44.123840 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:11:44.127494 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m13:11:44.131808 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:11:44.134862 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m13:11:44.135508 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:11:44.140486 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m13:11:44.144198 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:11:44.147750 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:11:44.148879 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:11:44.149658 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m13:11:44.150419 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: BEGIN
[0m13:11:44.151056 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:11:44.151770 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:11:44.228959 [debug] [Thread-1 (]: SQL status: BEGIN in 0.078 seconds
[0m13:11:44.229857 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:11:44.230455 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id
  );
  
[0m13:11:44.236363 [debug] [Thread-3 (]: SQL status: BEGIN in 0.085 seconds
[0m13:11:44.236947 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:11:44.237515 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,

        case
            when delivered_at is null then null
            else (delivered_at::date - expected_delivery_date::date)
        end as delivery_delay_days,

        case
            when delivered_at is null then null
            when delivered_at::date > expected_delivery_date::date then true
            else false
        end as late_delivery

    from "delivery_analytics"."analytics"."fct_shipments"
)

select *
from sla
  );
  
[0m13:11:44.241892 [debug] [Thread-1 (]: SQL status: SELECT 3 in 0.011 seconds
[0m13:11:44.247344 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:11:44.248079 [debug] [Thread-3 (]: SQL status: SELECT 6 in 0.010 seconds
[0m13:11:44.248672 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance" rename to "courier_performance__dbt_backup"
[0m13:11:44.252375 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:11:44.253073 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla" rename to "fct_shipments_sla__dbt_backup"
[0m13:11:44.253797 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:11:44.254312 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:11:44.257589 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:11:44.261332 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:11:44.261934 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance__dbt_tmp" rename to "courier_performance"
[0m13:11:44.262667 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp" rename to "fct_shipments_sla"
[0m13:11:44.264451 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:11:44.264950 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m13:11:44.266867 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m13:11:44.269009 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m13:11:44.269717 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:11:44.270396 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:11:44.271041 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m13:11:44.271605 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m13:11:44.273507 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m13:11:44.274135 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m13:11:44.276970 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup"
[0m13:11:44.281140 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."courier_performance__dbt_backup"
[0m13:11:44.282663 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:11:44.283684 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:11:44.284310 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup" cascade
[0m13:11:44.285014 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
drop table if exists "delivery_analytics"."analytics"."courier_performance__dbt_backup" cascade
[0m13:11:44.291600 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.006 seconds
[0m13:11:44.292201 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.006 seconds
[0m13:11:44.294168 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: Close
[0m13:11:44.296089 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m13:11:44.297228 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a10f594-1669-464c-affb-f70c792e2702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4C1756A0>]}
[0m13:11:44.297881 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a10f594-1669-464c-affb-f70c792e2702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4DC8B150>]}
[0m13:11:44.298814 [info ] [Thread-1 (]: 5 of 6 OK created sql table model analytics.courier_performance ................ [[32mSELECT 3[0m in 0.18s]
[0m13:11:44.301139 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m13:11:44.299799 [info ] [Thread-3 (]: 6 of 6 OK created sql table model analytics.fct_shipments_sla .................. [[32mSELECT 6[0m in 0.18s]
[0m13:11:44.306843 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:11:44.309244 [debug] [MainThread]: Using postgres connection "master"
[0m13:11:44.309772 [debug] [MainThread]: On master: BEGIN
[0m13:11:44.310309 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:11:44.405139 [debug] [MainThread]: SQL status: BEGIN in 0.095 seconds
[0m13:11:44.405792 [debug] [MainThread]: On master: COMMIT
[0m13:11:44.406579 [debug] [MainThread]: Using postgres connection "master"
[0m13:11:44.407123 [debug] [MainThread]: On master: COMMIT
[0m13:11:44.407763 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:11:44.408485 [debug] [MainThread]: On master: Close
[0m13:11:44.409446 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:11:44.409823 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m13:11:44.410273 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m13:11:44.410741 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_sla' was properly closed.
[0m13:11:44.411215 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments' was properly closed.
[0m13:11:44.411670 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipments_by_hour_day' was properly closed.
[0m13:11:44.412210 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m13:11:44.412951 [info ] [MainThread]: 
[0m13:11:44.413755 [info ] [MainThread]: Finished running 6 table models in 0 hours 0 minutes and 1.31 seconds (1.31s).
[0m13:11:44.415812 [debug] [MainThread]: Command end result
[0m13:11:44.450586 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:11:44.454948 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:11:44.465734 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m13:11:44.466290 [info ] [MainThread]: 
[0m13:11:44.467060 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m13:11:44.467778 [info ] [MainThread]: 
[0m13:11:44.468624 [error] [MainThread]: [31mFailure in model shipments_by_hour_day (models\marts\analytics\shipments_by_hour_day.sql)[0m
[0m13:11:44.469408 [error] [MainThread]:   Database Error in model shipments_by_hour_day (models\marts\analytics\shipments_by_hour_day.sql)
  column "delivered_at" does not exist
  LINE 18:         delivered_at,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\shipments_by_hour_day.sql
[0m13:11:44.470144 [info ] [MainThread]: 
[0m13:11:44.470878 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\shipments_by_hour_day.sql
[0m13:11:44.471534 [info ] [MainThread]: 
[0m13:11:44.472324 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=6
[0m13:11:44.474191 [debug] [MainThread]: Command `dbt run` failed at 13:11:44.473957 after 3.48 seconds
[0m13:11:44.475026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4BCBEB70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4E22E930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024A4E28AF30>]}
[0m13:11:44.475835 [debug] [MainThread]: Flushing usage events
[0m13:11:45.719762 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:13:38.965904 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA32B4980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA3D5CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA43A7D90>]}


============================== 13:13:38.971667 | 0c40b60f-65b8-429a-94de-e27961958084 ==============================
[0m13:13:38.971667 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:13:38.973001 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select marts.analytics', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:13:39.229022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0c40b60f-65b8-429a-94de-e27961958084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA3718770>]}
[0m13:13:39.319894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0c40b60f-65b8-429a-94de-e27961958084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA424EE00>]}
[0m13:13:39.321683 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:13:39.769521 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:13:40.059119 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:13:40.060222 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\shipments_by_hour_day.sql
[0m13:13:40.444103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0c40b60f-65b8-429a-94de-e27961958084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA5CE9B50>]}
[0m13:13:40.705199 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:13:40.711656 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:13:40.748714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0c40b60f-65b8-429a-94de-e27961958084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA423FD40>]}
[0m13:13:40.749469 [info ] [MainThread]: Found 14 models, 5 data tests, 5 sources, 459 macros
[0m13:13:40.750427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c40b60f-65b8-429a-94de-e27961958084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA028B150>]}
[0m13:13:40.753096 [info ] [MainThread]: 
[0m13:13:40.753731 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:13:40.754433 [info ] [MainThread]: 
[0m13:13:40.755427 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m13:13:40.761650 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m13:13:40.890702 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m13:13:40.891559 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m13:13:40.892197 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:13:40.970312 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.078 seconds
[0m13:13:40.972422 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m13:13:40.976757 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m13:13:40.984282 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m13:13:40.984811 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m13:13:40.985284 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:13:41.049615 [debug] [ThreadPool]: SQL status: BEGIN in 0.064 seconds
[0m13:13:41.050149 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m13:13:41.050588 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m13:13:41.060512 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.009 seconds
[0m13:13:41.062585 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m13:13:41.063356 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m13:13:41.073643 [debug] [MainThread]: Using postgres connection "master"
[0m13:13:41.074494 [debug] [MainThread]: On master: BEGIN
[0m13:13:41.075065 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:13:41.143645 [debug] [MainThread]: SQL status: BEGIN in 0.068 seconds
[0m13:13:41.144190 [debug] [MainThread]: Using postgres connection "master"
[0m13:13:41.144682 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m13:13:41.159353 [debug] [MainThread]: SQL status: SELECT 5 in 0.014 seconds
[0m13:13:41.162010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c40b60f-65b8-429a-94de-e27961958084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA6109160>]}
[0m13:13:41.162735 [debug] [MainThread]: On master: ROLLBACK
[0m13:13:41.163517 [debug] [MainThread]: Using postgres connection "master"
[0m13:13:41.163993 [debug] [MainThread]: On master: BEGIN
[0m13:13:41.164964 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m13:13:41.165494 [debug] [MainThread]: On master: COMMIT
[0m13:13:41.166097 [debug] [MainThread]: Using postgres connection "master"
[0m13:13:41.166564 [debug] [MainThread]: On master: COMMIT
[0m13:13:41.167338 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:13:41.167897 [debug] [MainThread]: On master: Close
[0m13:13:41.175214 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m13:13:41.176022 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:13:41.176717 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m13:13:41.177313 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:13:41.177993 [info ] [Thread-2 (]: 2 of 6 START sql table model analytics.fct_shipments ........................... [RUN]
[0m13:13:41.178723 [info ] [Thread-4 (]: 4 of 6 START sql table model analytics.shipments_by_hour_day ................... [RUN]
[0m13:13:41.179526 [info ] [Thread-1 (]: 1 of 6 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m13:13:41.181600 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m13:13:41.180355 [info ] [Thread-3 (]: 3 of 6 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m13:13:41.182843 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.shipments_by_hour_day'
[0m13:13:41.183654 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m13:13:41.184189 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m13:13:41.184814 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m13:13:41.185359 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:13:41.185891 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m13:13:41.192325 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:13:41.200223 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m13:13:41.205206 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:13:41.208443 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:13:41.213336 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:13:41.216615 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m13:13:41.217349 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:13:41.298112 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m13:13:41.298808 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:13:41.299572 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m13:13:41.305429 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:13:41.310503 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:13:41.314366 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:13:41.316960 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:13:41.317851 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: BEGIN
[0m13:13:41.318577 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:13:41.319203 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:13:41.320080 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m13:13:41.320674 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:13:41.321175 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:13:41.321932 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:13:41.323338 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m13:13:41.322425 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m13:13:41.324047 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:13:41.324813 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:13:41.466757 [debug] [Thread-4 (]: SQL status: BEGIN in 0.148 seconds
[0m13:13:41.467585 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:13:41.468247 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */

  
    

  create  table "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select
        s.shipment_id,
        s.shipment_date,
        extract(hour from s.shipment_date) as shipment_hour,
        extract(dow from s.shipment_date) as day_of_week,
        ca.delivered_at,
        s.expected_delivery_date
    from "delivery_analytics"."raw"."shipments" s
    left join "delivery_analytics"."raw"."courier_assignments" ca
        on s.shipment_id = ca.shipment_id
)

select
    day_of_week,
    shipment_hour,
    count(distinct shipment_id) as total_shipments,
    sum(case when delivered_at is not null and delivered_at::date <= expected_delivery_date::date then 1 else 0 end) as on_time_deliveries,
    sum(case when delivered_at::date > expected_delivery_date::date then 1 else 0 end) as late_deliveries
from fct
group by day_of_week, shipment_hour
order by day_of_week, shipment_hour
  );
  
[0m13:13:41.472668 [debug] [Thread-2 (]: SQL status: BEGIN in 0.151 seconds
[0m13:13:41.473270 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:13:41.473747 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."raw"."shipments"
),

assignments as (
    select
        shipment_id,
        courier_id,
        delivered_at
    from "delivery_analytics"."raw"."courier_assignments"
)

select
    s.shipment_id,
    a.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    a.delivered_at,
    s.price,

    case
        when a.delivered_at is null then null
        when a.delivered_at::date > s.expected_delivery_date::date then true
        else false
    end as late_delivery

from shipments s
left join assignments a
    on s.shipment_id = a.shipment_id
  );
  
[0m13:13:41.479865 [debug] [Thread-3 (]: SQL status: BEGIN in 0.155 seconds
[0m13:13:41.480506 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:13:41.480989 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    s.shipment_id,
    ca.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    ca.delivered_at,
    s.price,

    case
        when ca.delivered_at is null then null
        when ca.delivered_at::date > s.expected_delivery_date then true
        else false
    end as late_delivery

from "delivery_analytics"."raw"."shipments" s
left join "delivery_analytics"."raw"."courier_assignments" ca
    on s.shipment_id = ca.shipment_id
  );
  
[0m13:13:41.485260 [debug] [Thread-2 (]: SQL status: SELECT 6 in 0.011 seconds
[0m13:13:41.498000 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:13:41.498711 [debug] [Thread-3 (]: SQL status: SELECT 6 in 0.017 seconds
[0m13:13:41.499579 [debug] [Thread-4 (]: Postgres adapter: Postgres error: unit "hour" not supported for type date

[0m13:13:41.500412 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m13:13:41.501151 [debug] [Thread-1 (]: SQL status: BEGIN in 0.177 seconds
[0m13:13:41.505210 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:13:41.505815 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: ROLLBACK
[0m13:13:41.506610 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:13:41.507290 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:13:41.507911 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue" rename to "fct_shipments_revenue__dbt_backup"
[0m13:13:41.508649 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    select
    status_timestamp::date as shipment_date,
    status,
    count(*) as shipments_count
from "delivery_analytics"."analytics"."stg_shipment_status"
group by
    status_timestamp::date,
    status
order by
    shipment_date,
    status
  );
  
[0m13:13:41.509313 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: Close
[0m13:13:41.512578 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:13:41.513508 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m13:13:41.515413 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m13:13:41.519783 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:13:41.520448 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m13:13:41.521118 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp" rename to "fct_shipments_revenue"
[0m13:13:41.542556 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m13:13:41.543500 [debug] [Thread-1 (]: SQL status: SELECT 5 in 0.030 seconds
[0m13:13:41.544648 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:13:41.548488 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:13:41.549123 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m13:13:41.550013 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.006 seconds
[0m13:13:41.551068 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m13:13:41.553758 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m13:13:41.554412 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m13:13:41.555196 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:13:41.555821 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:13:41.568374 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m13:13:41.569168 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m13:13:41.573426 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:13:41.580625 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:13:41.581892 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m13:13:41.582650 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m13:13:41.583427 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m13:13:41.587181 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup"
[0m13:13:41.588397 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:13:41.588963 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m13:13:41.589665 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup" cascade
[0m13:13:41.590522 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.006 seconds
[0m13:13:41.593041 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m13:13:41.596842 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m13:13:41.597629 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:13:41.598663 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m13:13:41.599329 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.006 seconds
[0m13:13:41.603330 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m13:13:41.604025 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m13:13:41.607920 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m13:13:41.609640 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:13:41.610520 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m13:13:41.611843 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c40b60f-65b8-429a-94de-e27961958084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA61C0290>]}
[0m13:13:41.612348 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c40b60f-65b8-429a-94de-e27961958084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA5D836A0>]}
[0m13:13:41.613273 [info ] [Thread-2 (]: 2 of 6 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 6[0m in 0.42s]
[0m13:13:41.615446 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m13:13:41.614104 [info ] [Thread-3 (]: 3 of 6 OK created sql table model analytics.fct_shipments_revenue .............. [[32mSELECT 6[0m in 0.42s]
[0m13:13:41.619758 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:13:41.620432 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:13:41.621156 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m13:13:41.623414 [debug] [Thread-4 (]: Database Error in model shipments_by_hour_day (models\marts\analytics\shipments_by_hour_day.sql)
  unit "hour" not supported for type date
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\shipments_by_hour_day.sql
[0m13:13:41.624196 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.013 seconds
[0m13:13:41.622206 [info ] [Thread-3 (]: 6 of 6 START sql table model analytics.fct_shipments_sla ....................... [RUN]
[0m13:13:41.625971 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c40b60f-65b8-429a-94de-e27961958084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA5DEBC50>]}
[0m13:13:41.625251 [info ] [Thread-2 (]: 5 of 6 START sql table model analytics.courier_performance ..................... [RUN]
[0m13:13:41.628293 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m13:13:41.629091 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments_revenue, now model.dbt_delivery_analytics.fct_shipments_sla)
[0m13:13:41.630680 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.courier_performance)
[0m13:13:41.630075 [error] [Thread-4 (]: 4 of 6 ERROR creating sql table model analytics.shipments_by_hour_day .......... [[31mERROR[0m in 0.44s]
[0m13:13:41.631613 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:13:41.632378 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c40b60f-65b8-429a-94de-e27961958084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA61B7C50>]}
[0m13:13:41.632963 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m13:13:41.633918 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:13:41.638021 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:13:41.639133 [info ] [Thread-1 (]: 1 of 6 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 5[0m in 0.45s]
[0m13:13:41.645716 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m13:13:41.646726 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.shipments_by_hour_day' to be skipped because of status 'error'.  Reason: Database Error in model shipments_by_hour_day (models\marts\analytics\shipments_by_hour_day.sql)
  unit "hour" not supported for type date
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\shipments_by_hour_day.sql.
[0m13:13:41.648386 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m13:13:41.651378 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:13:41.655719 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:13:41.656802 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m13:13:41.662820 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m13:13:41.664138 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:13:41.664730 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: BEGIN
[0m13:13:41.665270 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:13:41.666365 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:13:41.666930 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m13:13:41.667382 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:13:41.741878 [debug] [Thread-3 (]: SQL status: BEGIN in 0.077 seconds
[0m13:13:41.742562 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:13:41.743090 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,

        case
            when delivered_at is null then null
            else (delivered_at::date - expected_delivery_date::date)
        end as delivery_delay_days,

        case
            when delivered_at is null then null
            when delivered_at::date > expected_delivery_date::date then true
            else false
        end as late_delivery

    from "delivery_analytics"."analytics"."fct_shipments"
)

select *
from sla
  );
  
[0m13:13:41.748312 [debug] [Thread-2 (]: SQL status: BEGIN in 0.081 seconds
[0m13:13:41.748917 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:13:41.749496 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id
  );
  
[0m13:13:41.753127 [debug] [Thread-3 (]: SQL status: SELECT 6 in 0.009 seconds
[0m13:13:41.757280 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:13:41.757863 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla" rename to "fct_shipments_sla__dbt_backup"
[0m13:13:41.758447 [debug] [Thread-2 (]: SQL status: SELECT 3 in 0.008 seconds
[0m13:13:41.762183 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:13:41.762663 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m13:13:41.763221 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance" rename to "courier_performance__dbt_backup"
[0m13:13:41.766601 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:13:41.767298 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp" rename to "fct_shipments_sla"
[0m13:13:41.768248 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:13:41.771761 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:13:41.772235 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance__dbt_tmp" rename to "courier_performance"
[0m13:13:41.772674 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:13:41.775164 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m13:13:41.775753 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m13:13:41.776293 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:13:41.778309 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m13:13:41.778890 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m13:13:41.779513 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:13:41.780137 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m13:13:41.781657 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m13:13:41.784604 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup"
[0m13:13:41.785163 [debug] [Thread-2 (]: SQL status: COMMIT in 0.005 seconds
[0m13:13:41.786224 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:13:41.788822 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."courier_performance__dbt_backup"
[0m13:13:41.789418 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup" cascade
[0m13:13:41.790434 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:13:41.791316 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
drop table if exists "delivery_analytics"."analytics"."courier_performance__dbt_backup" cascade
[0m13:13:41.797686 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.006 seconds
[0m13:13:41.798259 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.007 seconds
[0m13:13:41.800190 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m13:13:41.802515 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_sla: Close
[0m13:13:41.803925 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c40b60f-65b8-429a-94de-e27961958084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA631D390>]}
[0m13:13:41.804885 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c40b60f-65b8-429a-94de-e27961958084', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA631D320>]}
[0m13:13:41.805849 [info ] [Thread-2 (]: 5 of 6 OK created sql table model analytics.courier_performance ................ [[32mSELECT 3[0m in 0.17s]
[0m13:13:41.807032 [info ] [Thread-3 (]: 6 of 6 OK created sql table model analytics.fct_shipments_sla .................. [[32mSELECT 6[0m in 0.18s]
[0m13:13:41.808945 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m13:13:41.809891 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:13:41.812408 [debug] [MainThread]: Using postgres connection "master"
[0m13:13:41.812847 [debug] [MainThread]: On master: BEGIN
[0m13:13:41.813300 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:13:41.980673 [debug] [MainThread]: SQL status: BEGIN in 0.167 seconds
[0m13:13:41.981314 [debug] [MainThread]: On master: COMMIT
[0m13:13:41.981797 [debug] [MainThread]: Using postgres connection "master"
[0m13:13:41.982226 [debug] [MainThread]: On master: COMMIT
[0m13:13:41.982882 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:13:41.983301 [debug] [MainThread]: On master: Close
[0m13:13:41.983957 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:13:41.984348 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m13:13:41.984726 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m13:13:41.985149 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m13:13:41.985622 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipments_by_hour_day' was properly closed.
[0m13:13:41.986088 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m13:13:41.986536 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_sla' was properly closed.
[0m13:13:41.987126 [info ] [MainThread]: 
[0m13:13:41.987923 [info ] [MainThread]: Finished running 6 table models in 0 hours 0 minutes and 1.23 seconds (1.23s).
[0m13:13:41.990486 [debug] [MainThread]: Command end result
[0m13:13:42.026166 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:13:42.030579 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:13:42.039845 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m13:13:42.040521 [info ] [MainThread]: 
[0m13:13:42.041672 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m13:13:42.042574 [info ] [MainThread]: 
[0m13:13:42.043299 [error] [MainThread]: [31mFailure in model shipments_by_hour_day (models\marts\analytics\shipments_by_hour_day.sql)[0m
[0m13:13:42.044100 [error] [MainThread]:   Database Error in model shipments_by_hour_day (models\marts\analytics\shipments_by_hour_day.sql)
  unit "hour" not supported for type date
  compiled code at target\run\dbt_delivery_analytics\models\marts\analytics\shipments_by_hour_day.sql
[0m13:13:42.044752 [info ] [MainThread]: 
[0m13:13:42.045640 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\analytics\shipments_by_hour_day.sql
[0m13:13:42.046404 [info ] [MainThread]: 
[0m13:13:42.047351 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=6
[0m13:13:42.049456 [debug] [MainThread]: Command `dbt run` failed at 13:13:42.049220 after 3.30 seconds
[0m13:13:42.050283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA61CE090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA5BDEA50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CA1B415B0>]}
[0m13:13:42.050861 [debug] [MainThread]: Flushing usage events
[0m13:13:43.250087 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:16:51.496598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A11E014980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A11EABCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A11F107D90>]}


============================== 13:16:51.502439 | 14da193a-3b8a-4dbe-acd6-e91424771905 ==============================
[0m13:16:51.502439 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:16:51.503950 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run --select marts.analytics', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:16:51.826042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '14da193a-3b8a-4dbe-acd6-e91424771905', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A11E478770>]}
[0m13:16:51.933793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '14da193a-3b8a-4dbe-acd6-e91424771905', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A11EFAEE00>]}
[0m13:16:51.935833 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:16:52.400416 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:16:52.627456 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m13:16:52.628784 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\schema.yml
[0m13:16:52.629480 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\analytics\shipments_by_hour_day.sql
[0m13:16:53.001504 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'shipments_by_city_vehicle' in the 'models' section of file 'models\schema.yml'
[0m13:16:53.249667 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_delivery_analytics.not_null_shipments_by_city_vehicle_city.1d36860158' (models\schema.yml) depends on a node named 'shipments_by_city_vehicle' in package '' which was not found
[0m13:16:53.251371 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_delivery_analytics.not_null_shipments_by_city_vehicle_vehicle_type.51a805e811' (models\schema.yml) depends on a node named 'shipments_by_city_vehicle' in package '' which was not found
[0m13:16:53.371908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '14da193a-3b8a-4dbe-acd6-e91424771905', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A120AA8F50>]}
[0m13:16:53.505462 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:16:53.510900 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:16:53.548865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '14da193a-3b8a-4dbe-acd6-e91424771905', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A1209E3A70>]}
[0m13:16:53.549972 [info ] [MainThread]: Found 14 models, 7 data tests, 5 sources, 459 macros
[0m13:16:53.550825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '14da193a-3b8a-4dbe-acd6-e91424771905', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A1207BE510>]}
[0m13:16:53.554403 [info ] [MainThread]: 
[0m13:16:53.555360 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:16:53.556159 [info ] [MainThread]: 
[0m13:16:53.558695 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m13:16:53.567162 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m13:16:53.743194 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m13:16:53.743963 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m13:16:53.744544 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:16:53.861533 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.117 seconds
[0m13:16:53.863346 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m13:16:53.868207 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m13:16:53.877458 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m13:16:53.877976 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m13:16:53.878347 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:16:53.939333 [debug] [ThreadPool]: SQL status: BEGIN in 0.061 seconds
[0m13:16:53.940201 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m13:16:53.940837 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m13:16:53.953766 [debug] [ThreadPool]: SQL status: SELECT 13 in 0.012 seconds
[0m13:16:53.955599 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m13:16:53.956323 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m13:16:53.968135 [debug] [MainThread]: Using postgres connection "master"
[0m13:16:53.968769 [debug] [MainThread]: On master: BEGIN
[0m13:16:53.969243 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:16:54.031952 [debug] [MainThread]: SQL status: BEGIN in 0.063 seconds
[0m13:16:54.032652 [debug] [MainThread]: Using postgres connection "master"
[0m13:16:54.033205 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m13:16:54.044175 [debug] [MainThread]: SQL status: SELECT 5 in 0.010 seconds
[0m13:16:54.045854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '14da193a-3b8a-4dbe-acd6-e91424771905', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A120DCB6C0>]}
[0m13:16:54.046416 [debug] [MainThread]: On master: ROLLBACK
[0m13:16:54.047065 [debug] [MainThread]: Using postgres connection "master"
[0m13:16:54.047457 [debug] [MainThread]: On master: BEGIN
[0m13:16:54.048303 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m13:16:54.048801 [debug] [MainThread]: On master: COMMIT
[0m13:16:54.049272 [debug] [MainThread]: Using postgres connection "master"
[0m13:16:54.049695 [debug] [MainThread]: On master: COMMIT
[0m13:16:54.050684 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:16:54.051107 [debug] [MainThread]: On master: Close
[0m13:16:54.058918 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:16:54.059894 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m13:16:54.060609 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:16:54.061082 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m13:16:54.061766 [info ] [Thread-4 (]: 4 of 6 START sql table model analytics.shipments_by_hour_day ................... [RUN]
[0m13:16:54.062489 [info ] [Thread-1 (]: 1 of 6 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m13:16:54.063574 [info ] [Thread-3 (]: 3 of 6 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m13:16:54.065352 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.shipments_by_hour_day'
[0m13:16:54.066118 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m13:16:54.064475 [info ] [Thread-2 (]: 2 of 6 START sql table model analytics.fct_shipments ........................... [RUN]
[0m13:16:54.066905 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m13:16:54.067424 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:16:54.067955 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m13:16:54.068595 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m13:16:54.069406 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:16:54.080671 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:16:54.086257 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:16:54.086864 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m13:16:54.090175 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:16:54.095693 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m13:16:54.097514 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m13:16:54.134462 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:16:54.162399 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:16:54.163052 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:16:54.168284 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:16:54.173872 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:16:54.175271 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m13:16:54.176145 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:16:54.181924 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m13:16:54.182701 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m13:16:54.183573 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:16:54.184439 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:16:54.185261 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:16:54.185788 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m13:16:54.186809 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: BEGIN
[0m13:16:54.187609 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:16:54.188535 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:16:54.189121 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:16:54.191148 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m13:16:54.192426 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:16:54.274243 [debug] [Thread-3 (]: SQL status: BEGIN in 0.090 seconds
[0m13:16:54.275128 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:16:54.275814 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    s.shipment_id,
    ca.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    ca.delivered_at,
    s.price,

    case
        when ca.delivered_at is null then null
        when ca.delivered_at::date > s.expected_delivery_date then true
        else false
    end as late_delivery

from "delivery_analytics"."raw"."shipments" s
left join "delivery_analytics"."raw"."courier_assignments" ca
    on s.shipment_id = ca.shipment_id
  );
  
[0m13:16:54.288938 [debug] [Thread-3 (]: SQL status: SELECT 6 in 0.012 seconds
[0m13:16:54.289969 [debug] [Thread-1 (]: SQL status: BEGIN in 0.102 seconds
[0m13:16:54.306622 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:16:54.307603 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:16:54.308331 [debug] [Thread-2 (]: SQL status: BEGIN in 0.116 seconds
[0m13:16:54.308839 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue" rename to "fct_shipments_revenue__dbt_backup"
[0m13:16:54.309522 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    select
    status_timestamp::date as shipment_date,
    status,
    count(*) as shipments_count
from "delivery_analytics"."analytics"."stg_shipment_status"
group by
    status_timestamp::date,
    status
order by
    shipment_date,
    status
  );
  
[0m13:16:54.310116 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:16:54.311051 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."raw"."shipments"
),

assignments as (
    select
        shipment_id,
        courier_id,
        delivered_at
    from "delivery_analytics"."raw"."courier_assignments"
)

select
    s.shipment_id,
    a.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    a.delivered_at,
    s.price,

    case
        when a.delivered_at is null then null
        when a.delivered_at::date > s.expected_delivery_date::date then true
        else false
    end as late_delivery

from shipments s
left join assignments a
    on s.shipment_id = a.shipment_id
  );
  
[0m13:16:54.311752 [debug] [Thread-4 (]: SQL status: BEGIN in 0.123 seconds
[0m13:16:54.312347 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m13:16:54.313235 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:16:54.318636 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:16:54.319529 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */

  
    

  create  table "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select
        s.shipment_id,
        s.shipment_date::timestamp as shipment_timestamp,
        extract(hour from s.shipment_date::timestamp) as shipment_hour,
        extract(dow from s.shipment_date) as day_of_week,
        ca.delivered_at,
        s.expected_delivery_date
    from "delivery_analytics"."raw"."shipments" s
    left join "delivery_analytics"."raw"."courier_assignments" ca
        on s.shipment_id = ca.shipment_id
)

select
    day_of_week,
    shipment_hour,
    count(distinct shipment_id) as total_shipments,
    sum(case when delivered_at is not null and delivered_at::date <= expected_delivery_date::date then 1 else 0 end) as on_time_deliveries,
    sum(case when delivered_at::date > expected_delivery_date::date then 1 else 0 end) as late_deliveries
from fct
group by day_of_week, shipment_hour
order by day_of_week, shipment_hour
  );
  
[0m13:16:54.320411 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp" rename to "fct_shipments_revenue"
[0m13:16:54.321143 [debug] [Thread-1 (]: SQL status: SELECT 5 in 0.010 seconds
[0m13:16:54.326307 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:16:54.326892 [debug] [Thread-2 (]: SQL status: SELECT 6 in 0.014 seconds
[0m13:16:54.327509 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m13:16:54.328142 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.006 seconds
[0m13:16:54.332534 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:16:54.338157 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m13:16:54.354002 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m13:16:54.354744 [debug] [Thread-4 (]: SQL status: SELECT 5 in 0.033 seconds
[0m13:16:54.358408 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m13:16:54.361735 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:16:54.365287 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:16:54.365810 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m13:16:54.366266 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:16:54.366774 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m13:16:54.367290 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */
alter table "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_tmp" rename to "shipments_by_hour_day"
[0m13:16:54.370648 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:16:54.371155 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m13:16:54.372042 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m13:16:54.373186 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:16:54.373915 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:16:54.374549 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m13:16:54.375022 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m13:16:54.376701 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: COMMIT
[0m13:16:54.378418 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m13:16:54.380112 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m13:16:54.387796 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup"
[0m13:16:54.388355 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:16:54.388867 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:16:54.389442 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:16:54.396281 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:16:54.396910 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: COMMIT
[0m13:16:54.397418 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m13:16:54.397908 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m13:16:54.398456 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup" cascade
[0m13:16:54.400630 [debug] [Thread-4 (]: SQL status: COMMIT in 0.002 seconds
[0m13:16:54.404805 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_backup"
[0m13:16:54.405498 [debug] [Thread-1 (]: SQL status: COMMIT in 0.006 seconds
[0m13:16:54.406214 [debug] [Thread-2 (]: SQL status: COMMIT in 0.007 seconds
[0m13:16:54.407007 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.008 seconds
[0m13:16:54.408084 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:16:54.410737 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m13:16:54.413298 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m13:16:54.416262 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m13:16:54.416807 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */
drop table if exists "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_backup" cascade
[0m13:16:54.417701 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:16:54.418586 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:16:54.421816 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m13:16:54.419460 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m13:16:54.422896 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.004 seconds
[0m13:16:54.425637 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: Close
[0m13:16:54.427295 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '14da193a-3b8a-4dbe-acd6-e91424771905', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A1210BCD10>]}
[0m13:16:54.428099 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '14da193a-3b8a-4dbe-acd6-e91424771905', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A120FD2AF0>]}
[0m13:16:54.429862 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.007 seconds
[0m13:16:54.430559 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.007 seconds
[0m13:16:54.429124 [info ] [Thread-3 (]: 3 of 6 OK created sql table model analytics.fct_shipments_revenue .............. [[32mSELECT 6[0m in 0.35s]
[0m13:16:54.431665 [info ] [Thread-4 (]: 4 of 6 OK created sql table model analytics.shipments_by_hour_day .............. [[32mSELECT 5[0m in 0.36s]
[0m13:16:54.433962 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m13:16:54.435776 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m13:16:54.436589 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:16:54.437406 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:16:54.438573 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '14da193a-3b8a-4dbe-acd6-e91424771905', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A1210E1C70>]}
[0m13:16:54.439285 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '14da193a-3b8a-4dbe-acd6-e91424771905', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A121244170>]}
[0m13:16:54.440641 [info ] [Thread-2 (]: 2 of 6 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 6[0m in 0.37s]
[0m13:16:54.443131 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m13:16:54.441770 [info ] [Thread-1 (]: 1 of 6 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 5[0m in 0.37s]
[0m13:16:54.444553 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m13:16:54.445575 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m13:16:54.446295 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:16:54.447109 [info ] [Thread-3 (]: 5 of 6 START sql table model analytics.courier_performance ..................... [RUN]
[0m13:16:54.449111 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments_revenue, now model.dbt_delivery_analytics.courier_performance)
[0m13:16:54.448269 [info ] [Thread-4 (]: 6 of 6 START sql table model analytics.fct_shipments_sla ....................... [RUN]
[0m13:16:54.449999 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m13:16:54.450810 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.shipments_by_hour_day, now model.dbt_delivery_analytics.fct_shipments_sla)
[0m13:16:54.454198 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m13:16:54.454790 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:16:54.459368 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:16:54.462553 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:16:54.463183 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m13:16:54.467813 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:16:54.471881 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m13:16:54.475403 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:16:54.476298 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_sla: BEGIN
[0m13:16:54.477046 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:16:54.477529 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:16:54.478256 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m13:16:54.479301 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:16:54.561894 [debug] [Thread-4 (]: SQL status: BEGIN in 0.084 seconds
[0m13:16:54.562719 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:16:54.563442 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,

        case
            when delivered_at is null then null
            else (delivered_at::date - expected_delivery_date::date)
        end as delivery_delay_days,

        case
            when delivered_at is null then null
            when delivered_at::date > expected_delivery_date::date then true
            else false
        end as late_delivery

    from "delivery_analytics"."analytics"."fct_shipments"
)

select *
from sla
  );
  
[0m13:16:54.566999 [debug] [Thread-3 (]: SQL status: BEGIN in 0.088 seconds
[0m13:16:54.567754 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:16:54.568517 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id
  );
  
[0m13:16:54.573765 [debug] [Thread-4 (]: SQL status: SELECT 6 in 0.010 seconds
[0m13:16:54.580160 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:16:54.580787 [debug] [Thread-3 (]: SQL status: SELECT 3 in 0.012 seconds
[0m13:16:54.581392 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla" rename to "fct_shipments_sla__dbt_backup"
[0m13:16:54.584726 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:16:54.585725 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance" rename to "courier_performance__dbt_backup"
[0m13:16:54.586399 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:16:54.590323 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:16:54.591016 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m13:16:54.591491 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp" rename to "fct_shipments_sla"
[0m13:16:54.595302 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:16:54.596858 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance__dbt_tmp" rename to "courier_performance"
[0m13:16:54.597582 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:16:54.599589 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m13:16:54.600241 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m13:16:54.600904 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:16:54.603761 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m13:16:54.604545 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m13:16:54.605310 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:16:54.606137 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m13:16:54.607621 [debug] [Thread-4 (]: SQL status: COMMIT in 0.002 seconds
[0m13:16:54.610606 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup"
[0m13:16:54.611109 [debug] [Thread-3 (]: SQL status: COMMIT in 0.004 seconds
[0m13:16:54.611957 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:16:54.619444 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."courier_performance__dbt_backup"
[0m13:16:54.620436 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup" cascade
[0m13:16:54.621520 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:16:54.622392 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
drop table if exists "delivery_analytics"."analytics"."courier_performance__dbt_backup" cascade
[0m13:16:54.628938 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.007 seconds
[0m13:16:54.629530 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.006 seconds
[0m13:16:54.631261 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipments_sla: Close
[0m13:16:54.632767 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m13:16:54.633726 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '14da193a-3b8a-4dbe-acd6-e91424771905', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A1212FA510>]}
[0m13:16:54.634711 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '14da193a-3b8a-4dbe-acd6-e91424771905', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A1212FA190>]}
[0m13:16:54.637233 [info ] [Thread-4 (]: 6 of 6 OK created sql table model analytics.fct_shipments_sla .................. [[32mSELECT 6[0m in 0.18s]
[0m13:16:54.639393 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:16:54.638222 [info ] [Thread-3 (]: 5 of 6 OK created sql table model analytics.courier_performance ................ [[32mSELECT 3[0m in 0.19s]
[0m13:16:54.640927 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m13:16:54.643331 [debug] [MainThread]: Using postgres connection "master"
[0m13:16:54.643937 [debug] [MainThread]: On master: BEGIN
[0m13:16:54.644509 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:16:54.711702 [debug] [MainThread]: SQL status: BEGIN in 0.067 seconds
[0m13:16:54.712488 [debug] [MainThread]: On master: COMMIT
[0m13:16:54.713041 [debug] [MainThread]: Using postgres connection "master"
[0m13:16:54.713489 [debug] [MainThread]: On master: COMMIT
[0m13:16:54.714178 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:16:54.714776 [debug] [MainThread]: On master: Close
[0m13:16:54.715642 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:16:54.716159 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m13:16:54.716650 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m13:16:54.717145 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_sla' was properly closed.
[0m13:16:54.717576 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.daily_shipments_status' was properly closed.
[0m13:16:54.717959 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m13:16:54.718324 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments' was properly closed.
[0m13:16:54.719043 [info ] [MainThread]: 
[0m13:16:54.719765 [info ] [MainThread]: Finished running 6 table models in 0 hours 0 minutes and 1.16 seconds (1.16s).
[0m13:16:54.722720 [debug] [MainThread]: Command end result
[0m13:16:54.767082 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:16:54.772251 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:16:54.787306 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m13:16:54.788269 [info ] [MainThread]: 
[0m13:16:54.789074 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:16:54.789764 [info ] [MainThread]: 
[0m13:16:54.790488 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m13:16:54.792120 [debug] [MainThread]: Command `dbt run` succeeded at 13:16:54.791873 after 3.51 seconds
[0m13:16:54.792625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A11EFA9C70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A120756390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A1212279B0>]}
[0m13:16:54.793105 [debug] [MainThread]: Flushing usage events
[0m13:16:56.049471 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:17:33.805308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DBED8980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DC96CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DCFBBD90>]}


============================== 13:17:33.812436 | a769962c-7990-4b47-9acd-c64a8ef8a5c9 ==============================
[0m13:17:33.812436 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:17:33.814426 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select marts.analytics', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:17:34.096029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a769962c-7990-4b47-9acd-c64a8ef8a5c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DC338770>]}
[0m13:17:34.192851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a769962c-7990-4b47-9acd-c64a8ef8a5c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DCE5EE00>]}
[0m13:17:34.194529 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:17:34.675771 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:17:34.910013 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m13:17:34.911021 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\analytics\shipments_by_city_vehicle.sql
[0m13:17:35.328136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a769962c-7990-4b47-9acd-c64a8ef8a5c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DE90DD50>]}
[0m13:17:35.557003 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:17:35.563549 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:17:35.625693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a769962c-7990-4b47-9acd-c64a8ef8a5c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DCE4FD40>]}
[0m13:17:35.626918 [info ] [MainThread]: Found 15 models, 7 data tests, 5 sources, 459 macros
[0m13:17:35.628087 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a769962c-7990-4b47-9acd-c64a8ef8a5c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114D8E9F150>]}
[0m13:17:35.631928 [info ] [MainThread]: 
[0m13:17:35.632764 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:17:35.633525 [info ] [MainThread]: 
[0m13:17:35.634665 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m13:17:35.643036 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m13:17:35.838738 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m13:17:35.839305 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m13:17:35.839716 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:17:35.980815 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.141 seconds
[0m13:17:35.982789 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m13:17:35.987024 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m13:17:35.995581 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m13:17:35.996094 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m13:17:35.996492 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:17:36.073355 [debug] [ThreadPool]: SQL status: BEGIN in 0.077 seconds
[0m13:17:36.074219 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m13:17:36.074884 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m13:17:36.087725 [debug] [ThreadPool]: SQL status: SELECT 14 in 0.012 seconds
[0m13:17:36.089680 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m13:17:36.090372 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m13:17:36.102537 [debug] [MainThread]: Using postgres connection "master"
[0m13:17:36.103133 [debug] [MainThread]: On master: BEGIN
[0m13:17:36.103751 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:17:36.211469 [debug] [MainThread]: SQL status: BEGIN in 0.108 seconds
[0m13:17:36.212154 [debug] [MainThread]: Using postgres connection "master"
[0m13:17:36.212768 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m13:17:36.224910 [debug] [MainThread]: SQL status: SELECT 5 in 0.011 seconds
[0m13:17:36.227289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a769962c-7990-4b47-9acd-c64a8ef8a5c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DED25160>]}
[0m13:17:36.227870 [debug] [MainThread]: On master: ROLLBACK
[0m13:17:36.228564 [debug] [MainThread]: Using postgres connection "master"
[0m13:17:36.229126 [debug] [MainThread]: On master: BEGIN
[0m13:17:36.230260 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m13:17:36.230835 [debug] [MainThread]: On master: COMMIT
[0m13:17:36.231850 [debug] [MainThread]: Using postgres connection "master"
[0m13:17:36.232398 [debug] [MainThread]: On master: COMMIT
[0m13:17:36.233182 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:17:36.233589 [debug] [MainThread]: On master: Close
[0m13:17:36.239626 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:17:36.240551 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m13:17:36.241221 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m13:17:36.241904 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m13:17:36.242852 [info ] [Thread-3 (]: 3 of 7 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m13:17:36.243632 [info ] [Thread-4 (]: 4 of 7 START sql table model analytics.shipments_by_city_vehicle ............... [RUN]
[0m13:17:36.244571 [info ] [Thread-1 (]: 1 of 7 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m13:17:36.249738 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m13:17:36.246807 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m13:17:36.245542 [info ] [Thread-2 (]: 2 of 7 START sql table model analytics.fct_shipments ........................... [RUN]
[0m13:17:36.248324 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.shipments_by_city_vehicle'
[0m13:17:36.250754 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m13:17:36.251547 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:17:36.252427 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m13:17:36.253249 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m13:17:36.266744 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:17:36.271799 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:17:36.272585 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m13:17:36.277540 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m13:17:36.284148 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m13:17:36.286478 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:17:36.287260 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m13:17:36.366227 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:17:36.368715 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:17:36.369274 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m13:17:36.374445 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m13:17:36.374973 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m13:17:36.380472 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m13:17:36.382042 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:17:36.382704 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m13:17:36.383432 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:17:36.384166 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m13:17:36.385034 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: BEGIN
[0m13:17:36.385917 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m13:17:36.386621 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:17:36.387210 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:17:36.388115 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:17:36.388893 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:17:36.389655 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m13:17:36.391275 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:17:36.474207 [debug] [Thread-3 (]: SQL status: BEGIN in 0.087 seconds
[0m13:17:36.474937 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:17:36.475688 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    s.shipment_id,
    ca.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    ca.delivered_at,
    s.price,

    case
        when ca.delivered_at is null then null
        when ca.delivered_at::date > s.expected_delivery_date then true
        else false
    end as late_delivery

from "delivery_analytics"."raw"."shipments" s
left join "delivery_analytics"."raw"."courier_assignments" ca
    on s.shipment_id = ca.shipment_id
  );
  
[0m13:17:36.481513 [debug] [Thread-4 (]: SQL status: BEGIN in 0.093 seconds
[0m13:17:36.482485 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m13:17:36.483143 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_city_vehicle"} */

  
    

  create  table "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select
        s.shipment_id,
        s.shipment_date,
        ca.courier_id,
        c.city,
        c.vehicle_type,
        s.expected_delivery_date,
        ca.delivered_at
    from "delivery_analytics"."raw"."shipments" s
    left join "delivery_analytics"."raw"."courier_assignments" ca
        on s.shipment_id = ca.shipment_id
    left join "delivery_analytics"."raw"."couriers" c
        on ca.courier_id = c.courier_id
)

select
    city,
    vehicle_type,
    count(distinct shipment_id) as total_shipments,
    sum(case when delivered_at is not null and delivered_at::date <= expected_delivery_date::date then 1 else 0 end) as on_time_deliveries,
    sum(case when delivered_at::date > expected_delivery_date::date then 1 else 0 end) as late_deliveries
from fct
group by city, vehicle_type
order by city, vehicle_type
  );
  
[0m13:17:36.489224 [debug] [Thread-3 (]: SQL status: SELECT 6 in 0.013 seconds
[0m13:17:36.490072 [debug] [Thread-1 (]: SQL status: BEGIN in 0.101 seconds
[0m13:17:36.504108 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:17:36.504698 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:17:36.505191 [debug] [Thread-4 (]: SQL status: SELECT 3 in 0.022 seconds
[0m13:17:36.505732 [debug] [Thread-2 (]: SQL status: BEGIN in 0.114 seconds
[0m13:17:36.506201 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue" rename to "fct_shipments_revenue__dbt_backup"
[0m13:17:36.506768 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    select
    status_timestamp::date as shipment_date,
    status,
    count(*) as shipments_count
from "delivery_analytics"."analytics"."stg_shipment_status"
group by
    status_timestamp::date,
    status
order by
    shipment_date,
    status
  );
  
[0m13:17:36.510703 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m13:17:36.511246 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:17:36.512188 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_city_vehicle"} */
alter table "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_tmp" rename to "shipments_by_city_vehicle"
[0m13:17:36.513253 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:17:36.514213 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."raw"."shipments"
),

assignments as (
    select
        shipment_id,
        courier_id,
        delivered_at
    from "delivery_analytics"."raw"."courier_assignments"
)

select
    s.shipment_id,
    a.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    a.delivered_at,
    s.price,

    case
        when a.delivered_at is null then null
        when a.delivered_at::date > s.expected_delivery_date::date then true
        else false
    end as late_delivery

from shipments s
left join assignments a
    on s.shipment_id = a.shipment_id
  );
  
[0m13:17:36.518634 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:17:36.519279 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m13:17:36.520007 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp" rename to "fct_shipments_revenue"
[0m13:17:36.541160 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: COMMIT
[0m13:17:36.541757 [debug] [Thread-1 (]: SQL status: SELECT 5 in 0.030 seconds
[0m13:17:36.542295 [debug] [Thread-2 (]: SQL status: SELECT 6 in 0.022 seconds
[0m13:17:36.542951 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m13:17:36.548452 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:17:36.549144 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.006 seconds
[0m13:17:36.552627 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:17:36.553230 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: COMMIT
[0m13:17:36.553785 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m13:17:36.558145 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m13:17:36.559112 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m13:17:36.560683 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:17:36.561779 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:17:36.562531 [debug] [Thread-4 (]: SQL status: COMMIT in 0.002 seconds
[0m13:17:36.563246 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m13:17:36.564031 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:17:36.572303 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:17:36.580120 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_backup"
[0m13:17:36.584821 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:17:36.585406 [debug] [Thread-3 (]: SQL status: COMMIT in 0.004 seconds
[0m13:17:36.585885 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m13:17:36.593216 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m13:17:36.593719 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m13:17:36.596708 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup"
[0m13:17:36.597703 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_city_vehicle"} */
drop table if exists "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_backup" cascade
[0m13:17:36.599460 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m13:17:36.600047 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:17:36.600620 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m13:17:36.601210 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m13:17:36.601689 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup" cascade
[0m13:17:36.603653 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m13:17:36.605429 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m13:17:36.609980 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: Close
[0m13:17:36.611125 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:17:36.611947 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:17:36.612789 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m13:17:36.615999 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m13:17:36.617530 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a769962c-7990-4b47-9acd-c64a8ef8a5c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DED80290>]}
[0m13:17:36.618283 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.008 seconds
[0m13:17:36.619061 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m13:17:36.619747 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m13:17:36.623931 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m13:17:36.621253 [info ] [Thread-4 (]: 4 of 7 OK created sql table model analytics.shipments_by_city_vehicle .......... [[32mSELECT 3[0m in 0.37s]
[0m13:17:36.628158 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m13:17:36.632461 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m13:17:36.633514 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m13:17:36.634374 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a769962c-7990-4b47-9acd-c64a8ef8a5c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DEDFD7B0>]}
[0m13:17:36.635721 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:17:36.637012 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m13:17:36.638000 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:17:36.640149 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m13:17:36.639385 [info ] [Thread-3 (]: 3 of 7 OK created sql table model analytics.fct_shipments_revenue .............. [[32mSELECT 6[0m in 0.39s]
[0m13:17:36.641153 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m13:17:36.642308 [info ] [Thread-4 (]: 5 of 7 START sql table model analytics.shipments_by_hour_day ................... [RUN]
[0m13:17:36.643753 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m13:17:36.644884 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.shipments_by_city_vehicle, now model.dbt_delivery_analytics.shipments_by_hour_day)
[0m13:17:36.645830 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:17:36.651079 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:17:36.651847 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.007 seconds
[0m13:17:36.652555 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.010 seconds
[0m13:17:36.655311 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m13:17:36.657115 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m13:17:36.658334 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a769962c-7990-4b47-9acd-c64a8ef8a5c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DEEB8690>]}
[0m13:17:36.659103 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:17:36.660099 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a769962c-7990-4b47-9acd-c64a8ef8a5c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DEE32600>]}
[0m13:17:36.667696 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:17:36.661304 [info ] [Thread-1 (]: 1 of 7 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 5[0m in 0.41s]
[0m13:17:36.669175 [info ] [Thread-2 (]: 2 of 7 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 6[0m in 0.41s]
[0m13:17:36.670504 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m13:17:36.671684 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m13:17:36.673219 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:17:36.674114 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:17:36.674764 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m13:17:36.676365 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: BEGIN
[0m13:17:36.675705 [info ] [Thread-1 (]: 7 of 7 START sql table model analytics.fct_shipments_sla ....................... [RUN]
[0m13:17:36.678007 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:17:36.677439 [info ] [Thread-3 (]: 6 of 7 START sql table model analytics.courier_performance ..................... [RUN]
[0m13:17:36.678944 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.daily_shipments_status, now model.dbt_delivery_analytics.fct_shipments_sla)
[0m13:17:36.680079 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments_revenue, now model.dbt_delivery_analytics.courier_performance)
[0m13:17:36.680955 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:17:36.681793 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m13:17:36.687487 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:17:36.692133 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m13:17:36.695405 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m13:17:36.696517 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:17:36.703737 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m13:17:36.710206 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:17:36.713716 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:17:36.714854 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:17:36.715834 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m13:17:36.716704 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: BEGIN
[0m13:17:36.717581 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:17:36.718342 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:17:36.764696 [debug] [Thread-4 (]: SQL status: BEGIN in 0.087 seconds
[0m13:17:36.765646 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:17:36.766347 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */

  
    

  create  table "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select
        s.shipment_id,
        s.shipment_date::timestamp as shipment_timestamp,
        extract(hour from s.shipment_date::timestamp) as shipment_hour,
        extract(dow from s.shipment_date) as day_of_week,
        ca.delivered_at,
        s.expected_delivery_date
    from "delivery_analytics"."raw"."shipments" s
    left join "delivery_analytics"."raw"."courier_assignments" ca
        on s.shipment_id = ca.shipment_id
)

select
    day_of_week,
    shipment_hour,
    count(distinct shipment_id) as total_shipments,
    sum(case when delivered_at is not null and delivered_at::date <= expected_delivery_date::date then 1 else 0 end) as on_time_deliveries,
    sum(case when delivered_at::date > expected_delivery_date::date then 1 else 0 end) as late_deliveries
from fct
group by day_of_week, shipment_hour
order by day_of_week, shipment_hour
  );
  
[0m13:17:36.786193 [debug] [Thread-4 (]: SQL status: SELECT 5 in 0.019 seconds
[0m13:17:36.792731 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:17:36.793596 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */
alter table "delivery_analytics"."analytics"."shipments_by_hour_day" rename to "shipments_by_hour_day__dbt_backup"
[0m13:17:36.794910 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:17:36.795648 [debug] [Thread-1 (]: SQL status: BEGIN in 0.077 seconds
[0m13:17:36.801017 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:17:36.801653 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:17:36.802204 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */
alter table "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_tmp" rename to "shipments_by_hour_day"
[0m13:17:36.802862 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,

        case
            when delivered_at is null then null
            else (delivered_at::date - expected_delivery_date::date)
        end as delivery_delay_days,

        case
            when delivered_at is null then null
            when delivered_at::date > expected_delivery_date::date then true
            else false
        end as late_delivery

    from "delivery_analytics"."analytics"."fct_shipments"
)

select *
from sla
  );
  
[0m13:17:36.804629 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:17:36.807270 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: COMMIT
[0m13:17:36.807772 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:17:36.808210 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: COMMIT
[0m13:17:36.809694 [debug] [Thread-3 (]: SQL status: BEGIN in 0.092 seconds
[0m13:17:36.810203 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m13:17:36.810665 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:17:36.814933 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_backup"
[0m13:17:36.815636 [debug] [Thread-1 (]: SQL status: SELECT 6 in 0.012 seconds
[0m13:17:36.816182 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id
  );
  
[0m13:17:36.817179 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m13:17:36.821096 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:17:36.821827 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */
drop table if exists "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_backup" cascade
[0m13:17:36.822567 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla" rename to "fct_shipments_sla__dbt_backup"
[0m13:17:36.824123 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m13:17:36.827223 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:17:36.827794 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp" rename to "fct_shipments_sla"
[0m13:17:36.829112 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:17:36.831832 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m13:17:36.832497 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:17:36.833085 [debug] [Thread-3 (]: SQL status: SELECT 3 in 0.011 seconds
[0m13:17:36.833706 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m13:17:36.838067 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:17:36.838620 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.015 seconds
[0m13:17:36.839219 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance" rename to "courier_performance__dbt_backup"
[0m13:17:36.841067 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: Close
[0m13:17:36.841599 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m13:17:36.844833 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup"
[0m13:17:36.845513 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m13:17:36.846453 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a769962c-7990-4b47-9acd-c64a8ef8a5c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DE433620>]}
[0m13:17:36.847656 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m13:17:36.852359 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:17:36.854031 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup" cascade
[0m13:17:36.853355 [info ] [Thread-4 (]: 5 of 7 OK created sql table model analytics.shipments_by_hour_day .............. [[32mSELECT 5[0m in 0.20s]
[0m13:17:36.854982 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance__dbt_tmp" rename to "courier_performance"
[0m13:17:36.856355 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.shipments_by_hour_day
[0m13:17:36.858129 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:17:36.860331 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m13:17:36.860879 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.005 seconds
[0m13:17:36.861351 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:17:36.863001 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: Close
[0m13:17:36.863908 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m13:17:36.865295 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a769962c-7990-4b47-9acd-c64a8ef8a5c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DED9E930>]}
[0m13:17:36.867141 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m13:17:36.866478 [info ] [Thread-1 (]: 7 of 7 OK created sql table model analytics.fct_shipments_sla .................. [[32mSELECT 6[0m in 0.19s]
[0m13:17:36.872432 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."courier_performance__dbt_backup"
[0m13:17:36.873811 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_sla
[0m13:17:36.874949 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m13:17:36.875834 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
drop table if exists "delivery_analytics"."analytics"."courier_performance__dbt_backup" cascade
[0m13:17:36.881965 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.005 seconds
[0m13:17:36.884145 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m13:17:36.885239 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a769962c-7990-4b47-9acd-c64a8ef8a5c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DED9ECF0>]}
[0m13:17:36.886505 [info ] [Thread-3 (]: 6 of 7 OK created sql table model analytics.courier_performance ................ [[32mSELECT 3[0m in 0.21s]
[0m13:17:36.887898 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m13:17:36.890483 [debug] [MainThread]: Using postgres connection "master"
[0m13:17:36.891047 [debug] [MainThread]: On master: BEGIN
[0m13:17:36.891521 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:17:36.958732 [debug] [MainThread]: SQL status: BEGIN in 0.067 seconds
[0m13:17:36.959399 [debug] [MainThread]: On master: COMMIT
[0m13:17:36.959886 [debug] [MainThread]: Using postgres connection "master"
[0m13:17:36.960259 [debug] [MainThread]: On master: COMMIT
[0m13:17:36.960859 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:17:36.961254 [debug] [MainThread]: On master: Close
[0m13:17:36.962004 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:17:36.962595 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m13:17:36.963202 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m13:17:36.963675 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m13:17:36.964409 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipments_by_hour_day' was properly closed.
[0m13:17:36.965012 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_sla' was properly closed.
[0m13:17:36.965505 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments' was properly closed.
[0m13:17:36.966208 [info ] [MainThread]: 
[0m13:17:36.967273 [info ] [MainThread]: Finished running 7 table models in 0 hours 0 minutes and 1.33 seconds (1.33s).
[0m13:17:36.970181 [debug] [MainThread]: Command end result
[0m13:17:37.018156 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:17:37.022348 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:17:37.033833 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m13:17:37.034607 [info ] [MainThread]: 
[0m13:17:37.035553 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:17:37.036423 [info ] [MainThread]: 
[0m13:17:37.037122 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=7
[0m13:17:37.038584 [debug] [MainThread]: Command `dbt run` succeeded at 13:17:37.038412 after 3.46 seconds
[0m13:17:37.039085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DC965430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DEFDFDD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000114DEFDF890>]}
[0m13:17:37.039551 [debug] [MainThread]: Flushing usage events
[0m13:17:38.359968 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:53:51.101576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207034A0980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020703F4CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002070459BD90>]}


============================== 12:53:51.142454 | 3d853480-882e-4885-9bd2-c1c00764c882 ==============================
[0m12:53:51.142454 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:53:51.143712 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select marts.analytics', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:53:51.530387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3d853480-882e-4885-9bd2-c1c00764c882', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020703908770>]}
[0m12:53:51.615149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3d853480-882e-4885-9bd2-c1c00764c882', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002070443EE00>]}
[0m12:53:51.619150 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:53:52.214022 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:53:52.506234 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:53:52.506964 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:53:52.565965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3d853480-882e-4885-9bd2-c1c00764c882', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020705F29750>]}
[0m12:53:52.712850 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:53:52.718271 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:53:52.779457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3d853480-882e-4885-9bd2-c1c00764c882', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002070442FD40>]}
[0m12:53:52.780124 [info ] [MainThread]: Found 15 models, 7 data tests, 5 sources, 459 macros
[0m12:53:52.780832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d853480-882e-4885-9bd2-c1c00764c882', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002070459D9B0>]}
[0m12:53:52.783930 [info ] [MainThread]: 
[0m12:53:52.784496 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:53:52.785018 [info ] [MainThread]: 
[0m12:53:52.785724 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:53:52.793253 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m12:53:53.115081 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m12:53:53.115612 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m12:53:53.116008 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:53:53.300960 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.185 seconds
[0m12:53:53.302685 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m12:53:53.306224 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m12:53:53.313703 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:53:53.314515 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m12:53:53.315079 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:53:53.375474 [debug] [ThreadPool]: SQL status: BEGIN in 0.060 seconds
[0m12:53:53.376022 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:53:53.376519 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m12:53:53.414394 [debug] [ThreadPool]: SQL status: SELECT 15 in 0.037 seconds
[0m12:53:53.416429 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m12:53:53.417201 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m12:53:53.427276 [debug] [MainThread]: Using postgres connection "master"
[0m12:53:53.427758 [debug] [MainThread]: On master: BEGIN
[0m12:53:53.428145 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:53:53.486902 [debug] [MainThread]: SQL status: BEGIN in 0.059 seconds
[0m12:53:53.487451 [debug] [MainThread]: Using postgres connection "master"
[0m12:53:53.487989 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m12:53:53.514918 [debug] [MainThread]: SQL status: SELECT 5 in 0.026 seconds
[0m12:53:53.516882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d853480-882e-4885-9bd2-c1c00764c882', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002070627DF30>]}
[0m12:53:53.517485 [debug] [MainThread]: On master: ROLLBACK
[0m12:53:53.518198 [debug] [MainThread]: Using postgres connection "master"
[0m12:53:53.518606 [debug] [MainThread]: On master: BEGIN
[0m12:53:53.519582 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m12:53:53.519989 [debug] [MainThread]: On master: COMMIT
[0m12:53:53.520386 [debug] [MainThread]: Using postgres connection "master"
[0m12:53:53.520764 [debug] [MainThread]: On master: COMMIT
[0m12:53:53.521393 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:53:53.521912 [debug] [MainThread]: On master: Close
[0m12:53:53.529828 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m12:53:53.530524 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m12:53:53.531138 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m12:53:53.531789 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m12:53:53.532521 [info ] [Thread-2 (]: 2 of 7 START sql table model analytics.fct_shipments ........................... [RUN]
[0m12:53:53.533214 [info ] [Thread-1 (]: 1 of 7 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m12:53:53.534164 [info ] [Thread-4 (]: 4 of 7 START sql table model analytics.shipments_by_city_vehicle ............... [RUN]
[0m12:53:53.536238 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m12:53:53.535117 [info ] [Thread-3 (]: 3 of 7 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m12:53:53.537197 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m12:53:53.537938 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.shipments_by_city_vehicle'
[0m12:53:53.538539 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m12:53:53.539247 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m12:53:53.539852 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m12:53:53.540420 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m12:53:53.551372 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m12:53:53.552127 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m12:53:53.557510 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:53:53.562790 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:53:53.571942 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:53:53.573561 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m12:53:53.576087 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m12:53:53.746638 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m12:53:53.752351 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:53:53.753527 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m12:53:53.759665 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:53:53.760300 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m12:53:53.765913 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:53:53.766640 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:53:53.767462 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m12:53:53.768104 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:53:53.768836 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:53:53.769737 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m12:53:53.770707 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:53:53.771346 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:53:53.772145 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m12:53:53.773002 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:53:53.773968 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m12:53:53.774757 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: BEGIN
[0m12:53:53.775738 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:53:53.849553 [debug] [Thread-2 (]: SQL status: BEGIN in 0.081 seconds
[0m12:53:53.850174 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:53:53.850667 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."raw"."shipments"
),

assignments as (
    select
        shipment_id,
        courier_id,
        delivered_at
    from "delivery_analytics"."raw"."courier_assignments"
)

select
    s.shipment_id,
    a.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    a.delivered_at,
    s.price,

    case
        when a.delivered_at is null then null
        when a.delivered_at::date > s.expected_delivery_date::date then true
        else false
    end as late_delivery

from shipments s
left join assignments a
    on s.shipment_id = a.shipment_id
  );
  
[0m12:53:53.854408 [debug] [Thread-1 (]: SQL status: BEGIN in 0.083 seconds
[0m12:53:53.855030 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:53:53.855486 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    select
    status_timestamp::date as shipment_date,
    status,
    count(*) as shipments_count
from "delivery_analytics"."analytics"."stg_shipment_status"
group by
    status_timestamp::date,
    status
order by
    shipment_date,
    status
  );
  
[0m12:53:53.866521 [debug] [Thread-3 (]: SQL status: BEGIN in 0.092 seconds
[0m12:53:53.867303 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:53:53.867827 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    s.shipment_id,
    ca.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    ca.delivered_at,
    s.price,

    case
        when ca.delivered_at is null then null
        when ca.delivered_at::date > s.expected_delivery_date then true
        else false
    end as late_delivery

from "delivery_analytics"."raw"."shipments" s
left join "delivery_analytics"."raw"."courier_assignments" ca
    on s.shipment_id = ca.shipment_id
  );
  
[0m12:53:53.870744 [debug] [Thread-4 (]: SQL status: BEGIN in 0.095 seconds
[0m12:53:53.871288 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:53:53.871776 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_city_vehicle"} */

  
    

  create  table "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select
        s.shipment_id,
        s.shipment_date,
        ca.courier_id,
        c.city,
        c.vehicle_type,
        s.expected_delivery_date,
        ca.delivered_at
    from "delivery_analytics"."raw"."shipments" s
    left join "delivery_analytics"."raw"."courier_assignments" ca
        on s.shipment_id = ca.shipment_id
    left join "delivery_analytics"."raw"."couriers" c
        on ca.courier_id = c.courier_id
)

select
    city,
    vehicle_type,
    count(distinct shipment_id) as total_shipments,
    sum(case when delivered_at is not null and delivered_at::date <= expected_delivery_date::date then 1 else 0 end) as on_time_deliveries,
    sum(case when delivered_at::date > expected_delivery_date::date then 1 else 0 end) as late_deliveries
from fct
group by city, vehicle_type
order by city, vehicle_type
  );
  
[0m12:53:53.891818 [debug] [Thread-1 (]: SQL status: SELECT 9 in 0.036 seconds
[0m12:53:53.892481 [debug] [Thread-2 (]: SQL status: SELECT 31 in 0.041 seconds
[0m12:53:53.893107 [debug] [Thread-4 (]: SQL status: SELECT 3 in 0.021 seconds
[0m12:53:53.893684 [debug] [Thread-3 (]: SQL status: SELECT 31 in 0.025 seconds
[0m12:53:53.906336 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:53:53.911478 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:53:53.915587 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:53:53.919861 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:53:53.920552 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m12:53:53.921781 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m12:53:53.922632 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_city_vehicle"} */
alter table "delivery_analytics"."analytics"."shipments_by_city_vehicle" rename to "shipments_by_city_vehicle__dbt_backup"
[0m12:53:53.923347 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue" rename to "fct_shipments_revenue__dbt_backup"
[0m12:53:53.926018 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:53:53.926664 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:53:53.927313 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m12:53:53.927898 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m12:53:53.930989 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:53:53.934649 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:53:53.938935 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:53:53.942673 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:53:53.943373 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp" rename to "fct_shipments_revenue"
[0m12:53:53.944414 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_city_vehicle"} */
alter table "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_tmp" rename to "shipments_by_city_vehicle"
[0m12:53:53.945445 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m12:53:53.946461 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m12:53:53.949164 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:53:53.949780 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:53:53.950321 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m12:53:53.950886 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m12:53:53.971873 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m12:53:53.973925 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m12:53:53.976403 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: COMMIT
[0m12:53:53.978851 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m12:53:53.979497 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:53:53.980196 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:53:53.981204 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:53:53.982313 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:53:53.982913 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m12:53:53.983553 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m12:53:53.984202 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: COMMIT
[0m12:53:53.984883 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m12:53:53.988604 [debug] [Thread-3 (]: SQL status: COMMIT in 0.003 seconds
[0m12:53:53.997186 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup"
[0m12:53:53.997759 [debug] [Thread-4 (]: SQL status: COMMIT in 0.012 seconds
[0m12:53:54.004582 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:53:54.005152 [debug] [Thread-1 (]: SQL status: COMMIT in 0.019 seconds
[0m12:53:54.005981 [debug] [Thread-2 (]: SQL status: COMMIT in 0.020 seconds
[0m12:53:54.008755 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_backup"
[0m12:53:54.009468 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup" cascade
[0m12:53:54.013424 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m12:53:54.017066 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m12:53:54.018083 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:53:54.019306 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:53:54.020487 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:53:54.021170 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_city_vehicle"} */
drop table if exists "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_backup" cascade
[0m12:53:54.021907 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m12:53:54.022711 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m12:53:54.035928 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.012 seconds
[0m12:53:54.036717 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.013 seconds
[0m12:53:54.037351 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.014 seconds
[0m12:53:54.040661 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m12:53:54.041236 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.022 seconds
[0m12:53:54.042924 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: Close
[0m12:53:54.044985 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m12:53:54.047163 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m12:53:54.051007 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d853480-882e-4885-9bd2-c1c00764c882', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207064C4710>]}
[0m12:53:54.051568 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d853480-882e-4885-9bd2-c1c00764c882', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207060A6830>]}
[0m12:53:54.052047 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d853480-882e-4885-9bd2-c1c00764c882', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020706454730>]}
[0m12:53:54.053710 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d853480-882e-4885-9bd2-c1c00764c882', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020703884C30>]}
[0m12:53:54.053148 [info ] [Thread-1 (]: 1 of 7 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.51s]
[0m12:53:54.054990 [info ] [Thread-4 (]: 4 of 7 OK created sql table model analytics.shipments_by_city_vehicle .......... [[32mSELECT 3[0m in 0.51s]
[0m12:53:54.057788 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m12:53:54.055860 [info ] [Thread-2 (]: 2 of 7 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 31[0m in 0.51s]
[0m12:53:54.058754 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m12:53:54.056814 [info ] [Thread-3 (]: 3 of 7 OK created sql table model analytics.fct_shipments_revenue .............. [[32mSELECT 31[0m in 0.51s]
[0m12:53:54.059520 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.shipments_by_hour_day
[0m12:53:54.060534 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m12:53:54.061560 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m12:53:54.062301 [info ] [Thread-1 (]: 5 of 7 START sql table model analytics.shipments_by_hour_day ................... [RUN]
[0m12:53:54.064631 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.daily_shipments_status, now model.dbt_delivery_analytics.shipments_by_hour_day)
[0m12:53:54.065663 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_hour_day
[0m12:53:54.066325 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m12:53:54.072124 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:53:54.072933 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments_sla
[0m12:53:54.074053 [info ] [Thread-4 (]: 6 of 7 START sql table model analytics.courier_performance ..................... [RUN]
[0m12:53:54.076329 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.shipments_by_city_vehicle, now model.dbt_delivery_analytics.courier_performance)
[0m12:53:54.075507 [info ] [Thread-2 (]: 7 of 7 START sql table model analytics.fct_shipments_sla ....................... [RUN]
[0m12:53:54.077429 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m12:53:54.078711 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.fct_shipments_sla)
[0m12:53:54.084855 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m12:53:54.099247 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_sla
[0m12:53:54.100456 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.shipments_by_hour_day
[0m12:53:54.106553 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:53:54.113693 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:53:54.115232 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m12:53:54.120660 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m12:53:54.122699 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:53:54.123568 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_sla
[0m12:53:54.124250 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: BEGIN
[0m12:53:54.129819 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:53:54.130475 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:53:54.131550 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:53:54.132068 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m12:53:54.132587 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m12:53:54.133926 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:53:54.134742 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: BEGIN
[0m12:53:54.135800 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:53:54.208498 [debug] [Thread-1 (]: SQL status: BEGIN in 0.078 seconds
[0m12:53:54.209236 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:53:54.209811 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */

  
    

  create  table "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select
        s.shipment_id,
        s.shipment_date::timestamp as shipment_timestamp,
        extract(hour from s.shipment_date::timestamp) as shipment_hour,
        extract(dow from s.shipment_date) as day_of_week,
        ca.delivered_at,
        s.expected_delivery_date
    from "delivery_analytics"."raw"."shipments" s
    left join "delivery_analytics"."raw"."courier_assignments" ca
        on s.shipment_id = ca.shipment_id
)

select
    day_of_week,
    shipment_hour,
    count(distinct shipment_id) as total_shipments,
    sum(case when delivered_at is not null and delivered_at::date <= expected_delivery_date::date then 1 else 0 end) as on_time_deliveries,
    sum(case when delivered_at::date > expected_delivery_date::date then 1 else 0 end) as late_deliveries
from fct
group by day_of_week, shipment_hour
order by day_of_week, shipment_hour
  );
  
[0m12:53:54.214670 [debug] [Thread-4 (]: SQL status: BEGIN in 0.082 seconds
[0m12:53:54.215167 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:53:54.215643 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id
  );
  
[0m12:53:54.221435 [debug] [Thread-2 (]: SQL status: BEGIN in 0.086 seconds
[0m12:53:54.221973 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:53:54.222485 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,

        case
            when delivered_at is null then null
            else (delivered_at::date - expected_delivery_date::date)
        end as delivery_delay_days,

        case
            when delivered_at is null then null
            when delivered_at::date > expected_delivery_date::date then true
            else false
        end as late_delivery

    from "delivery_analytics"."analytics"."fct_shipments"
)

select *
from sla
  );
  
[0m12:53:54.226277 [debug] [Thread-4 (]: SQL status: SELECT 5 in 0.010 seconds
[0m12:53:54.230237 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:53:54.230738 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance" rename to "courier_performance__dbt_backup"
[0m12:53:54.233042 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:53:54.236866 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:53:54.237384 [debug] [Thread-2 (]: SQL status: SELECT 31 in 0.014 seconds
[0m12:53:54.237899 [debug] [Thread-1 (]: SQL status: SELECT 7 in 0.028 seconds
[0m12:53:54.238470 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance__dbt_tmp" rename to "courier_performance"
[0m12:53:54.242385 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:53:54.246354 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:53:54.247136 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla" rename to "fct_shipments_sla__dbt_backup"
[0m12:53:54.247785 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */
alter table "delivery_analytics"."analytics"."shipments_by_hour_day" rename to "shipments_by_hour_day__dbt_backup"
[0m12:53:54.248436 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:53:54.250612 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m12:53:54.251202 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:53:54.251899 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:53:54.252853 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m12:53:54.257517 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:53:54.258125 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m12:53:54.261676 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:53:54.262320 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp" rename to "fct_shipments_sla"
[0m12:53:54.263158 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */
alter table "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_tmp" rename to "shipments_by_hour_day"
[0m12:53:54.264535 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m12:53:54.265303 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:53:54.265917 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:53:54.268820 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."courier_performance__dbt_backup"
[0m12:53:54.271024 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m12:53:54.273756 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: COMMIT
[0m12:53:54.274999 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:53:54.276013 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:53:54.277019 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:53:54.277958 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
drop table if exists "delivery_analytics"."analytics"."courier_performance__dbt_backup" cascade
[0m12:53:54.278980 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m12:53:54.279986 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: COMMIT
[0m12:53:54.282822 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m12:53:54.283457 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:53:54.291789 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup"
[0m12:53:54.292558 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.012 seconds
[0m12:53:54.295192 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_backup"
[0m12:53:54.296595 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:53:54.298513 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m12:53:54.299606 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:53:54.300513 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup" cascade
[0m12:53:54.301748 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */
drop table if exists "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_backup" cascade
[0m12:53:54.302626 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d853480-882e-4885-9bd2-c1c00764c882', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207065378C0>]}
[0m12:53:54.304186 [info ] [Thread-4 (]: 6 of 7 OK created sql table model analytics.courier_performance ................ [[32mSELECT 5[0m in 0.23s]
[0m12:53:54.305296 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m12:53:54.310339 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.007 seconds
[0m12:53:54.312341 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: Close
[0m12:53:54.313356 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d853480-882e-4885-9bd2-c1c00764c882', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020706081F70>]}
[0m12:53:54.314231 [info ] [Thread-2 (]: 7 of 7 OK created sql table model analytics.fct_shipments_sla .................. [[32mSELECT 31[0m in 0.23s]
[0m12:53:54.315306 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_sla
[0m12:53:54.315837 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.012 seconds
[0m12:53:54.318337 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: Close
[0m12:53:54.319558 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d853480-882e-4885-9bd2-c1c00764c882', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020705BB8710>]}
[0m12:53:54.320615 [info ] [Thread-1 (]: 5 of 7 OK created sql table model analytics.shipments_by_hour_day .............. [[32mSELECT 7[0m in 0.26s]
[0m12:53:54.321967 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.shipments_by_hour_day
[0m12:53:54.324370 [debug] [MainThread]: Using postgres connection "master"
[0m12:53:54.324961 [debug] [MainThread]: On master: BEGIN
[0m12:53:54.325354 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:53:54.387483 [debug] [MainThread]: SQL status: BEGIN in 0.062 seconds
[0m12:53:54.388174 [debug] [MainThread]: On master: COMMIT
[0m12:53:54.388558 [debug] [MainThread]: Using postgres connection "master"
[0m12:53:54.389099 [debug] [MainThread]: On master: COMMIT
[0m12:53:54.389851 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:53:54.390595 [debug] [MainThread]: On master: Close
[0m12:53:54.391540 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:53:54.391936 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m12:53:54.392265 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m12:53:54.392662 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_sla' was properly closed.
[0m12:53:54.393004 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipments_by_hour_day' was properly closed.
[0m12:53:54.393331 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m12:53:54.393682 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_revenue' was properly closed.
[0m12:53:54.394198 [info ] [MainThread]: 
[0m12:53:54.395015 [info ] [MainThread]: Finished running 7 table models in 0 hours 0 minutes and 1.61 seconds (1.61s).
[0m12:53:54.397723 [debug] [MainThread]: Command end result
[0m12:53:54.431293 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:53:54.435039 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:53:54.478770 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m12:53:54.479317 [info ] [MainThread]: 
[0m12:53:54.480186 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:53:54.480869 [info ] [MainThread]: 
[0m12:53:54.481690 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=7
[0m12:53:54.483148 [debug] [MainThread]: Command `dbt run` succeeded at 12:53:54.482997 after 3.59 seconds
[0m12:53:54.483641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020704492270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207017774D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002070645C470>]}
[0m12:53:54.484117 [debug] [MainThread]: Flushing usage events
[0m12:53:55.807701 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:57:21.020524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D5100980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D5B9CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D61E7D90>]}


============================== 12:57:21.026633 | d960f04a-4c27-4087-895a-ee95df1b5308 ==============================
[0m12:57:21.026633 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:57:21.027996 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt run --select marts.analytics', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:57:21.276848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd960f04a-4c27-4087-895a-ee95df1b5308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D5568770>]}
[0m12:57:21.358636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd960f04a-4c27-4087-895a-ee95df1b5308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D608EE00>]}
[0m12:57:21.359964 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:57:21.777938 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:57:21.980933 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m12:57:21.981743 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\analytics\shipments_trend_daily.sql
[0m12:57:22.481126 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd960f04a-4c27-4087-895a-ee95df1b5308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D7AE9D50>]}
[0m12:57:22.677910 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:57:22.682636 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:57:22.714904 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd960f04a-4c27-4087-895a-ee95df1b5308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D607FD40>]}
[0m12:57:22.715791 [info ] [MainThread]: Found 16 models, 7 data tests, 5 sources, 459 macros
[0m12:57:22.716650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd960f04a-4c27-4087-895a-ee95df1b5308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D20CB150>]}
[0m12:57:22.719408 [info ] [MainThread]: 
[0m12:57:22.720217 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:57:22.720853 [info ] [MainThread]: 
[0m12:57:22.721891 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:57:22.727802 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m12:57:22.858322 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m12:57:22.858879 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m12:57:22.859286 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:57:22.989303 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.130 seconds
[0m12:57:22.991728 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m12:57:22.995310 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m12:57:23.004616 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:57:23.005076 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m12:57:23.005445 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:57:23.063152 [debug] [ThreadPool]: SQL status: BEGIN in 0.058 seconds
[0m12:57:23.063700 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:57:23.064103 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m12:57:23.072718 [debug] [ThreadPool]: SQL status: SELECT 15 in 0.008 seconds
[0m12:57:23.074974 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m12:57:23.075711 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m12:57:23.086199 [debug] [MainThread]: Using postgres connection "master"
[0m12:57:23.086697 [debug] [MainThread]: On master: BEGIN
[0m12:57:23.087070 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:57:23.145096 [debug] [MainThread]: SQL status: BEGIN in 0.058 seconds
[0m12:57:23.145733 [debug] [MainThread]: Using postgres connection "master"
[0m12:57:23.146221 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m12:57:23.156484 [debug] [MainThread]: SQL status: SELECT 5 in 0.010 seconds
[0m12:57:23.159155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd960f04a-4c27-4087-895a-ee95df1b5308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D7F15230>]}
[0m12:57:23.159781 [debug] [MainThread]: On master: ROLLBACK
[0m12:57:23.160521 [debug] [MainThread]: Using postgres connection "master"
[0m12:57:23.161005 [debug] [MainThread]: On master: BEGIN
[0m12:57:23.161949 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m12:57:23.162393 [debug] [MainThread]: On master: COMMIT
[0m12:57:23.162777 [debug] [MainThread]: Using postgres connection "master"
[0m12:57:23.163189 [debug] [MainThread]: On master: COMMIT
[0m12:57:23.163776 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:57:23.164169 [debug] [MainThread]: On master: Close
[0m12:57:23.169754 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m12:57:23.170522 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m12:57:23.171231 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m12:57:23.171804 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m12:57:23.172632 [info ] [Thread-3 (]: 3 of 8 START sql table model analytics.fct_shipments_revenue ................... [RUN]
[0m12:57:23.173430 [info ] [Thread-4 (]: 4 of 8 START sql table model analytics.shipments_by_city_vehicle ............... [RUN]
[0m12:57:23.175890 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m12:57:23.176889 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.shipments_by_city_vehicle'
[0m12:57:23.174334 [info ] [Thread-1 (]: 1 of 8 START sql table model analytics.daily_shipments_status .................. [RUN]
[0m12:57:23.177713 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m12:57:23.178514 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m12:57:23.174980 [info ] [Thread-2 (]: 2 of 8 START sql table model analytics.fct_shipments ........................... [RUN]
[0m12:57:23.179660 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m12:57:23.190130 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:57:23.194947 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:57:23.195892 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m12:57:23.196546 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m12:57:23.197525 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m12:57:23.201427 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:57:23.205057 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m12:57:23.206396 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m12:57:23.207041 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m12:57:23.266014 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:57:23.270711 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:57:23.271966 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m12:57:23.272664 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m12:57:23.277195 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m12:57:23.281296 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:57:23.282750 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:57:23.283617 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:57:23.284135 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m12:57:23.284733 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: BEGIN
[0m12:57:23.285451 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m12:57:23.286227 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:57:23.286951 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:57:23.287776 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:57:23.288516 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m12:57:23.289478 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m12:57:23.290180 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:57:23.291046 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:57:23.376836 [debug] [Thread-3 (]: SQL status: BEGIN in 0.091 seconds
[0m12:57:23.377668 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:57:23.378517 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    s.shipment_id,
    ca.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    ca.delivered_at,
    s.price,

    case
        when ca.delivered_at is null then null
        when ca.delivered_at::date > s.expected_delivery_date then true
        else false
    end as late_delivery

from "delivery_analytics"."raw"."shipments" s
left join "delivery_analytics"."raw"."courier_assignments" ca
    on s.shipment_id = ca.shipment_id
  );
  
[0m12:57:23.381128 [debug] [Thread-4 (]: SQL status: BEGIN in 0.094 seconds
[0m12:57:23.381648 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:57:23.382135 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_city_vehicle"} */

  
    

  create  table "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select
        s.shipment_id,
        s.shipment_date,
        ca.courier_id,
        c.city,
        c.vehicle_type,
        s.expected_delivery_date,
        ca.delivered_at
    from "delivery_analytics"."raw"."shipments" s
    left join "delivery_analytics"."raw"."courier_assignments" ca
        on s.shipment_id = ca.shipment_id
    left join "delivery_analytics"."raw"."couriers" c
        on ca.courier_id = c.courier_id
)

select
    city,
    vehicle_type,
    count(distinct shipment_id) as total_shipments,
    sum(case when delivered_at is not null and delivered_at::date <= expected_delivery_date::date then 1 else 0 end) as on_time_deliveries,
    sum(case when delivered_at::date > expected_delivery_date::date then 1 else 0 end) as late_deliveries
from fct
group by city, vehicle_type
order by city, vehicle_type
  );
  
[0m12:57:23.391278 [debug] [Thread-1 (]: SQL status: BEGIN in 0.100 seconds
[0m12:57:23.391932 [debug] [Thread-3 (]: SQL status: SELECT 31 in 0.013 seconds
[0m12:57:23.392416 [debug] [Thread-2 (]: SQL status: BEGIN in 0.102 seconds
[0m12:57:23.392972 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:57:23.393457 [debug] [Thread-4 (]: SQL status: SELECT 3 in 0.011 seconds
[0m12:57:23.405082 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:57:23.405669 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:57:23.406327 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    select
    status_timestamp::date as shipment_date,
    status,
    count(*) as shipments_count
from "delivery_analytics"."analytics"."stg_shipment_status"
group by
    status_timestamp::date,
    status
order by
    shipment_date,
    status
  );
  
[0m12:57:23.410140 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:57:23.410732 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue" rename to "fct_shipments_revenue__dbt_backup"
[0m12:57:23.411467 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."raw"."shipments"
),

assignments as (
    select
        shipment_id,
        courier_id,
        delivered_at
    from "delivery_analytics"."raw"."courier_assignments"
)

select
    s.shipment_id,
    a.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    a.delivered_at,
    s.price,

    case
        when a.delivered_at is null then null
        when a.delivered_at::date > s.expected_delivery_date::date then true
        else false
    end as late_delivery

from shipments s
left join assignments a
    on s.shipment_id = a.shipment_id
  );
  
[0m12:57:23.412249 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_city_vehicle"} */
alter table "delivery_analytics"."analytics"."shipments_by_city_vehicle" rename to "shipments_by_city_vehicle__dbt_backup"
[0m12:57:23.413395 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:57:23.413946 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:57:23.417069 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:57:23.420267 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:57:23.420857 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp" rename to "fct_shipments_revenue"
[0m12:57:23.421466 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_city_vehicle"} */
alter table "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_tmp" rename to "shipments_by_city_vehicle"
[0m12:57:23.422549 [debug] [Thread-1 (]: SQL status: SELECT 9 in 0.010 seconds
[0m12:57:23.423128 [debug] [Thread-2 (]: SQL status: SELECT 31 in 0.010 seconds
[0m12:57:23.423606 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:57:23.427467 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:57:23.428032 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.006 seconds
[0m12:57:23.431663 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:57:23.457414 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m12:57:23.458146 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m12:57:23.460342 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: COMMIT
[0m12:57:23.460964 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m12:57:23.461601 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:57:23.462333 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:57:23.463041 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:57:23.463612 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:57:23.464153 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m12:57:23.464670 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: COMMIT
[0m12:57:23.468121 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:57:23.471590 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:57:23.472480 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m12:57:23.473111 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m12:57:23.473928 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m12:57:23.482934 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup"
[0m12:57:23.483610 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.010 seconds
[0m12:57:23.484222 [debug] [Thread-4 (]: SQL status: COMMIT in 0.012 seconds
[0m12:57:23.484769 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.011 seconds
[0m12:57:23.491936 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:57:23.494295 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m12:57:23.497062 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_backup"
[0m12:57:23.499010 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m12:57:23.499664 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup" cascade
[0m12:57:23.500371 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:57:23.501553 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:57:23.502115 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:57:23.502855 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m12:57:23.503523 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_city_vehicle"} */
drop table if exists "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_backup" cascade
[0m12:57:23.504107 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m12:57:23.505842 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m12:57:23.509058 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m12:57:23.509752 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.007 seconds
[0m12:57:23.510382 [debug] [Thread-2 (]: SQL status: COMMIT in 0.005 seconds
[0m12:57:23.510889 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.006 seconds
[0m12:57:23.511834 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:57:23.515181 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m12:57:23.518034 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m12:57:23.520425 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: Close
[0m12:57:23.521136 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m12:57:23.522452 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:57:23.525850 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m12:57:23.526901 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd960f04a-4c27-4087-895a-ee95df1b5308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D801CDD0>]}
[0m12:57:23.527588 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd960f04a-4c27-4087-895a-ee95df1b5308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D7FF0F70>]}
[0m12:57:23.528893 [info ] [Thread-3 (]: 3 of 8 OK created sql table model analytics.fct_shipments_revenue .............. [[32mSELECT 31[0m in 0.35s]
[0m12:57:23.531243 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m12:57:23.530037 [info ] [Thread-4 (]: 4 of 8 OK created sql table model analytics.shipments_by_city_vehicle .......... [[32mSELECT 3[0m in 0.35s]
[0m12:57:23.532204 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.shipments_by_hour_day
[0m12:57:23.533443 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m12:57:23.534156 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.007 seconds
[0m12:57:23.534775 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.009 seconds
[0m12:57:23.535520 [info ] [Thread-3 (]: 5 of 8 START sql table model analytics.shipments_by_hour_day ................... [RUN]
[0m12:57:23.537903 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m12:57:23.539614 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m12:57:23.540411 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments_revenue, now model.dbt_delivery_analytics.shipments_by_hour_day)
[0m12:57:23.541528 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_hour_day
[0m12:57:23.542640 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd960f04a-4c27-4087-895a-ee95df1b5308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D81F1310>]}
[0m12:57:23.543467 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd960f04a-4c27-4087-895a-ee95df1b5308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D81F4200>]}
[0m12:57:23.549185 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:57:23.550245 [info ] [Thread-1 (]: 1 of 8 OK created sql table model analytics.daily_shipments_status ............. [[32mSELECT 9[0m in 0.36s]
[0m12:57:23.551267 [info ] [Thread-2 (]: 2 of 8 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 31[0m in 0.35s]
[0m12:57:23.552780 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m12:57:23.553682 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m12:57:23.555008 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m12:57:23.555616 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.shipments_trend_daily
[0m12:57:23.556170 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.shipments_by_hour_day
[0m12:57:23.556694 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments_sla
[0m12:57:23.557419 [info ] [Thread-4 (]: 6 of 8 START sql table model analytics.courier_performance ..................... [RUN]
[0m12:57:23.563284 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:57:23.558222 [info ] [Thread-2 (]: 8 of 8 START sql table model analytics.shipments_trend_daily ................... [RUN]
[0m12:57:23.565378 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.shipments_by_city_vehicle, now model.dbt_delivery_analytics.courier_performance)
[0m12:57:23.564353 [info ] [Thread-1 (]: 7 of 8 START sql table model analytics.fct_shipments_sla ....................... [RUN]
[0m12:57:23.566377 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.shipments_trend_daily)
[0m12:57:23.566977 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m12:57:23.567688 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.daily_shipments_status, now model.dbt_delivery_analytics.fct_shipments_sla)
[0m12:57:23.568271 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.shipments_trend_daily
[0m12:57:23.571633 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m12:57:23.572310 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_sla
[0m12:57:23.573214 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:57:23.577086 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_trend_daily"
[0m12:57:23.580318 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:57:23.580934 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: BEGIN
[0m12:57:23.582024 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:57:23.584588 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m12:57:23.589630 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m12:57:23.591002 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.shipments_trend_daily
[0m12:57:23.591651 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_sla
[0m12:57:23.596995 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_trend_daily"
[0m12:57:23.601273 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:57:23.602662 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:57:23.603351 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m12:57:23.603900 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m12:57:23.605672 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_daily"
[0m12:57:23.606518 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:57:23.607104 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_trend_daily: BEGIN
[0m12:57:23.607790 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: BEGIN
[0m12:57:23.608342 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:57:23.609659 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:57:23.678990 [debug] [Thread-3 (]: SQL status: BEGIN in 0.097 seconds
[0m12:57:23.679875 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:57:23.680473 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */

  
    

  create  table "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select
        s.shipment_id,
        s.shipment_date::timestamp as shipment_timestamp,
        extract(hour from s.shipment_date::timestamp) as shipment_hour,
        extract(dow from s.shipment_date) as day_of_week,
        ca.delivered_at,
        s.expected_delivery_date
    from "delivery_analytics"."raw"."shipments" s
    left join "delivery_analytics"."raw"."courier_assignments" ca
        on s.shipment_id = ca.shipment_id
)

select
    day_of_week,
    shipment_hour,
    count(distinct shipment_id) as total_shipments,
    sum(case when delivered_at is not null and delivered_at::date <= expected_delivery_date::date then 1 else 0 end) as on_time_deliveries,
    sum(case when delivered_at::date > expected_delivery_date::date then 1 else 0 end) as late_deliveries
from fct
group by day_of_week, shipment_hour
order by day_of_week, shipment_hour
  );
  
[0m12:57:23.687197 [debug] [Thread-4 (]: SQL status: BEGIN in 0.083 seconds
[0m12:57:23.687782 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:57:23.688300 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id
  );
  
[0m12:57:23.694208 [debug] [Thread-2 (]: SQL status: BEGIN in 0.086 seconds
[0m12:57:23.694930 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_daily"
[0m12:57:23.695411 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_trend_daily: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_trend_daily"} */

  
    

  create  table "delivery_analytics"."analytics"."shipments_trend_daily__dbt_tmp"
  
  
    as
  
  (
    -- models/analytics/shipments_trend_daily.sql
with shipments as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date::date as date,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from shipments
group by shipment_date::date
order by shipment_date::date
  );
  
[0m12:57:23.698531 [debug] [Thread-4 (]: SQL status: SELECT 5 in 0.010 seconds
[0m12:57:23.702424 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:57:23.703027 [debug] [Thread-3 (]: SQL status: SELECT 7 in 0.022 seconds
[0m12:57:23.703660 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance" rename to "courier_performance__dbt_backup"
[0m12:57:23.707524 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:57:23.708206 [debug] [Thread-2 (]: SQL status: SELECT 8 in 0.012 seconds
[0m12:57:23.708869 [debug] [Thread-1 (]: SQL status: BEGIN in 0.099 seconds
[0m12:57:23.709638 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */
alter table "delivery_analytics"."analytics"."shipments_by_hour_day" rename to "shipments_by_hour_day__dbt_backup"
[0m12:57:23.710249 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:57:23.713963 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_daily"
[0m12:57:23.714614 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:57:23.718397 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:57:23.719022 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m12:57:23.719655 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_trend_daily: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_trend_daily"} */
alter table "delivery_analytics"."analytics"."shipments_trend_daily__dbt_tmp" rename to "shipments_trend_daily"
[0m12:57:23.720383 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,

        case
            when delivered_at is null then null
            else (delivered_at::date - expected_delivery_date::date)
        end as delivery_delay_days,

        case
            when delivered_at is null then null
            when delivered_at::date > expected_delivery_date::date then true
            else false
        end as late_delivery

    from "delivery_analytics"."analytics"."fct_shipments"
)

select *
from sla
  );
  
[0m12:57:23.720998 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance__dbt_tmp" rename to "courier_performance"
[0m12:57:23.724542 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:57:23.725583 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */
alter table "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_tmp" rename to "shipments_by_hour_day"
[0m12:57:23.726323 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:57:23.726927 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:57:23.727453 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:57:23.729488 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_trend_daily: COMMIT
[0m12:57:23.731735 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m12:57:23.733595 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: COMMIT
[0m12:57:23.734170 [debug] [Thread-1 (]: SQL status: SELECT 31 in 0.009 seconds
[0m12:57:23.734838 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_daily"
[0m12:57:23.735547 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:57:23.736279 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:57:23.740068 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:57:23.740716 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_trend_daily: COMMIT
[0m12:57:23.741407 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m12:57:23.742067 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: COMMIT
[0m12:57:23.742814 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla" rename to "fct_shipments_sla__dbt_backup"
[0m12:57:23.744328 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:57:23.748117 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:57:23.748714 [debug] [Thread-2 (]: SQL status: COMMIT in 0.005 seconds
[0m12:57:23.749343 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp" rename to "fct_shipments_sla"
[0m12:57:23.750030 [debug] [Thread-4 (]: SQL status: COMMIT in 0.006 seconds
[0m12:57:23.750669 [debug] [Thread-3 (]: SQL status: COMMIT in 0.007 seconds
[0m12:57:23.753645 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."shipments_trend_daily__dbt_backup"
[0m12:57:23.756764 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."courier_performance__dbt_backup"
[0m12:57:23.757414 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m12:57:23.760255 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_backup"
[0m12:57:23.761384 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_daily"
[0m12:57:23.762399 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:57:23.764687 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m12:57:23.765847 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:57:23.766535 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_trend_daily: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_trend_daily"} */
drop table if exists "delivery_analytics"."analytics"."shipments_trend_daily__dbt_backup" cascade
[0m12:57:23.767203 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
drop table if exists "delivery_analytics"."analytics"."courier_performance__dbt_backup" cascade
[0m12:57:23.767861 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:57:23.768556 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */
drop table if exists "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_backup" cascade
[0m12:57:23.769493 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m12:57:23.770214 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m12:57:23.772614 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_trend_daily: Close
[0m12:57:23.773436 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:57:23.776606 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup"
[0m12:57:23.777654 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:57:23.778348 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.009 seconds
[0m12:57:23.778976 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup" cascade
[0m12:57:23.779840 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd960f04a-4c27-4087-895a-ee95df1b5308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D81E29E0>]}
[0m12:57:23.781911 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m12:57:23.782476 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.012 seconds
[0m12:57:23.783453 [info ] [Thread-2 (]: 8 of 8 OK created sql table model analytics.shipments_trend_daily .............. [[32mSELECT 8[0m in 0.21s]
[0m12:57:23.785587 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: Close
[0m12:57:23.786294 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd960f04a-4c27-4087-895a-ee95df1b5308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D8098E30>]}
[0m12:57:23.787312 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.shipments_trend_daily
[0m12:57:23.789335 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd960f04a-4c27-4087-895a-ee95df1b5308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D809A810>]}
[0m12:57:23.790100 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.007 seconds
[0m12:57:23.788535 [info ] [Thread-4 (]: 6 of 8 OK created sql table model analytics.courier_performance ................ [[32mSELECT 5[0m in 0.22s]
[0m12:57:23.794295 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_sla: Close
[0m12:57:23.791566 [info ] [Thread-3 (]: 5 of 8 OK created sql table model analytics.shipments_by_hour_day .............. [[32mSELECT 7[0m in 0.25s]
[0m12:57:23.795542 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m12:57:23.796801 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.shipments_by_hour_day
[0m12:57:23.797655 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd960f04a-4c27-4087-895a-ee95df1b5308', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D809A7B0>]}
[0m12:57:23.799190 [info ] [Thread-1 (]: 7 of 8 OK created sql table model analytics.fct_shipments_sla .................. [[32mSELECT 31[0m in 0.23s]
[0m12:57:23.800438 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_sla
[0m12:57:23.802987 [debug] [MainThread]: Using postgres connection "master"
[0m12:57:23.803560 [debug] [MainThread]: On master: BEGIN
[0m12:57:23.804001 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:57:23.878225 [debug] [MainThread]: SQL status: BEGIN in 0.074 seconds
[0m12:57:23.878903 [debug] [MainThread]: On master: COMMIT
[0m12:57:23.879402 [debug] [MainThread]: Using postgres connection "master"
[0m12:57:23.879797 [debug] [MainThread]: On master: COMMIT
[0m12:57:23.880481 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:57:23.881148 [debug] [MainThread]: On master: Close
[0m12:57:23.881878 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:57:23.882283 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m12:57:23.882746 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m12:57:23.883160 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipments_by_hour_day' was properly closed.
[0m12:57:23.883557 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_performance' was properly closed.
[0m12:57:23.883889 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_sla' was properly closed.
[0m12:57:23.884250 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipments_trend_daily' was properly closed.
[0m12:57:23.884748 [info ] [MainThread]: 
[0m12:57:23.885530 [info ] [MainThread]: Finished running 8 table models in 0 hours 0 minutes and 1.16 seconds (1.16s).
[0m12:57:23.888882 [debug] [MainThread]: Command end result
[0m12:57:23.929157 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:57:23.933467 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:57:23.942976 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m12:57:23.943581 [info ] [MainThread]: 
[0m12:57:23.944682 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:57:23.945455 [info ] [MainThread]: 
[0m12:57:23.946018 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=8
[0m12:57:23.947522 [debug] [MainThread]: Command `dbt run` succeeded at 12:57:23.947359 after 3.14 seconds
[0m12:57:23.948005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D53F1550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D7CCBB30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4D7F925D0>]}
[0m12:57:23.948483 [debug] [MainThread]: Flushing usage events
[0m12:57:25.195011 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:58:13.381502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C46AB0980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C4755CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C47BABD90>]}


============================== 12:58:13.386933 | f04eb5d7-a50e-4c49-ba46-b3c354c87c23 ==============================
[0m12:58:13.386933 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:58:13.388146 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run --select marts.analytics', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:58:13.632851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C46F18770>]}
[0m12:58:13.716899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C47A4EE00>]}
[0m12:58:13.718546 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:58:14.157692 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:58:14.360990 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 0 files changed.
[0m12:58:14.362318 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\analytics\shipments_trend_weekly.sql
[0m12:58:14.363260 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\analytics\shipments_trend_monthly.sql
[0m12:58:14.656353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'experimental_parser', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C49562050>]}
[0m12:58:14.754012 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C4948B110>]}
[0m12:58:14.961364 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:58:14.966372 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:58:15.000321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C49254830>]}
[0m12:58:15.001018 [info ] [MainThread]: Found 18 models, 7 data tests, 5 sources, 459 macros
[0m12:58:15.001916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C49726820>]}
[0m12:58:15.004880 [info ] [MainThread]: 
[0m12:58:15.005649 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:58:15.006461 [info ] [MainThread]: 
[0m12:58:15.007397 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:58:15.014779 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m12:58:15.148332 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m12:58:15.148889 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m12:58:15.149274 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:58:15.296313 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.147 seconds
[0m12:58:15.298097 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m12:58:15.302706 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m12:58:15.309807 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:58:15.310256 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m12:58:15.310627 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:58:15.371322 [debug] [ThreadPool]: SQL status: BEGIN in 0.061 seconds
[0m12:58:15.371961 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:58:15.372473 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m12:58:15.381448 [debug] [ThreadPool]: SQL status: SELECT 16 in 0.008 seconds
[0m12:58:15.383366 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m12:58:15.384097 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m12:58:15.394622 [debug] [MainThread]: Using postgres connection "master"
[0m12:58:15.395152 [debug] [MainThread]: On master: BEGIN
[0m12:58:15.395535 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:58:15.456300 [debug] [MainThread]: SQL status: BEGIN in 0.061 seconds
[0m12:58:15.456961 [debug] [MainThread]: Using postgres connection "master"
[0m12:58:15.457458 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m12:58:15.467716 [debug] [MainThread]: SQL status: SELECT 5 in 0.010 seconds
[0m12:58:15.470551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C49843DD0>]}
[0m12:58:15.471171 [debug] [MainThread]: On master: ROLLBACK
[0m12:58:15.471958 [debug] [MainThread]: Using postgres connection "master"
[0m12:58:15.472647 [debug] [MainThread]: On master: BEGIN
[0m12:58:15.473819 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m12:58:15.474255 [debug] [MainThread]: On master: COMMIT
[0m12:58:15.474633 [debug] [MainThread]: Using postgres connection "master"
[0m12:58:15.475054 [debug] [MainThread]: On master: COMMIT
[0m12:58:15.475664 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:58:15.476124 [debug] [MainThread]: On master: Close
[0m12:58:15.481825 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m12:58:15.482480 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m12:58:15.483025 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.daily_shipments_status
[0m12:58:15.483591 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m12:58:15.484371 [info ] [Thread-4 (]: 4 of 10 START sql table model analytics.shipments_by_city_vehicle .............. [RUN]
[0m12:58:15.485077 [info ] [Thread-3 (]: 3 of 10 START sql table model analytics.fct_shipments_revenue .................. [RUN]
[0m12:58:15.487739 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.shipments_by_city_vehicle'
[0m12:58:15.488528 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_revenue'
[0m12:58:15.485987 [info ] [Thread-1 (]: 1 of 10 START sql table model analytics.daily_shipments_status ................. [RUN]
[0m12:58:15.489389 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m12:58:15.490038 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_revenue
[0m12:58:15.486794 [info ] [Thread-2 (]: 2 of 10 START sql table model analytics.fct_shipments .......................... [RUN]
[0m12:58:15.490980 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.daily_shipments_status'
[0m12:58:15.501828 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:58:15.506058 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:58:15.506925 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m12:58:15.507636 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.daily_shipments_status
[0m12:58:15.509112 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m12:58:15.512263 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:58:15.515974 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m12:58:15.517873 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m12:58:15.518603 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_revenue
[0m12:58:15.567708 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:58:15.572768 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:58:15.574190 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.daily_shipments_status
[0m12:58:15.579680 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:58:15.580384 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m12:58:15.584758 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m12:58:15.586100 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:58:15.586983 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: BEGIN
[0m12:58:15.587704 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:58:15.588448 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:58:15.588952 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:58:15.589507 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: BEGIN
[0m12:58:15.590181 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: BEGIN
[0m12:58:15.590819 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:58:15.591655 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:58:15.592416 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m12:58:15.593271 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m12:58:15.595338 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:58:15.683635 [debug] [Thread-4 (]: SQL status: BEGIN in 0.095 seconds
[0m12:58:15.684288 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:58:15.684848 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_city_vehicle"} */

  
    

  create  table "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select
        s.shipment_id,
        s.shipment_date,
        ca.courier_id,
        c.city,
        c.vehicle_type,
        s.expected_delivery_date,
        ca.delivered_at
    from "delivery_analytics"."raw"."shipments" s
    left join "delivery_analytics"."raw"."courier_assignments" ca
        on s.shipment_id = ca.shipment_id
    left join "delivery_analytics"."raw"."couriers" c
        on ca.courier_id = c.courier_id
)

select
    city,
    vehicle_type,
    count(distinct shipment_id) as total_shipments,
    sum(case when delivered_at is not null and delivered_at::date <= expected_delivery_date::date then 1 else 0 end) as on_time_deliveries,
    sum(case when delivered_at::date > expected_delivery_date::date then 1 else 0 end) as late_deliveries
from fct
group by city, vehicle_type
order by city, vehicle_type
  );
  
[0m12:58:15.699678 [debug] [Thread-4 (]: SQL status: SELECT 3 in 0.014 seconds
[0m12:58:15.711563 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:58:15.712314 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_city_vehicle"} */
alter table "delivery_analytics"."analytics"."shipments_by_city_vehicle" rename to "shipments_by_city_vehicle__dbt_backup"
[0m12:58:15.713112 [debug] [Thread-1 (]: SQL status: BEGIN in 0.121 seconds
[0m12:58:15.713860 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:58:15.714499 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:58:15.714971 [debug] [Thread-3 (]: SQL status: BEGIN in 0.123 seconds
[0m12:58:15.715436 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */

  
    

  create  table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp"
  
  
    as
  
  (
    select
    status_timestamp::date as shipment_date,
    status,
    count(*) as shipments_count
from "delivery_analytics"."analytics"."stg_shipment_status"
group by
    status_timestamp::date,
    status
order by
    shipment_date,
    status
  );
  
[0m12:58:15.722213 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:58:15.722900 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:58:15.723613 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_city_vehicle"} */
alter table "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_tmp" rename to "shipments_by_city_vehicle"
[0m12:58:15.724117 [debug] [Thread-2 (]: SQL status: BEGIN in 0.129 seconds
[0m12:58:15.724654 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp"
  
  
    as
  
  (
    -- Analiza prihoda po danu, po kuriru ili po mjesecu.

select
    s.shipment_id,
    ca.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    ca.delivered_at,
    s.price,

    case
        when ca.delivered_at is null then null
        when ca.delivered_at::date > s.expected_delivery_date then true
        else false
    end as late_delivery

from "delivery_analytics"."raw"."shipments" s
left join "delivery_analytics"."raw"."courier_assignments" ca
    on s.shipment_id = ca.shipment_id
  );
  
[0m12:58:15.725267 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:58:15.725966 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics"."raw"."shipments"
),

assignments as (
    select
        shipment_id,
        courier_id,
        delivered_at
    from "delivery_analytics"."raw"."courier_assignments"
)

select
    s.shipment_id,
    a.courier_id,
    s.shipment_date,
    s.expected_delivery_date,
    a.delivered_at,
    s.price,

    case
        when a.delivered_at is null then null
        when a.delivered_at::date > s.expected_delivery_date::date then true
        else false
    end as late_delivery

from shipments s
left join assignments a
    on s.shipment_id = a.shipment_id
  );
  
[0m12:58:15.726532 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:58:15.747912 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: COMMIT
[0m12:58:15.748564 [debug] [Thread-3 (]: SQL status: SELECT 31 in 0.023 seconds
[0m12:58:15.749403 [debug] [Thread-1 (]: SQL status: SELECT 9 in 0.026 seconds
[0m12:58:15.750079 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:58:15.750711 [debug] [Thread-2 (]: SQL status: SELECT 31 in 0.024 seconds
[0m12:58:15.754768 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:58:15.759109 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:58:15.759699 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: COMMIT
[0m12:58:15.763517 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:58:15.764136 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue" rename to "fct_shipments_revenue__dbt_backup"
[0m12:58:15.764887 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status" rename to "daily_shipments_status__dbt_backup"
[0m12:58:15.765774 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m12:58:15.767046 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:58:15.770052 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:58:15.770577 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m12:58:15.771429 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m12:58:15.772311 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
alter table "delivery_analytics"."analytics"."daily_shipments_status__dbt_tmp" rename to "daily_shipments_status"
[0m12:58:15.775800 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:58:15.779951 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:58:15.780703 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
alter table "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_tmp" rename to "fct_shipments_revenue"
[0m12:58:15.781600 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:58:15.782244 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m12:58:15.784633 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m12:58:15.785234 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:58:15.785906 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:58:15.787755 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m12:58:15.788361 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m12:58:15.788936 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: COMMIT
[0m12:58:15.789547 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:58:15.791628 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m12:58:15.792336 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: COMMIT
[0m12:58:15.792965 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:58:15.793793 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m12:58:15.797505 [debug] [Thread-4 (]: SQL status: COMMIT in 0.032 seconds
[0m12:58:15.798267 [debug] [Thread-1 (]: SQL status: COMMIT in 0.006 seconds
[0m12:58:15.798875 [debug] [Thread-3 (]: SQL status: COMMIT in 0.005 seconds
[0m12:58:15.799419 [debug] [Thread-2 (]: SQL status: COMMIT in 0.005 seconds
[0m12:58:15.807427 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_backup"
[0m12:58:15.810300 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup"
[0m12:58:15.813218 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup"
[0m12:58:15.816218 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments__dbt_backup"
[0m12:58:15.822670 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_city_vehicle"
[0m12:58:15.823812 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.daily_shipments_status"
[0m12:58:15.824977 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_revenue"
[0m12:58:15.826653 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:58:15.827364 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_city_vehicle"} */
drop table if exists "delivery_analytics"."analytics"."shipments_by_city_vehicle__dbt_backup" cascade
[0m12:58:15.828079 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.daily_shipments_status"} */
drop table if exists "delivery_analytics"."analytics"."daily_shipments_status__dbt_backup" cascade
[0m12:58:15.828812 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_revenue"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_revenue__dbt_backup" cascade
[0m12:58:15.829571 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments__dbt_backup" cascade
[0m12:58:15.837324 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.007 seconds
[0m12:58:15.840461 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_city_vehicle: Close
[0m12:58:15.841054 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.011 seconds
[0m12:58:15.841692 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.011 seconds
[0m12:58:15.844021 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.014 seconds
[0m12:58:15.846130 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments_revenue: Close
[0m12:58:15.848119 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m12:58:15.849738 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.daily_shipments_status: Close
[0m12:58:15.850502 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C494D3750>]}
[0m12:58:15.851381 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C49B5AD50>]}
[0m12:58:15.852259 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C49BF7D90>]}
[0m12:58:15.854536 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C467823D0>]}
[0m12:58:15.853630 [info ] [Thread-4 (]: 4 of 10 OK created sql table model analytics.shipments_by_city_vehicle ......... [[32mSELECT 3[0m in 0.35s]
[0m12:58:15.855891 [info ] [Thread-3 (]: 3 of 10 OK created sql table model analytics.fct_shipments_revenue ............. [[32mSELECT 31[0m in 0.36s]
[0m12:58:15.859204 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.shipments_by_city_vehicle
[0m12:58:15.857008 [info ] [Thread-2 (]: 2 of 10 OK created sql table model analytics.fct_shipments ..................... [[32mSELECT 31[0m in 0.35s]
[0m12:58:15.860326 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_revenue
[0m12:58:15.861149 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.shipments_by_hour_day
[0m12:58:15.862370 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m12:58:15.858136 [info ] [Thread-1 (]: 1 of 10 OK created sql table model analytics.daily_shipments_status ............ [[32mSELECT 9[0m in 0.36s]
[0m12:58:15.863470 [info ] [Thread-4 (]: 5 of 10 START sql table model analytics.shipments_by_hour_day .................. [RUN]
[0m12:58:15.864937 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.daily_shipments_status
[0m12:58:15.865524 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.courier_performance
[0m12:58:15.866143 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_shipments_sla
[0m12:58:15.866871 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.shipments_by_city_vehicle, now model.dbt_delivery_analytics.shipments_by_hour_day)
[0m12:58:15.867655 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.shipments_trend_daily
[0m12:58:15.868414 [info ] [Thread-3 (]: 6 of 10 START sql table model analytics.courier_performance .................... [RUN]
[0m12:58:15.869857 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_hour_day
[0m12:58:15.869179 [info ] [Thread-2 (]: 7 of 10 START sql table model analytics.fct_shipments_sla ...................... [RUN]
[0m12:58:15.871827 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments_revenue, now model.dbt_delivery_analytics.courier_performance)
[0m12:58:15.870725 [info ] [Thread-1 (]: 8 of 10 START sql table model analytics.shipments_trend_daily .................. [RUN]
[0m12:58:15.875764 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:58:15.876388 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.fct_shipments_sla)
[0m12:58:15.877079 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.courier_performance
[0m12:58:15.877787 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.daily_shipments_status, now model.dbt_delivery_analytics.shipments_trend_daily)
[0m12:58:15.878668 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_sla
[0m12:58:15.882574 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_performance"
[0m12:58:15.883302 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.shipments_trend_daily
[0m12:58:15.886688 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:58:15.890602 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_trend_daily"
[0m12:58:15.891279 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.shipments_by_hour_day
[0m12:58:15.896861 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:58:15.897535 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.courier_performance
[0m12:58:15.902495 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_performance"
[0m12:58:15.903677 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.shipments_trend_daily
[0m12:58:15.904438 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_sla
[0m12:58:15.908888 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_trend_daily"
[0m12:58:15.913926 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:58:15.914848 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:58:15.915442 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:58:15.916199 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: BEGIN
[0m12:58:15.916926 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: BEGIN
[0m12:58:15.917480 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:58:15.918101 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m12:58:15.919488 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:58:15.920197 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_daily"
[0m12:58:15.920900 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: BEGIN
[0m12:58:15.921599 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_trend_daily: BEGIN
[0m12:58:15.922190 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:58:15.922861 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:58:15.995427 [debug] [Thread-4 (]: SQL status: BEGIN in 0.077 seconds
[0m12:58:15.996162 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:58:15.996781 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */

  
    

  create  table "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_tmp"
  
  
    as
  
  (
    with fct as (
    select
        s.shipment_id,
        s.shipment_date::timestamp as shipment_timestamp,
        extract(hour from s.shipment_date::timestamp) as shipment_hour,
        extract(dow from s.shipment_date) as day_of_week,
        ca.delivered_at,
        s.expected_delivery_date
    from "delivery_analytics"."raw"."shipments" s
    left join "delivery_analytics"."raw"."courier_assignments" ca
        on s.shipment_id = ca.shipment_id
)

select
    day_of_week,
    shipment_hour,
    count(distinct shipment_id) as total_shipments,
    sum(case when delivered_at is not null and delivered_at::date <= expected_delivery_date::date then 1 else 0 end) as on_time_deliveries,
    sum(case when delivered_at::date > expected_delivery_date::date then 1 else 0 end) as late_deliveries
from fct
group by day_of_week, shipment_hour
order by day_of_week, shipment_hour
  );
  
[0m12:58:16.004843 [debug] [Thread-3 (]: SQL status: BEGIN in 0.087 seconds
[0m12:58:16.005437 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:58:16.005909 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */

  
    

  create  table "delivery_analytics"."analytics"."courier_performance__dbt_tmp"
  
  
    as
  
  (
    select
    courier_id,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from "delivery_analytics"."analytics"."fct_shipments"
group by courier_id
  );
  
[0m12:58:16.011007 [debug] [Thread-2 (]: SQL status: BEGIN in 0.089 seconds
[0m12:58:16.011538 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:58:16.012071 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp"
  
  
    as
  
  (
    /*

Upoređujemo planirani datum isporuke 
(expected_delivery_date) sa stvarnim datumom 
isporuke (delivered_at). Izračunavamo trajanje 
isporuke i označavamo late_delivery kao true/false.

*/


with sla as (
    select
        shipment_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        delivered_at,

        case
            when delivered_at is null then null
            else (delivered_at::date - expected_delivery_date::date)
        end as delivery_delay_days,

        case
            when delivered_at is null then null
            when delivered_at::date > expected_delivery_date::date then true
            else false
        end as late_delivery

    from "delivery_analytics"."analytics"."fct_shipments"
)

select *
from sla
  );
  
[0m12:58:16.015364 [debug] [Thread-3 (]: SQL status: SELECT 5 in 0.009 seconds
[0m12:58:16.015997 [debug] [Thread-4 (]: SQL status: SELECT 7 in 0.019 seconds
[0m12:58:16.016497 [debug] [Thread-1 (]: SQL status: BEGIN in 0.094 seconds
[0m12:58:16.020825 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:58:16.021502 [debug] [Thread-2 (]: SQL status: SELECT 31 in 0.009 seconds
[0m12:58:16.025171 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:58:16.025805 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_daily"
[0m12:58:16.026458 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance" rename to "courier_performance__dbt_backup"
[0m12:58:16.030486 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:58:16.031167 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */
alter table "delivery_analytics"."analytics"."shipments_by_hour_day" rename to "shipments_by_hour_day__dbt_backup"
[0m12:58:16.032219 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_trend_daily: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_trend_daily"} */

  
    

  create  table "delivery_analytics"."analytics"."shipments_trend_daily__dbt_tmp"
  
  
    as
  
  (
    -- models/analytics/shipments_trend_daily.sql
with shipments as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    shipment_date::date as date,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from shipments
group by shipment_date::date
order by shipment_date::date
  );
  
[0m12:58:16.033452 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla" rename to "fct_shipments_sla__dbt_backup"
[0m12:58:16.034415 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:58:16.037975 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:58:16.038577 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m12:58:16.039175 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m12:58:16.039781 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
alter table "delivery_analytics"."analytics"."courier_performance__dbt_tmp" rename to "courier_performance"
[0m12:58:16.043122 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:58:16.043721 [debug] [Thread-1 (]: SQL status: SELECT 8 in 0.009 seconds
[0m12:58:16.047498 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:58:16.048457 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */
alter table "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_tmp" rename to "shipments_by_hour_day"
[0m12:58:16.049475 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:58:16.053536 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_daily"
[0m12:58:16.054266 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
alter table "delivery_analytics"."analytics"."fct_shipments_sla__dbt_tmp" rename to "fct_shipments_sla"
[0m12:58:16.056872 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m12:58:16.057583 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_trend_daily: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_trend_daily"} */
alter table "delivery_analytics"."analytics"."shipments_trend_daily" rename to "shipments_trend_daily__dbt_backup"
[0m12:58:16.058179 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m12:58:16.058945 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:58:16.061217 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: COMMIT
[0m12:58:16.061814 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m12:58:16.062454 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m12:58:16.063089 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: COMMIT
[0m12:58:16.063689 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:58:16.066069 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m12:58:16.069489 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_daily"
[0m12:58:16.070398 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: COMMIT
[0m12:58:16.071038 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:58:16.071716 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_trend_daily: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_trend_daily"} */
alter table "delivery_analytics"."analytics"."shipments_trend_daily__dbt_tmp" rename to "shipments_trend_daily"
[0m12:58:16.072310 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m12:58:16.073008 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: COMMIT
[0m12:58:16.075818 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."courier_performance__dbt_backup"
[0m12:58:16.076454 [debug] [Thread-4 (]: SQL status: COMMIT in 0.004 seconds
[0m12:58:16.077101 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m12:58:16.078346 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.courier_performance"
[0m12:58:16.078928 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m12:58:16.081470 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_backup"
[0m12:58:16.088969 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_trend_daily: COMMIT
[0m12:58:16.089768 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_performance"} */
drop table if exists "delivery_analytics"."analytics"."courier_performance__dbt_backup" cascade
[0m12:58:16.092901 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup"
[0m12:58:16.093997 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_hour_day"
[0m12:58:16.094720 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_daily"
[0m12:58:16.096021 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_sla"
[0m12:58:16.096669 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_hour_day"} */
drop table if exists "delivery_analytics"."analytics"."shipments_by_hour_day__dbt_backup" cascade
[0m12:58:16.097307 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_trend_daily: COMMIT
[0m12:58:16.098017 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_sla"} */
drop table if exists "delivery_analytics"."analytics"."fct_shipments_sla__dbt_backup" cascade
[0m12:58:16.099803 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m12:58:16.102323 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics"."analytics"."shipments_trend_daily__dbt_backup"
[0m12:58:16.102885 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.007 seconds
[0m12:58:16.103859 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_daily"
[0m12:58:16.104518 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.006 seconds
[0m12:58:16.106167 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.courier_performance: Close
[0m12:58:16.106837 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_trend_daily: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_trend_daily"} */
drop table if exists "delivery_analytics"."analytics"."shipments_trend_daily__dbt_backup" cascade
[0m12:58:16.109092 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_shipments_sla: Close
[0m12:58:16.109815 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.011 seconds
[0m12:58:16.110857 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C49A0F230>]}
[0m12:58:16.113033 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipments_by_hour_day: Close
[0m12:58:16.113743 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C49A0F590>]}
[0m12:58:16.114670 [info ] [Thread-3 (]: 6 of 10 OK created sql table model analytics.courier_performance ............... [[32mSELECT 5[0m in 0.24s]
[0m12:58:16.116726 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C49A0F590>]}
[0m12:58:16.117476 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.007 seconds
[0m12:58:16.118459 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.courier_performance
[0m12:58:16.115775 [info ] [Thread-2 (]: 7 of 10 OK created sql table model analytics.fct_shipments_sla ................. [[32mSELECT 31[0m in 0.24s]
[0m12:58:16.122299 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_trend_daily: Close
[0m12:58:16.119834 [info ] [Thread-4 (]: 5 of 10 OK created sql table model analytics.shipments_by_hour_day ............. [[32mSELECT 7[0m in 0.25s]
[0m12:58:16.123193 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.shipments_trend_monthly
[0m12:58:16.124121 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_sla
[0m12:58:16.125090 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.shipments_by_hour_day
[0m12:58:16.125790 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C49A0FAD0>]}
[0m12:58:16.127233 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.shipments_trend_weekly
[0m12:58:16.126574 [info ] [Thread-3 (]: 9 of 10 START sql table model analytics.shipments_trend_monthly ................ [RUN]
[0m12:58:16.129069 [info ] [Thread-1 (]: 8 of 10 OK created sql table model analytics.shipments_trend_daily ............. [[32mSELECT 8[0m in 0.25s]
[0m12:58:16.130831 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.courier_performance, now model.dbt_delivery_analytics.shipments_trend_monthly)
[0m12:58:16.131952 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.shipments_trend_daily
[0m12:58:16.129992 [info ] [Thread-2 (]: 10 of 10 START sql table model analytics.shipments_trend_weekly ................ [RUN]
[0m12:58:16.132735 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.shipments_trend_monthly
[0m12:58:16.133663 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments_sla, now model.dbt_delivery_analytics.shipments_trend_weekly)
[0m12:58:16.137327 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_trend_monthly"
[0m12:58:16.138004 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.shipments_trend_weekly
[0m12:58:16.141608 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_trend_weekly"
[0m12:58:16.144059 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.shipments_trend_monthly
[0m12:58:16.144872 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.shipments_trend_weekly
[0m12:58:16.149080 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_trend_monthly"
[0m12:58:16.153903 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_trend_weekly"
[0m12:58:16.156689 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_weekly"
[0m12:58:16.157354 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_monthly"
[0m12:58:16.157958 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_trend_weekly: BEGIN
[0m12:58:16.158516 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_trend_monthly: BEGIN
[0m12:58:16.159098 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m12:58:16.159805 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m12:58:16.236256 [debug] [Thread-2 (]: SQL status: BEGIN in 0.077 seconds
[0m12:58:16.237116 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_weekly"
[0m12:58:16.237994 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_trend_weekly: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_trend_weekly"} */

  
    

  create  table "delivery_analytics"."analytics"."shipments_trend_weekly__dbt_tmp"
  
  
    as
  
  (
    -- models/analytics/shipments_trend_weekly.sql
with shipments as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    date_trunc('week', shipment_date) as week_start,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from shipments
group by date_trunc('week', shipment_date)
order by week_start
  );
  
[0m12:58:16.241134 [debug] [Thread-3 (]: SQL status: BEGIN in 0.081 seconds
[0m12:58:16.241691 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_monthly"
[0m12:58:16.242207 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_trend_monthly: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_trend_monthly"} */

  
    

  create  table "delivery_analytics"."analytics"."shipments_trend_monthly__dbt_tmp"
  
  
    as
  
  (
    -- models/analytics/shipments_trend_monthly.sql
with shipments as (
    select *
    from "delivery_analytics"."analytics"."fct_shipments"
)

select
    date_trunc('month', shipment_date) as month,
    count(distinct shipment_id) as total_shipments,
    sum(case when late_delivery = false then 1 else 0 end) as on_time_shipments,
    sum(case when late_delivery = true then 1 else 0 end) as late_shipments
from shipments
group by date_trunc('month', shipment_date)
order by month
  );
  
[0m12:58:16.249266 [debug] [Thread-2 (]: SQL status: SELECT 2 in 0.010 seconds
[0m12:58:16.253126 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_weekly"
[0m12:58:16.253736 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.011 seconds
[0m12:58:16.254442 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_trend_weekly: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_trend_weekly"} */
alter table "delivery_analytics"."analytics"."shipments_trend_weekly__dbt_tmp" rename to "shipments_trend_weekly"
[0m12:58:16.258769 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_monthly"
[0m12:58:16.259479 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_trend_monthly: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_trend_monthly"} */
alter table "delivery_analytics"."analytics"."shipments_trend_monthly__dbt_tmp" rename to "shipments_trend_monthly"
[0m12:58:16.260577 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:58:16.261160 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:58:16.262963 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_trend_weekly: COMMIT
[0m12:58:16.264690 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_trend_monthly: COMMIT
[0m12:58:16.265302 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_weekly"
[0m12:58:16.265886 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_monthly"
[0m12:58:16.266537 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_trend_weekly: COMMIT
[0m12:58:16.267177 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_trend_monthly: COMMIT
[0m12:58:16.268941 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m12:58:16.271504 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics"."analytics"."shipments_trend_weekly__dbt_backup"
[0m12:58:16.272055 [debug] [Thread-3 (]: SQL status: COMMIT in 0.004 seconds
[0m12:58:16.273097 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_weekly"
[0m12:58:16.276019 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics"."analytics"."shipments_trend_monthly__dbt_backup"
[0m12:58:16.276642 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_trend_weekly: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_trend_weekly"} */
drop table if exists "delivery_analytics"."analytics"."shipments_trend_weekly__dbt_backup" cascade
[0m12:58:16.277651 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_trend_monthly"
[0m12:58:16.278411 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_trend_monthly: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_trend_monthly"} */
drop table if exists "delivery_analytics"."analytics"."shipments_trend_monthly__dbt_backup" cascade
[0m12:58:16.279066 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m12:58:16.281059 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_trend_weekly: Close
[0m12:58:16.281649 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.002 seconds
[0m12:58:16.283684 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_trend_monthly: Close
[0m12:58:16.284468 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C49250710>]}
[0m12:58:16.286272 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f04eb5d7-a50e-4c49-ba46-b3c354c87c23', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C49A687D0>]}
[0m12:58:16.285473 [info ] [Thread-2 (]: 10 of 10 OK created sql table model analytics.shipments_trend_weekly ........... [[32mSELECT 2[0m in 0.15s]
[0m12:58:16.288437 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.shipments_trend_weekly
[0m12:58:16.287473 [info ] [Thread-3 (]: 9 of 10 OK created sql table model analytics.shipments_trend_monthly ........... [[32mSELECT 1[0m in 0.16s]
[0m12:58:16.289806 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.shipments_trend_monthly
[0m12:58:16.292152 [debug] [MainThread]: Using postgres connection "master"
[0m12:58:16.292655 [debug] [MainThread]: On master: BEGIN
[0m12:58:16.293063 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:58:16.353767 [debug] [MainThread]: SQL status: BEGIN in 0.061 seconds
[0m12:58:16.354419 [debug] [MainThread]: On master: COMMIT
[0m12:58:16.354971 [debug] [MainThread]: Using postgres connection "master"
[0m12:58:16.355626 [debug] [MainThread]: On master: COMMIT
[0m12:58:16.356425 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:58:16.357019 [debug] [MainThread]: On master: Close
[0m12:58:16.358135 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:58:16.358643 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m12:58:16.359076 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m12:58:16.359558 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipments_by_hour_day' was properly closed.
[0m12:58:16.360054 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipments_trend_monthly' was properly closed.
[0m12:58:16.360593 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipments_trend_daily' was properly closed.
[0m12:58:16.361027 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipments_trend_weekly' was properly closed.
[0m12:58:16.361691 [info ] [MainThread]: 
[0m12:58:16.362582 [info ] [MainThread]: Finished running 10 table models in 0 hours 0 minutes and 1.35 seconds (1.35s).
[0m12:58:16.365397 [debug] [MainThread]: Command end result
[0m12:58:16.399150 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:58:16.403015 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:58:16.414805 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m12:58:16.415377 [info ] [MainThread]: 
[0m12:58:16.416205 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:58:16.417176 [info ] [MainThread]: 
[0m12:58:16.418014 [info ] [MainThread]: Done. PASS=10 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=10
[0m12:58:16.419959 [debug] [MainThread]: Command `dbt run` succeeded at 12:58:16.419733 after 3.22 seconds
[0m12:58:16.420608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C47A4BD70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C495F8D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C49CEBF50>]}
[0m12:58:16.421132 [debug] [MainThread]: Flushing usage events
[0m12:58:17.640959 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:56:22.503325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE6AB0C980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE6B5C0B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE6BBFFD90>]}


============================== 11:56:22.543204 | 0b9868a8-b051-42ef-904e-b3d4fd4fd196 ==============================
[0m11:56:22.543204 [info ] [MainThread]: Running with dbt=1.10.15
[0m11:56:22.544518 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select fact_shipments', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m11:56:22.929481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0b9868a8-b051-42ef-904e-b3d4fd4fd196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE6AF70770>]}
[0m11:56:23.022978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0b9868a8-b051-42ef-904e-b3d4fd4fd196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE6BAA6E00>]}
[0m11:56:23.026907 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m11:56:23.625379 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m11:56:23.979157 [debug] [MainThread]: Partial parsing enabled: 16 files deleted, 10 files added, 3 files changed.
[0m11:56:23.980031 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\dimensions\dim_city.sql
[0m11:56:23.980543 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\intermediate\int_shipment_status.sql
[0m11:56:23.981016 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\dimensions\dim_customers.sql
[0m11:56:23.981460 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\staging\stg_customers.sql
[0m11:56:23.981890 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\intermediate\int_shipment_delays.sql
[0m11:56:23.982351 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\facts\fct_courier_performance.sql
[0m11:56:23.982788 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\dimensions\dim_couriers.sql
[0m11:56:23.983202 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\facts\fct_courier_load.sql
[0m11:56:23.983609 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\facts\fct_shipments.sql
[0m11:56:23.984052 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\intermediate\int_active_delays.sql
[0m11:56:23.984855 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\sources.yml
[0m11:56:23.985503 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\core\dim_couriers.sql
[0m11:56:23.986227 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\analytics\shipments_by_hour_day.sql
[0m11:56:23.986858 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\staging\stg_courier_assigments.sql
[0m11:56:23.987482 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\analytics\shipments_trend_weekly.sql
[0m11:56:23.988009 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\analytics\shipments_trend_monthly.sql
[0m11:56:23.988409 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\analytics\fct_shipments_revenue.sql
[0m11:56:23.988914 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\analytics\fct_shipments_sla.sql
[0m11:56:23.989661 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\analytics\fct_shipments.sql
[0m11:56:23.990274 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\core\dim_users.sql
[0m11:56:23.990853 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\staging\stg_shipment_status.sql
[0m11:56:23.991388 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\core\dim_dates.sql
[0m11:56:23.991882 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\analytics\shipments_trend_daily.sql
[0m11:56:23.992267 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\analytics\daily_shipments_status.sql
[0m11:56:23.992823 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\analytics\shipments_by_city_vehicle.sql
[0m11:56:23.993341 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\analytics\courier_performance.sql
[0m11:56:23.993924 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\staging\stg_users.sql
[0m11:56:23.994691 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\staging\stg_couriers.sql
[0m11:56:23.995416 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\staging\stg_shipments.sql
[0m11:56:24.552870 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'shipments_by_hour_day' in the 'models' section of file 'models\schema.yml'
[0m11:56:24.827743 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_delivery_analytics.not_null_shipments_by_hour_day_day_of_week.e2d126a571' (models\schema.yml) depends on a node named 'shipments_by_hour_day' in package '' which was not found
[0m11:56:24.828774 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_delivery_analytics.not_null_shipments_by_hour_day_shipment_hour.6ee57859e9' (models\schema.yml) depends on a node named 'shipments_by_hour_day' in package '' which was not found
[0m11:56:24.968378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0b9868a8-b051-42ef-904e-b3d4fd4fd196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE6D53C850>]}
[0m11:56:25.141304 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m11:56:25.146996 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m11:56:25.247995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0b9868a8-b051-42ef-904e-b3d4fd4fd196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE6BA93D40>]}
[0m11:56:25.248696 [info ] [MainThread]: Found 12 models, 5 data tests, 5 sources, 459 macros
[0m11:56:25.249458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0b9868a8-b051-42ef-904e-b3d4fd4fd196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE6D1AA970>]}
[0m11:56:25.250375 [warn ] [MainThread]: The selection criterion 'fact_shipments' does not match any enabled nodes
[0m11:56:25.252639 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m11:56:25.253772 [debug] [MainThread]: Command end result
[0m11:56:25.300635 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m11:56:25.306030 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m11:56:25.311548 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m11:56:25.313084 [debug] [MainThread]: Command `dbt run` succeeded at 11:56:25.312845 after 3.03 seconds
[0m11:56:25.313833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE6D82B040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE6B9CB590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BE6D87AED0>]}
[0m11:56:25.314585 [debug] [MainThread]: Flushing usage events
[0m11:56:26.512182 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:56:33.930186 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CFBC58980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CFC710B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CFCD4FD90>]}


============================== 11:56:33.935803 | 79a8869e-4335-48aa-bd0f-f709e9ec1775 ==============================
[0m11:56:33.935803 [info ] [MainThread]: Running with dbt=1.10.15
[0m11:56:33.936813 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt test --select fact_shipments', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m11:56:34.178381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '79a8869e-4335-48aa-bd0f-f709e9ec1775', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CFC0C0770>]}
[0m11:56:34.260293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '79a8869e-4335-48aa-bd0f-f709e9ec1775', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CFCBF6E00>]}
[0m11:56:34.261584 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m11:56:34.700926 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m11:56:34.898731 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:56:34.899293 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:56:34.958842 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '79a8869e-4335-48aa-bd0f-f709e9ec1775', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CFE225250>]}
[0m11:56:35.072358 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m11:56:35.076517 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m11:56:35.125980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '79a8869e-4335-48aa-bd0f-f709e9ec1775', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CFE837890>]}
[0m11:56:35.126632 [info ] [MainThread]: Found 12 models, 5 data tests, 5 sources, 459 macros
[0m11:56:35.127375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '79a8869e-4335-48aa-bd0f-f709e9ec1775', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CFE8E8050>]}
[0m11:56:35.128351 [warn ] [MainThread]: The selection criterion 'fact_shipments' does not match any enabled nodes
[0m11:56:35.130299 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m11:56:35.134346 [debug] [MainThread]: Command end result
[0m11:56:35.174387 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m11:56:35.178028 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m11:56:35.184143 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m11:56:35.185777 [debug] [MainThread]: Command `dbt test` succeeded at 11:56:35.185552 after 1.45 seconds
[0m11:56:35.186627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CFE8BBC70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CFCB1B590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CFE7EF890>]}
[0m11:56:35.187110 [debug] [MainThread]: Flushing usage events
[0m11:56:36.305764 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:59:07.409191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002016BC28980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002016C6DCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002016CD1FD90>]}


============================== 11:59:07.413858 | 0b24acf8-3a1f-44d8-bb41-e49285945c02 ==============================
[0m11:59:07.413858 [info ] [MainThread]: Running with dbt=1.10.15
[0m11:59:07.415295 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt list', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m11:59:07.610117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0b24acf8-3a1f-44d8-bb41-e49285945c02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002016C090770>]}
[0m11:59:07.683520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0b24acf8-3a1f-44d8-bb41-e49285945c02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002016CBC6F10>]}
[0m11:59:07.684791 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m11:59:08.008093 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m11:59:08.162709 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:59:08.163648 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\facts\fct_shipments.sql
[0m11:59:08.712500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0b24acf8-3a1f-44d8-bb41-e49285945c02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002016E619C50>]}
[0m11:59:08.814565 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m11:59:08.825791 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m11:59:08.883035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0b24acf8-3a1f-44d8-bb41-e49285945c02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002016E5455E0>]}
[0m11:59:08.884515 [info ] [MainThread]: dbt_delivery_analytics.marts.dimensions.dim_city
[0m11:59:08.885217 [info ] [MainThread]: dbt_delivery_analytics.marts.dimensions.dim_couriers
[0m11:59:08.885949 [info ] [MainThread]: dbt_delivery_analytics.marts.dimensions.dim_customers
[0m11:59:08.886629 [info ] [MainThread]: dbt_delivery_analytics.marts.facts.fct_courier_load
[0m11:59:08.887426 [info ] [MainThread]: dbt_delivery_analytics.marts.facts.fct_courier_performance
[0m11:59:08.887925 [info ] [MainThread]: dbt_delivery_analytics.marts.facts.fct_shipments
[0m11:59:08.888469 [info ] [MainThread]: dbt_delivery_analytics.intermediate.int_active_delays
[0m11:59:08.889020 [info ] [MainThread]: dbt_delivery_analytics.intermediate.int_shipment_delays
[0m11:59:08.889492 [info ] [MainThread]: dbt_delivery_analytics.intermediate.int_shipment_status
[0m11:59:08.890090 [info ] [MainThread]: dbt_delivery_analytics.staging.stg_couriers
[0m11:59:08.890752 [info ] [MainThread]: dbt_delivery_analytics.staging.stg_customers
[0m11:59:08.891298 [info ] [MainThread]: dbt_delivery_analytics.staging.stg_shipments
[0m11:59:08.891813 [info ] [MainThread]: source:dbt_delivery_analytics.raw.courier_assignments
[0m11:59:08.892305 [info ] [MainThread]: source:dbt_delivery_analytics.raw.couriers
[0m11:59:08.892796 [info ] [MainThread]: source:dbt_delivery_analytics.raw.shipment_status
[0m11:59:08.893350 [info ] [MainThread]: source:dbt_delivery_analytics.raw.shipments
[0m11:59:08.893935 [info ] [MainThread]: source:dbt_delivery_analytics.raw.users
[0m11:59:08.894441 [info ] [MainThread]: dbt_delivery_analytics.accepted_values_fct_shipments_late_delivery__True__False
[0m11:59:08.895041 [info ] [MainThread]: dbt_delivery_analytics.not_null_fct_shipments_courier_id
[0m11:59:08.895640 [info ] [MainThread]: dbt_delivery_analytics.not_null_fct_shipments_price
[0m11:59:08.896235 [info ] [MainThread]: dbt_delivery_analytics.not_null_fct_shipments_shipment_id
[0m11:59:08.896769 [info ] [MainThread]: dbt_delivery_analytics.unique_fct_shipments_shipment_id
[0m11:59:08.897933 [debug] [MainThread]: Command `dbt list` succeeded at 11:59:08.897750 after 1.64 seconds
[0m11:59:08.898500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002016E3643D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002016E3657F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002016E7E28F0>]}
[0m11:59:08.899061 [debug] [MainThread]: Flushing usage events
[0m11:59:09.872849 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:00:55.841914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3D41BC980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3D4C6CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3D52AFD90>]}


============================== 12:00:55.847354 | 680455cf-e9a6-4710-a743-6bb7dcde03de ==============================
[0m12:00:55.847354 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:00:55.848832 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'use_colors': 'True', 'invocation_command': 'dbt run --select fct_shipments', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:00:56.103936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '680455cf-e9a6-4710-a743-6bb7dcde03de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3D4620770>]}
[0m12:00:56.185466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '680455cf-e9a6-4710-a743-6bb7dcde03de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3D5156E00>]}
[0m12:00:56.186726 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:00:56.601274 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:00:56.804180 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:00:56.805304 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\facts\fct_shipments.sql
[0m12:00:57.497872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '680455cf-e9a6-4710-a743-6bb7dcde03de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3D6C54C50>]}
[0m12:00:57.621814 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:00:57.626116 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:00:57.654724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '680455cf-e9a6-4710-a743-6bb7dcde03de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3D5143D40>]}
[0m12:00:57.655713 [info ] [MainThread]: Found 12 models, 5 data tests, 5 sources, 459 macros
[0m12:00:57.656638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '680455cf-e9a6-4710-a743-6bb7dcde03de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3D6957930>]}
[0m12:00:57.658861 [info ] [MainThread]: 
[0m12:00:57.659619 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:00:57.660317 [info ] [MainThread]: 
[0m12:00:57.661495 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:00:57.663162 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m12:00:58.028021 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m12:00:58.028542 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m12:00:58.028938 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:00:58.157168 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.128 seconds
[0m12:00:58.159204 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m12:00:58.167818 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m12:00:58.174846 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:00:58.175297 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m12:00:58.175674 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:00:58.243045 [debug] [ThreadPool]: SQL status: BEGIN in 0.067 seconds
[0m12:00:58.243651 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:00:58.244060 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m12:00:58.270987 [debug] [ThreadPool]: SQL status: SELECT 18 in 0.026 seconds
[0m12:00:58.273390 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m12:00:58.274599 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m12:00:58.285997 [debug] [MainThread]: Using postgres connection "master"
[0m12:00:58.286614 [debug] [MainThread]: On master: BEGIN
[0m12:00:58.287076 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:00:58.354093 [debug] [MainThread]: SQL status: BEGIN in 0.067 seconds
[0m12:00:58.355483 [debug] [MainThread]: Using postgres connection "master"
[0m12:00:58.356299 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m12:00:58.378969 [debug] [MainThread]: SQL status: SELECT 5 in 0.022 seconds
[0m12:00:58.381222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '680455cf-e9a6-4710-a743-6bb7dcde03de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3D6F6ADD0>]}
[0m12:00:58.381787 [debug] [MainThread]: On master: ROLLBACK
[0m12:00:58.382555 [debug] [MainThread]: Using postgres connection "master"
[0m12:00:58.383160 [debug] [MainThread]: On master: BEGIN
[0m12:00:58.384400 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m12:00:58.384952 [debug] [MainThread]: On master: COMMIT
[0m12:00:58.385409 [debug] [MainThread]: Using postgres connection "master"
[0m12:00:58.385774 [debug] [MainThread]: On master: COMMIT
[0m12:00:58.386432 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:00:58.386930 [debug] [MainThread]: On master: Close
[0m12:00:58.394338 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m12:00:58.395156 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics.fct_shipments ........................... [RUN]
[0m12:00:58.396303 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m12:00:58.396927 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m12:00:58.407286 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m12:00:58.410569 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m12:00:58.488465 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m12:00:58.492460 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:00:58.493235 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m12:00:58.493721 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:00:58.564125 [debug] [Thread-1 (]: SQL status: BEGIN in 0.070 seconds
[0m12:00:58.564814 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:00:58.565402 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (

    select *
    from "delivery_analytics"."analytics"."stg_shipments"

),

customers as (

    select *
    from "delivery_analytics"."analytics"."stg_customers"

),

couriers as (

    select *
    from "delivery_analytics"."analytics"."stg_couriers"

)

select
    s.shipment_id,

    -- keys
    s.sender_id,
    s.receiver_id,
    s.courier_id,

    -- dates
    s.shipment_date,
    s.expected_delivery_date,
    s.completed_at,

    -- metrics
    s.price,
    s.distance_km,

    -- attributes
    s.delivery_city,
    s.delivery_type,
    s.current_status,

    -- derived metrics
    case
        when s.completed_at is not null
        then (s.completed_at::date - s.shipment_date)
    end as delivery_duration_days,

    case
        when s.completed_at > s.expected_delivery_date then true
        else false
    end as is_late_delivery

from shipments s




/*
    FACT TABLE: fct_shipments

    Svaki red u ovoj tabeli predstavlja jednu pošiljku (shipment) iz naše baze.

    Svrha:
    - Centralizovati sve metrike vezane za pošiljke na jednom mjestu
    - Omogućiti analitiku po vremenu, kuriru, tipu pošiljke i gradu
    - Poslužiti kao ulaz za BI dashboard ili dalju analitiku u marts layer-u

    Šta se ovdje računa:
    1. Ključne kolone:
        - shipment_id
        - sender_id, receiver_id
        - courier_id
    2. Datumi:
        - shipment_date (kada je pošiljka poslana)
        - expected_delivery_date (planirani datum isporuke)
        - completed_at (kada je pošiljka zaista dostavljena)
    3. Metričke kolone:
        - price
        - distance_km
    4. Atributi:
        - delivery_city
        - delivery_type (standard / express)
        - current_status (pending, picked_up, delivered)
    5. Derived metrics (izvedene kolone):
        - delivery_duration_days = koliko je dana trajala pošiljka
        - is_late_delivery = boolean, true ako je pošiljka dostavljena nakon expected_delivery_date

    Napomena:
    - Ova tabela služi kao "fact" jer sadrži sve mjere koje se mogu agregirati
    - Za dimenzije ćemo imati posebne dim_tables: dim_customers, dim_couriers, dim_cities
    - Može se koristiti za: analizu performansi kurira, kašnjenja po gradu, tip pošiljke, revenue po gradu itd.

*/
  );
  
[0m12:00:58.570443 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "analytics.stg_customers" does not exist
LINE 22:     from "delivery_analytics"."analytics"."stg_customers"
                  ^

[0m12:00:58.571145 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: ROLLBACK
[0m12:00:58.571953 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m12:00:58.588105 [debug] [Thread-1 (]: Database Error in model fct_shipments (models\marts\facts\fct_shipments.sql)
  relation "analytics.stg_customers" does not exist
  LINE 22:     from "delivery_analytics"."analytics"."stg_customers"
                    ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\facts\fct_shipments.sql
[0m12:00:58.591520 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '680455cf-e9a6-4710-a743-6bb7dcde03de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3D7230DD0>]}
[0m12:00:58.592459 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics.fct_shipments .................. [[31mERROR[0m in 0.19s]
[0m12:00:58.593778 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m12:00:58.594670 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments (models\marts\facts\fct_shipments.sql)
  relation "analytics.stg_customers" does not exist
  LINE 22:     from "delivery_analytics"."analytics"."stg_customers"
                    ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\facts\fct_shipments.sql.
[0m12:00:58.597447 [debug] [MainThread]: Using postgres connection "master"
[0m12:00:58.597858 [debug] [MainThread]: On master: BEGIN
[0m12:00:58.598264 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:00:58.676385 [debug] [MainThread]: SQL status: BEGIN in 0.078 seconds
[0m12:00:58.677017 [debug] [MainThread]: On master: COMMIT
[0m12:00:58.677515 [debug] [MainThread]: Using postgres connection "master"
[0m12:00:58.677967 [debug] [MainThread]: On master: COMMIT
[0m12:00:58.678716 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:00:58.679215 [debug] [MainThread]: On master: Close
[0m12:00:58.679925 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:00:58.680299 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m12:00:58.680663 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m12:00:58.681013 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments' was properly closed.
[0m12:00:58.681378 [info ] [MainThread]: 
[0m12:00:58.682103 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 1.02 seconds (1.02s).
[0m12:00:58.683475 [debug] [MainThread]: Command end result
[0m12:00:58.718180 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:00:58.723027 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:00:58.733102 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m12:00:58.733816 [info ] [MainThread]: 
[0m12:00:58.734731 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m12:00:58.735597 [info ] [MainThread]: 
[0m12:00:58.736334 [error] [MainThread]: [31mFailure in model fct_shipments (models\marts\facts\fct_shipments.sql)[0m
[0m12:00:58.737147 [error] [MainThread]:   Database Error in model fct_shipments (models\marts\facts\fct_shipments.sql)
  relation "analytics.stg_customers" does not exist
  LINE 22:     from "delivery_analytics"."analytics"."stg_customers"
                    ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\facts\fct_shipments.sql
[0m12:00:58.737935 [info ] [MainThread]: 
[0m12:00:58.739003 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\facts\fct_shipments.sql
[0m12:00:58.740057 [info ] [MainThread]: 
[0m12:00:58.740901 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m12:00:58.742881 [debug] [MainThread]: Command `dbt run` failed at 12:00:58.742691 after 3.09 seconds
[0m12:00:58.743528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3D7175860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3D6F27610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3D6F26210>]}
[0m12:00:58.744081 [debug] [MainThread]: Flushing usage events
[0m12:00:59.903531 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:01:54.085100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF105C8980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF11080B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF116BFD90>]}


============================== 12:01:54.091510 | 9a56cf16-1af6-4ec4-97e5-b4948a0e5269 ==============================
[0m12:01:54.091510 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:01:54.092924 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run --select fct_shipments', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:01:54.343578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9a56cf16-1af6-4ec4-97e5-b4948a0e5269', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF10A30770>]}
[0m12:01:54.420415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9a56cf16-1af6-4ec4-97e5-b4948a0e5269', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF11566E00>]}
[0m12:01:54.421617 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:01:54.778588 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:01:54.929295 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:01:54.930008 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\sources.yml
[0m12:01:55.143089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9a56cf16-1af6-4ec4-97e5-b4948a0e5269', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF13051250>]}
[0m12:01:55.226044 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:01:55.229740 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:01:55.258926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9a56cf16-1af6-4ec4-97e5-b4948a0e5269', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF11553D40>]}
[0m12:01:55.259752 [info ] [MainThread]: Found 12 models, 5 data tests, 3 sources, 459 macros
[0m12:01:55.260666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a56cf16-1af6-4ec4-97e5-b4948a0e5269', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF12D97A10>]}
[0m12:01:55.262994 [info ] [MainThread]: 
[0m12:01:55.263663 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:01:55.264222 [info ] [MainThread]: 
[0m12:01:55.265044 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:01:55.266239 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m12:01:55.454131 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m12:01:55.454561 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m12:01:55.454884 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:01:55.548600 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.094 seconds
[0m12:01:55.550161 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m12:01:55.559082 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m12:01:55.569102 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:01:55.569522 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m12:01:55.569801 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:01:55.633818 [debug] [ThreadPool]: SQL status: BEGIN in 0.064 seconds
[0m12:01:55.634325 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:01:55.634699 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m12:01:55.644484 [debug] [ThreadPool]: SQL status: SELECT 18 in 0.009 seconds
[0m12:01:55.646298 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m12:01:55.646793 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m12:01:55.656940 [debug] [MainThread]: Using postgres connection "master"
[0m12:01:55.657361 [debug] [MainThread]: On master: BEGIN
[0m12:01:55.657668 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:01:55.729213 [debug] [MainThread]: SQL status: BEGIN in 0.071 seconds
[0m12:01:55.729759 [debug] [MainThread]: Using postgres connection "master"
[0m12:01:55.730208 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m12:01:55.742938 [debug] [MainThread]: SQL status: SELECT 5 in 0.012 seconds
[0m12:01:55.744577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a56cf16-1af6-4ec4-97e5-b4948a0e5269', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF131FAF70>]}
[0m12:01:55.745069 [debug] [MainThread]: On master: ROLLBACK
[0m12:01:55.745637 [debug] [MainThread]: Using postgres connection "master"
[0m12:01:55.745966 [debug] [MainThread]: On master: BEGIN
[0m12:01:55.746732 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m12:01:55.747055 [debug] [MainThread]: On master: COMMIT
[0m12:01:55.747337 [debug] [MainThread]: Using postgres connection "master"
[0m12:01:55.747598 [debug] [MainThread]: On master: COMMIT
[0m12:01:55.748085 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:01:55.748404 [debug] [MainThread]: On master: Close
[0m12:01:55.752716 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m12:01:55.753343 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics.fct_shipments ........................... [RUN]
[0m12:01:55.754075 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m12:01:55.754458 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m12:01:55.761704 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m12:01:55.763326 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m12:01:55.800958 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m12:01:55.802688 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:01:55.803094 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m12:01:55.803417 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:01:55.861382 [debug] [Thread-1 (]: SQL status: BEGIN in 0.058 seconds
[0m12:01:55.861918 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:01:55.862347 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (

    select *
    from "delivery_analytics"."analytics"."stg_shipments"

),

customers as (

    select *
    from "delivery_analytics"."analytics"."stg_customers"

),

couriers as (

    select *
    from "delivery_analytics"."analytics"."stg_couriers"

)

select
    s.shipment_id,

    -- keys
    s.sender_id,
    s.receiver_id,
    s.courier_id,

    -- dates
    s.shipment_date,
    s.expected_delivery_date,
    s.completed_at,

    -- metrics
    s.price,
    s.distance_km,

    -- attributes
    s.delivery_city,
    s.delivery_type,
    s.current_status,

    -- derived metrics
    case
        when s.completed_at is not null
        then (s.completed_at::date - s.shipment_date)
    end as delivery_duration_days,

    case
        when s.completed_at > s.expected_delivery_date then true
        else false
    end as is_late_delivery

from shipments s




/*
    FACT TABLE: fct_shipments

    Svaki red u ovoj tabeli predstavlja jednu pošiljku (shipment) iz naše baze.

    Svrha:
    - Centralizovati sve metrike vezane za pošiljke na jednom mjestu
    - Omogućiti analitiku po vremenu, kuriru, tipu pošiljke i gradu
    - Poslužiti kao ulaz za BI dashboard ili dalju analitiku u marts layer-u

    Šta se ovdje računa:
    1. Ključne kolone:
        - shipment_id
        - sender_id, receiver_id
        - courier_id
    2. Datumi:
        - shipment_date (kada je pošiljka poslana)
        - expected_delivery_date (planirani datum isporuke)
        - completed_at (kada je pošiljka zaista dostavljena)
    3. Metričke kolone:
        - price
        - distance_km
    4. Atributi:
        - delivery_city
        - delivery_type (standard / express)
        - current_status (pending, picked_up, delivered)
    5. Derived metrics (izvedene kolone):
        - delivery_duration_days = koliko je dana trajala pošiljka
        - is_late_delivery = boolean, true ako je pošiljka dostavljena nakon expected_delivery_date

    Napomena:
    - Ova tabela služi kao "fact" jer sadrži sve mjere koje se mogu agregirati
    - Za dimenzije ćemo imati posebne dim_tables: dim_customers, dim_couriers, dim_cities
    - Može se koristiti za: analizu performansi kurira, kašnjenja po gradu, tip pošiljke, revenue po gradu itd.

*/
  );
  
[0m12:01:55.863781 [debug] [Thread-1 (]: Postgres adapter: Postgres error: relation "analytics.stg_customers" does not exist
LINE 22:     from "delivery_analytics"."analytics"."stg_customers"
                  ^

[0m12:01:55.864233 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: ROLLBACK
[0m12:01:55.864904 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m12:01:55.872083 [debug] [Thread-1 (]: Database Error in model fct_shipments (models\marts\facts\fct_shipments.sql)
  relation "analytics.stg_customers" does not exist
  LINE 22:     from "delivery_analytics"."analytics"."stg_customers"
                    ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\facts\fct_shipments.sql
[0m12:01:55.874325 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a56cf16-1af6-4ec4-97e5-b4948a0e5269', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF13500DD0>]}
[0m12:01:55.875114 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model analytics.fct_shipments .................. [[31mERROR[0m in 0.12s]
[0m12:01:55.876037 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m12:01:55.876828 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.fct_shipments' to be skipped because of status 'error'.  Reason: Database Error in model fct_shipments (models\marts\facts\fct_shipments.sql)
  relation "analytics.stg_customers" does not exist
  LINE 22:     from "delivery_analytics"."analytics"."stg_customers"
                    ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\facts\fct_shipments.sql.
[0m12:01:55.878900 [debug] [MainThread]: Using postgres connection "master"
[0m12:01:55.879265 [debug] [MainThread]: On master: BEGIN
[0m12:01:55.879567 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:01:55.946855 [debug] [MainThread]: SQL status: BEGIN in 0.067 seconds
[0m12:01:55.947487 [debug] [MainThread]: On master: COMMIT
[0m12:01:55.947969 [debug] [MainThread]: Using postgres connection "master"
[0m12:01:55.948393 [debug] [MainThread]: On master: COMMIT
[0m12:01:55.949002 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:01:55.949466 [debug] [MainThread]: On master: Close
[0m12:01:55.949988 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:01:55.950337 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m12:01:55.950684 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m12:01:55.950950 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments' was properly closed.
[0m12:01:55.951245 [info ] [MainThread]: 
[0m12:01:55.951770 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.69 seconds (0.69s).
[0m12:01:55.953080 [debug] [MainThread]: Command end result
[0m12:01:55.980228 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:01:55.983866 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:01:55.992661 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m12:01:55.993069 [info ] [MainThread]: 
[0m12:01:55.993636 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m12:01:55.994230 [info ] [MainThread]: 
[0m12:01:55.994952 [error] [MainThread]: [31mFailure in model fct_shipments (models\marts\facts\fct_shipments.sql)[0m
[0m12:01:55.995510 [error] [MainThread]:   Database Error in model fct_shipments (models\marts\facts\fct_shipments.sql)
  relation "analytics.stg_customers" does not exist
  LINE 22:     from "delivery_analytics"."analytics"."stg_customers"
                    ^
  compiled code at target\run\dbt_delivery_analytics\models\marts\facts\fct_shipments.sql
[0m12:01:55.995968 [info ] [MainThread]: 
[0m12:01:55.996634 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\marts\facts\fct_shipments.sql
[0m12:01:55.997189 [info ] [MainThread]: 
[0m12:01:55.997885 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m12:01:55.999186 [debug] [MainThread]: Command `dbt run` failed at 12:01:55.999060 after 2.10 seconds
[0m12:01:55.999582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF13287C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF12CC3570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF12CC0B90>]}
[0m12:01:55.999981 [debug] [MainThread]: Flushing usage events
[0m12:01:56.945415 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:03:40.453271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013C92B78980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013C9362CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013C93C6FD90>]}


============================== 12:03:40.459551 | 2131656a-fbd3-4d48-a5fb-9f4a455429a8 ==============================
[0m12:03:40.459551 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:03:40.460626 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run --select staging', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:03:40.710014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2131656a-fbd3-4d48-a5fb-9f4a455429a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013C92FE0770>]}
[0m12:03:40.799328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2131656a-fbd3-4d48-a5fb-9f4a455429a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013C93B16E00>]}
[0m12:03:40.800696 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:03:41.260941 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:03:41.470040 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:03:41.470578 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:03:41.531175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2131656a-fbd3-4d48-a5fb-9f4a455429a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013C95121250>]}
[0m12:03:41.642848 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:03:41.647409 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:03:41.678789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2131656a-fbd3-4d48-a5fb-9f4a455429a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013C93B03D40>]}
[0m12:03:41.679523 [info ] [MainThread]: Found 12 models, 5 data tests, 3 sources, 459 macros
[0m12:03:41.680329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2131656a-fbd3-4d48-a5fb-9f4a455429a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013C9519DE10>]}
[0m12:03:41.683371 [info ] [MainThread]: 
[0m12:03:41.684078 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:03:41.684584 [info ] [MainThread]: 
[0m12:03:41.685485 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:03:41.693229 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m12:03:41.833487 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m12:03:41.834055 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m12:03:41.834541 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:03:41.984168 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.150 seconds
[0m12:03:41.986152 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m12:03:41.991384 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m12:03:41.999691 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:03:42.000249 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m12:03:42.000618 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:03:42.064229 [debug] [ThreadPool]: SQL status: BEGIN in 0.063 seconds
[0m12:03:42.064801 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:03:42.065202 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m12:03:42.073933 [debug] [ThreadPool]: SQL status: SELECT 18 in 0.008 seconds
[0m12:03:42.076376 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m12:03:42.077100 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m12:03:42.089521 [debug] [MainThread]: Using postgres connection "master"
[0m12:03:42.090166 [debug] [MainThread]: On master: BEGIN
[0m12:03:42.090530 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:03:42.152501 [debug] [MainThread]: SQL status: BEGIN in 0.062 seconds
[0m12:03:42.153198 [debug] [MainThread]: Using postgres connection "master"
[0m12:03:42.153979 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m12:03:42.164311 [debug] [MainThread]: SQL status: SELECT 5 in 0.010 seconds
[0m12:03:42.165994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2131656a-fbd3-4d48-a5fb-9f4a455429a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013C95939710>]}
[0m12:03:42.166552 [debug] [MainThread]: On master: ROLLBACK
[0m12:03:42.167276 [debug] [MainThread]: Using postgres connection "master"
[0m12:03:42.167677 [debug] [MainThread]: On master: BEGIN
[0m12:03:42.168661 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m12:03:42.169068 [debug] [MainThread]: On master: COMMIT
[0m12:03:42.169451 [debug] [MainThread]: Using postgres connection "master"
[0m12:03:42.169828 [debug] [MainThread]: On master: COMMIT
[0m12:03:42.170465 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:03:42.170901 [debug] [MainThread]: On master: Close
[0m12:03:42.177476 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.stg_couriers
[0m12:03:42.178290 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.stg_customers
[0m12:03:42.179565 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.stg_shipments
[0m12:03:42.179110 [info ] [Thread-1 (]: 1 of 3 START sql view model analytics.stg_couriers ............................. [RUN]
[0m12:03:42.180533 [info ] [Thread-2 (]: 2 of 3 START sql view model analytics.stg_customers ............................ [RUN]
[0m12:03:42.182178 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_couriers'
[0m12:03:42.181286 [info ] [Thread-3 (]: 3 of 3 START sql view model analytics.stg_shipments ............................ [RUN]
[0m12:03:42.183059 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_customers'
[0m12:03:42.183689 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.stg_couriers
[0m12:03:42.184420 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_shipments'
[0m12:03:42.185009 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.stg_customers
[0m12:03:42.192935 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_couriers"
[0m12:03:42.194011 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.stg_shipments
[0m12:03:42.196501 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_customers"
[0m12:03:42.199618 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_shipments"
[0m12:03:42.201757 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.stg_couriers
[0m12:03:42.251468 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_couriers"
[0m12:03:42.253030 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.stg_shipments
[0m12:03:42.253888 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.stg_customers
[0m12:03:42.258864 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_shipments"
[0m12:03:42.334716 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_customers"
[0m12:03:42.335516 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m12:03:42.336295 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: BEGIN
[0m12:03:42.336847 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:03:42.340109 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m12:03:42.341097 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m12:03:42.341858 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: BEGIN
[0m12:03:42.342626 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: BEGIN
[0m12:03:42.343323 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m12:03:42.343952 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:03:42.420821 [debug] [Thread-2 (]: SQL status: BEGIN in 0.077 seconds
[0m12:03:42.421517 [debug] [Thread-1 (]: SQL status: BEGIN in 0.085 seconds
[0m12:03:42.422045 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m12:03:42.422625 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m12:03:42.423309 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_customers"} */

  create view "delivery_analytics"."analytics"."stg_customers__dbt_tmp"
    
    
  as (
    with source as (

    select
        customer_id,
        name,
        email,
        phone,
        address,
        city
    from raw.customers

);

select * from source;
  );
[0m12:03:42.423945 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_couriers"} */

  create view "delivery_analytics"."analytics"."stg_couriers__dbt_tmp"
    
    
  as (
    with source as (

    select
        courier_id,
        name,
        vehicle_type,
        city
    from raw.couriers

)

select * from source;
  );
[0m12:03:42.425654 [debug] [Thread-1 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 18: select * from source;
                             ^

[0m12:03:42.426216 [debug] [Thread-2 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 18: );
          ^

[0m12:03:42.426805 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: ROLLBACK
[0m12:03:42.427359 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: ROLLBACK
[0m12:03:42.428283 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: Close
[0m12:03:42.428770 [debug] [Thread-3 (]: SQL status: BEGIN in 0.085 seconds
[0m12:03:42.429416 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: Close
[0m12:03:42.430037 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m12:03:42.431890 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipments"} */

  create view "delivery_analytics"."analytics"."stg_shipments__dbt_tmp"
    
    
  as (
    with source as (

    select
        shipment_id,
        sender_id,
        receiver_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        completed_at,
        price,
        delivery_city,
        delivery_type,
        distance_km
    from raw.shipments

),

status_logic as (

    select
        *,

        case
            when shipment_date is null then 'pending'
            when shipment_date is not null and completed_at is null then 'picked_up'
            when completed_at is not null then 'delivered'
        end as current_status

    from source

)

select * from status_logic;
  );
[0m12:03:42.434497 [debug] [Thread-3 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 40: select * from status_logic;
                                   ^

[0m12:03:42.435222 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: ROLLBACK
[0m12:03:42.436747 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: Close
[0m12:03:42.452483 [debug] [Thread-1 (]: Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  syntax error at or near ";"
  LINE 18: select * from source;
                               ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_couriers.sql
[0m12:03:42.455369 [debug] [Thread-2 (]: Database Error in model stg_customers (models\staging\stg_customers.sql)
  syntax error at or near ";"
  LINE 18: );
            ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_customers.sql
[0m12:03:42.459265 [debug] [Thread-3 (]: Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  syntax error at or near ";"
  LINE 40: select * from status_logic;
                                     ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipments.sql
[0m12:03:42.460054 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2131656a-fbd3-4d48-a5fb-9f4a455429a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013C956EFD10>]}
[0m12:03:42.460556 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2131656a-fbd3-4d48-a5fb-9f4a455429a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013C95B045D0>]}
[0m12:03:42.461063 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2131656a-fbd3-4d48-a5fb-9f4a455429a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013C95B049F0>]}
[0m12:03:42.461865 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model analytics.stg_couriers .................... [[31mERROR[0m in 0.27s]
[0m12:03:42.464869 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.stg_couriers
[0m12:03:42.462694 [error] [Thread-2 (]: 2 of 3 ERROR creating sql view model analytics.stg_customers ................... [[31mERROR[0m in 0.27s]
[0m12:03:42.466091 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_couriers' to be skipped because of status 'error'.  Reason: Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  syntax error at or near ";"
  LINE 18: select * from source;
                               ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_couriers.sql.
[0m12:03:42.467086 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.stg_customers
[0m12:03:42.463754 [error] [Thread-3 (]: 3 of 3 ERROR creating sql view model analytics.stg_shipments ................... [[31mERROR[0m in 0.28s]
[0m12:03:42.469184 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_customers' to be skipped because of status 'error'.  Reason: Database Error in model stg_customers (models\staging\stg_customers.sql)
  syntax error at or near ";"
  LINE 18: );
            ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_customers.sql.
[0m12:03:42.470075 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.stg_shipments
[0m12:03:42.471126 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_shipments' to be skipped because of status 'error'.  Reason: Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  syntax error at or near ";"
  LINE 40: select * from status_logic;
                                     ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipments.sql.
[0m12:03:42.473123 [debug] [MainThread]: Using postgres connection "master"
[0m12:03:42.473592 [debug] [MainThread]: On master: BEGIN
[0m12:03:42.473992 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:03:42.539135 [debug] [MainThread]: SQL status: BEGIN in 0.065 seconds
[0m12:03:42.539674 [debug] [MainThread]: On master: COMMIT
[0m12:03:42.540065 [debug] [MainThread]: Using postgres connection "master"
[0m12:03:42.540427 [debug] [MainThread]: On master: COMMIT
[0m12:03:42.541098 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:03:42.541517 [debug] [MainThread]: On master: Close
[0m12:03:42.542149 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:03:42.542548 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m12:03:42.542889 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m12:03:42.543388 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_couriers' was properly closed.
[0m12:03:42.543812 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_customers' was properly closed.
[0m12:03:42.544222 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_shipments' was properly closed.
[0m12:03:42.544788 [info ] [MainThread]: 
[0m12:03:42.545509 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 0.86 seconds (0.86s).
[0m12:03:42.547142 [debug] [MainThread]: Command end result
[0m12:03:42.579745 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:03:42.583616 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:03:42.593232 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m12:03:42.593767 [info ] [MainThread]: 
[0m12:03:42.594482 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m12:03:42.595165 [info ] [MainThread]: 
[0m12:03:42.596010 [error] [MainThread]: [31mFailure in model stg_couriers (models\staging\stg_couriers.sql)[0m
[0m12:03:42.596794 [error] [MainThread]:   Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  syntax error at or near ";"
  LINE 18: select * from source;
                               ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_couriers.sql
[0m12:03:42.597447 [info ] [MainThread]: 
[0m12:03:42.598216 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_couriers.sql
[0m12:03:42.598868 [info ] [MainThread]: 
[0m12:03:42.599586 [error] [MainThread]: [31mFailure in model stg_customers (models\staging\stg_customers.sql)[0m
[0m12:03:42.600391 [error] [MainThread]:   Database Error in model stg_customers (models\staging\stg_customers.sql)
  syntax error at or near ";"
  LINE 18: );
            ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_customers.sql
[0m12:03:42.601209 [info ] [MainThread]: 
[0m12:03:42.602165 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_customers.sql
[0m12:03:42.602936 [info ] [MainThread]: 
[0m12:03:42.603643 [error] [MainThread]: [31mFailure in model stg_shipments (models\staging\stg_shipments.sql)[0m
[0m12:03:42.604580 [error] [MainThread]:   Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  syntax error at or near ";"
  LINE 40: select * from status_logic;
                                     ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipments.sql
[0m12:03:42.605398 [info ] [MainThread]: 
[0m12:03:42.606223 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_shipments.sql
[0m12:03:42.606989 [info ] [MainThread]: 
[0m12:03:42.607730 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=3
[0m12:03:42.609397 [debug] [MainThread]: Command `dbt run` failed at 12:03:42.609231 after 2.36 seconds
[0m12:03:42.609927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013C93C92AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013C951C13D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013C93CF0C90>]}
[0m12:03:42.610391 [debug] [MainThread]: Flushing usage events
[0m12:03:43.697461 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:04:44.152503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020698DB8980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002069986CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020699EAFD90>]}


============================== 12:04:44.161355 | 4a1d3106-8f0f-43d4-ab81-09cef5a80098 ==============================
[0m12:04:44.161355 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:04:44.162651 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run --select staging', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:04:44.451606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4a1d3106-8f0f-43d4-ab81-09cef5a80098', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020699220770>]}
[0m12:04:44.540820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4a1d3106-8f0f-43d4-ab81-09cef5a80098', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020699D56E00>]}
[0m12:04:44.542745 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:04:45.023250 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:04:45.254457 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m12:04:45.255569 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\staging\stg_shipments.sql
[0m12:04:45.256333 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\staging\stg_customers.sql
[0m12:04:45.257031 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\staging\stg_couriers.sql
[0m12:04:45.655325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4a1d3106-8f0f-43d4-ab81-09cef5a80098', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002069B7C9250>]}
[0m12:04:45.787757 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:04:45.792720 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:04:45.913777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4a1d3106-8f0f-43d4-ab81-09cef5a80098', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020699D43D40>]}
[0m12:04:45.914895 [info ] [MainThread]: Found 12 models, 5 data tests, 3 sources, 459 macros
[0m12:04:45.916300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4a1d3106-8f0f-43d4-ab81-09cef5a80098', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002069B7EAEB0>]}
[0m12:04:45.919364 [info ] [MainThread]: 
[0m12:04:45.920166 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:04:45.920881 [info ] [MainThread]: 
[0m12:04:45.921805 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:04:45.927152 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m12:04:46.079992 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m12:04:46.080523 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m12:04:46.080921 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:04:46.164967 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.084 seconds
[0m12:04:46.167792 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m12:04:46.171439 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m12:04:46.179255 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:04:46.179752 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m12:04:46.180135 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:04:46.243992 [debug] [ThreadPool]: SQL status: BEGIN in 0.064 seconds
[0m12:04:46.244746 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:04:46.245302 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m12:04:46.253976 [debug] [ThreadPool]: SQL status: SELECT 18 in 0.008 seconds
[0m12:04:46.255978 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m12:04:46.256658 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m12:04:46.270745 [debug] [MainThread]: Using postgres connection "master"
[0m12:04:46.271389 [debug] [MainThread]: On master: BEGIN
[0m12:04:46.271953 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:04:46.357137 [debug] [MainThread]: SQL status: BEGIN in 0.085 seconds
[0m12:04:46.358638 [debug] [MainThread]: Using postgres connection "master"
[0m12:04:46.359161 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m12:04:46.370322 [debug] [MainThread]: SQL status: SELECT 5 in 0.011 seconds
[0m12:04:46.372152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4a1d3106-8f0f-43d4-ab81-09cef5a80098', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002069BCFC390>]}
[0m12:04:46.372773 [debug] [MainThread]: On master: ROLLBACK
[0m12:04:46.373450 [debug] [MainThread]: Using postgres connection "master"
[0m12:04:46.373948 [debug] [MainThread]: On master: BEGIN
[0m12:04:46.374946 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m12:04:46.375373 [debug] [MainThread]: On master: COMMIT
[0m12:04:46.375770 [debug] [MainThread]: Using postgres connection "master"
[0m12:04:46.376113 [debug] [MainThread]: On master: COMMIT
[0m12:04:46.376728 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:04:46.377111 [debug] [MainThread]: On master: Close
[0m12:04:46.383283 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.stg_couriers
[0m12:04:46.384156 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.stg_customers
[0m12:04:46.384720 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.stg_shipments
[0m12:04:46.385548 [info ] [Thread-1 (]: 1 of 3 START sql view model analytics.stg_couriers ............................. [RUN]
[0m12:04:46.388024 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_couriers'
[0m12:04:46.386322 [info ] [Thread-2 (]: 2 of 3 START sql view model analytics.stg_customers ............................ [RUN]
[0m12:04:46.388877 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.stg_couriers
[0m12:04:46.387225 [info ] [Thread-3 (]: 3 of 3 START sql view model analytics.stg_shipments ............................ [RUN]
[0m12:04:46.389650 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_customers'
[0m12:04:46.397546 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_shipments'
[0m12:04:46.398564 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_couriers"
[0m12:04:46.399189 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.stg_customers
[0m12:04:46.399706 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.stg_shipments
[0m12:04:46.403101 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_customers"
[0m12:04:46.405412 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_shipments"
[0m12:04:46.406830 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.stg_couriers
[0m12:04:46.453491 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_couriers"
[0m12:04:46.455241 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.stg_customers
[0m12:04:46.456173 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.stg_shipments
[0m12:04:46.460471 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_customers"
[0m12:04:46.464476 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_shipments"
[0m12:04:46.465630 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m12:04:46.466378 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: BEGIN
[0m12:04:46.467268 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:04:46.468747 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m12:04:46.469520 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m12:04:46.470084 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: BEGIN
[0m12:04:46.470617 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: BEGIN
[0m12:04:46.471147 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:04:46.471788 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m12:04:46.551473 [debug] [Thread-1 (]: SQL status: BEGIN in 0.084 seconds
[0m12:04:46.554328 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m12:04:46.554863 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_couriers"} */

  create view "delivery_analytics"."analytics"."stg_couriers__dbt_tmp"
    
    
  as (
    with source as (

    select
        courier_id,
        name,
        vehicle_type,
        city
    from raw.couriers

)

select * from source
  );
[0m12:04:46.558874 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "name" does not exist
LINE 11:         name,
                 ^

[0m12:04:46.559482 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: ROLLBACK
[0m12:04:46.560444 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: Close
[0m12:04:46.561130 [debug] [Thread-2 (]: SQL status: BEGIN in 0.090 seconds
[0m12:04:46.561643 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m12:04:46.563205 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_customers"} */

  create view "delivery_analytics"."analytics"."stg_customers__dbt_tmp"
    
    
  as (
    with source as (

    select
        customer_id,
        name,
        email,
        phone,
        address,
        city
    from raw.customers

);

select * from source
  );
[0m12:04:46.564487 [debug] [Thread-2 (]: Postgres adapter: Postgres error: syntax error at or near ";"
LINE 18: );
          ^

[0m12:04:46.565064 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: ROLLBACK
[0m12:04:46.565701 [debug] [Thread-3 (]: SQL status: BEGIN in 0.094 seconds
[0m12:04:46.566450 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m12:04:46.567086 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: Close
[0m12:04:46.567737 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipments"} */

  create view "delivery_analytics"."analytics"."stg_shipments__dbt_tmp"
    
    
  as (
    with source as (

    select
        shipment_id,
        sender_id,
        receiver_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        completed_at,
        price,
        delivery_city,
        delivery_type,
        distance_km
    from raw.shipments

),

status_logic as (

    select
        *,

        case
            when shipment_date is null then 'pending'
            when shipment_date is not null and completed_at is null then 'picked_up'
            when completed_at is not null then 'delivered'
        end as current_status

    from source

)

select * from status_logic
  );
[0m12:04:46.569650 [debug] [Thread-3 (]: Postgres adapter: Postgres error: column "courier_id" does not exist
LINE 13:         courier_id,
                 ^

[0m12:04:46.570290 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: ROLLBACK
[0m12:04:46.571579 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: Close
[0m12:04:46.585737 [debug] [Thread-1 (]: Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  column "name" does not exist
  LINE 11:         name,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_couriers.sql
[0m12:04:46.591286 [debug] [Thread-3 (]: Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  column "courier_id" does not exist
  LINE 13:         courier_id,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipments.sql
[0m12:04:46.592578 [debug] [Thread-2 (]: Database Error in model stg_customers (models\staging\stg_customers.sql)
  syntax error at or near ";"
  LINE 18: );
            ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_customers.sql
[0m12:04:46.593694 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4a1d3106-8f0f-43d4-ab81-09cef5a80098', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002069BCAC7D0>]}
[0m12:04:46.594126 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4a1d3106-8f0f-43d4-ab81-09cef5a80098', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002069BCC5860>]}
[0m12:04:46.594608 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4a1d3106-8f0f-43d4-ab81-09cef5a80098', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002069BCC57B0>]}
[0m12:04:46.595495 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model analytics.stg_couriers .................... [[31mERROR[0m in 0.20s]
[0m12:04:46.596265 [error] [Thread-3 (]: 3 of 3 ERROR creating sql view model analytics.stg_shipments ................... [[31mERROR[0m in 0.20s]
[0m12:04:46.598415 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.stg_couriers
[0m12:04:46.597197 [error] [Thread-2 (]: 2 of 3 ERROR creating sql view model analytics.stg_customers ................... [[31mERROR[0m in 0.20s]
[0m12:04:46.599397 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.stg_shipments
[0m12:04:46.600265 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_couriers' to be skipped because of status 'error'.  Reason: Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  column "name" does not exist
  LINE 11:         name,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_couriers.sql.
[0m12:04:46.601236 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.stg_customers
[0m12:04:46.602948 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_shipments' to be skipped because of status 'error'.  Reason: Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  column "courier_id" does not exist
  LINE 13:         courier_id,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipments.sql.
[0m12:04:46.604170 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_customers' to be skipped because of status 'error'.  Reason: Database Error in model stg_customers (models\staging\stg_customers.sql)
  syntax error at or near ";"
  LINE 18: );
            ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_customers.sql.
[0m12:04:46.606408 [debug] [MainThread]: Using postgres connection "master"
[0m12:04:46.606853 [debug] [MainThread]: On master: BEGIN
[0m12:04:46.607214 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:04:46.672455 [debug] [MainThread]: SQL status: BEGIN in 0.065 seconds
[0m12:04:46.673069 [debug] [MainThread]: On master: COMMIT
[0m12:04:46.673489 [debug] [MainThread]: Using postgres connection "master"
[0m12:04:46.673845 [debug] [MainThread]: On master: COMMIT
[0m12:04:46.674542 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:04:46.675229 [debug] [MainThread]: On master: Close
[0m12:04:46.676076 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:04:46.676472 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m12:04:46.676842 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m12:04:46.677185 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_couriers' was properly closed.
[0m12:04:46.677579 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_customers' was properly closed.
[0m12:04:46.677889 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_shipments' was properly closed.
[0m12:04:46.678374 [info ] [MainThread]: 
[0m12:04:46.679198 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 0.76 seconds (0.76s).
[0m12:04:46.681049 [debug] [MainThread]: Command end result
[0m12:04:46.714826 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:04:46.719619 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:04:46.729163 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m12:04:46.729657 [info ] [MainThread]: 
[0m12:04:46.730442 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m12:04:46.731153 [info ] [MainThread]: 
[0m12:04:46.732143 [error] [MainThread]: [31mFailure in model stg_couriers (models\staging\stg_couriers.sql)[0m
[0m12:04:46.733342 [error] [MainThread]:   Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  column "name" does not exist
  LINE 11:         name,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_couriers.sql
[0m12:04:46.734322 [info ] [MainThread]: 
[0m12:04:46.735081 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_couriers.sql
[0m12:04:46.735747 [info ] [MainThread]: 
[0m12:04:46.736528 [error] [MainThread]: [31mFailure in model stg_shipments (models\staging\stg_shipments.sql)[0m
[0m12:04:46.737395 [error] [MainThread]:   Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  column "courier_id" does not exist
  LINE 13:         courier_id,
                   ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipments.sql
[0m12:04:46.738092 [info ] [MainThread]: 
[0m12:04:46.738843 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_shipments.sql
[0m12:04:46.739483 [info ] [MainThread]: 
[0m12:04:46.740303 [error] [MainThread]: [31mFailure in model stg_customers (models\staging\stg_customers.sql)[0m
[0m12:04:46.741236 [error] [MainThread]:   Database Error in model stg_customers (models\staging\stg_customers.sql)
  syntax error at or near ";"
  LINE 18: );
            ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_customers.sql
[0m12:04:46.742152 [info ] [MainThread]: 
[0m12:04:46.743151 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_customers.sql
[0m12:04:46.743863 [info ] [MainThread]: 
[0m12:04:46.744615 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=3
[0m12:04:46.746125 [debug] [MainThread]: Command `dbt run` failed at 12:04:46.745966 after 2.84 seconds
[0m12:04:46.746632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002069BA61050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206975FB250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020699F30C90>]}
[0m12:04:46.747138 [debug] [MainThread]: Flushing usage events
[0m12:04:47.854238 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:05:46.996375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023854464980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023854F1CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002385555FD90>]}


============================== 12:05:47.002246 | 509df1a8-0c27-48bd-bdcc-070a93c6df95 ==============================
[0m12:05:47.002246 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:05:47.003566 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'use_colors': 'True', 'invocation_command': 'dbt run --select staging', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:05:47.262665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '509df1a8-0c27-48bd-bdcc-070a93c6df95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238548D0770>]}
[0m12:05:47.354826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '509df1a8-0c27-48bd-bdcc-070a93c6df95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023855406E00>]}
[0m12:05:47.356435 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:05:47.827440 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:05:48.036638 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m12:05:48.037734 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\staging\stg_shipments.sql
[0m12:05:48.038590 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\staging\stg_couriers.sql
[0m12:05:48.039122 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\staging\stg_customers.sql
[0m12:05:48.422892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '509df1a8-0c27-48bd-bdcc-070a93c6df95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023856E81450>]}
[0m12:05:48.543472 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:05:48.548448 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:05:48.653754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '509df1a8-0c27-48bd-bdcc-070a93c6df95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238553F3D40>]}
[0m12:05:48.654448 [info ] [MainThread]: Found 12 models, 5 data tests, 3 sources, 459 macros
[0m12:05:48.655294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '509df1a8-0c27-48bd-bdcc-070a93c6df95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023856E6EEB0>]}
[0m12:05:48.657795 [info ] [MainThread]: 
[0m12:05:48.658528 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:05:48.659231 [info ] [MainThread]: 
[0m12:05:48.660160 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:05:48.666115 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics'
[0m12:05:48.804361 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics"
[0m12:05:48.804921 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    select distinct nspname from pg_namespace
  
[0m12:05:48.805309 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:05:48.945338 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.140 seconds
[0m12:05:48.947239 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m12:05:48.950185 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics_analytics'
[0m12:05:48.959308 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:05:48.959946 [debug] [ThreadPool]: On list_delivery_analytics_analytics: BEGIN
[0m12:05:48.960478 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:05:49.030926 [debug] [ThreadPool]: SQL status: BEGIN in 0.070 seconds
[0m12:05:49.031469 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics_analytics"
[0m12:05:49.031866 [debug] [ThreadPool]: On list_delivery_analytics_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_analytics"} */
select
      'delivery_analytics' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m12:05:49.041371 [debug] [ThreadPool]: SQL status: SELECT 18 in 0.009 seconds
[0m12:05:49.044198 [debug] [ThreadPool]: On list_delivery_analytics_analytics: ROLLBACK
[0m12:05:49.044970 [debug] [ThreadPool]: On list_delivery_analytics_analytics: Close
[0m12:05:49.057572 [debug] [MainThread]: Using postgres connection "master"
[0m12:05:49.058319 [debug] [MainThread]: On master: BEGIN
[0m12:05:49.058878 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:05:49.121784 [debug] [MainThread]: SQL status: BEGIN in 0.063 seconds
[0m12:05:49.123471 [debug] [MainThread]: Using postgres connection "master"
[0m12:05:49.123939 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m12:05:49.134233 [debug] [MainThread]: SQL status: SELECT 5 in 0.010 seconds
[0m12:05:49.136221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '509df1a8-0c27-48bd-bdcc-070a93c6df95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238570E4120>]}
[0m12:05:49.136778 [debug] [MainThread]: On master: ROLLBACK
[0m12:05:49.137513 [debug] [MainThread]: Using postgres connection "master"
[0m12:05:49.137912 [debug] [MainThread]: On master: BEGIN
[0m12:05:49.138866 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m12:05:49.139291 [debug] [MainThread]: On master: COMMIT
[0m12:05:49.139675 [debug] [MainThread]: Using postgres connection "master"
[0m12:05:49.140050 [debug] [MainThread]: On master: COMMIT
[0m12:05:49.140691 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:05:49.141105 [debug] [MainThread]: On master: Close
[0m12:05:49.147411 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.stg_couriers
[0m12:05:49.148026 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.stg_customers
[0m12:05:49.148571 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.stg_shipments
[0m12:05:49.149289 [info ] [Thread-1 (]: 1 of 3 START sql view model analytics.stg_couriers ............................. [RUN]
[0m12:05:49.150028 [info ] [Thread-2 (]: 2 of 3 START sql view model analytics.stg_customers ............................ [RUN]
[0m12:05:49.152227 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_couriers'
[0m12:05:49.151059 [info ] [Thread-3 (]: 3 of 3 START sql view model analytics.stg_shipments ............................ [RUN]
[0m12:05:49.153071 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_customers'
[0m12:05:49.153666 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.stg_couriers
[0m12:05:49.154364 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_shipments'
[0m12:05:49.154980 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.stg_customers
[0m12:05:49.164810 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_couriers"
[0m12:05:49.165478 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.stg_shipments
[0m12:05:49.169012 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_customers"
[0m12:05:49.172660 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_shipments"
[0m12:05:49.175008 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.stg_customers
[0m12:05:49.175755 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.stg_couriers
[0m12:05:49.225995 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.stg_shipments
[0m12:05:49.234774 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_customers"
[0m12:05:49.237264 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_couriers"
[0m12:05:49.242226 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_shipments"
[0m12:05:49.244585 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m12:05:49.245155 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m12:05:49.245725 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: BEGIN
[0m12:05:49.246275 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: BEGIN
[0m12:05:49.246833 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:05:49.247423 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m12:05:49.247915 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:05:49.248722 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: BEGIN
[0m12:05:49.249510 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m12:05:49.327154 [debug] [Thread-2 (]: SQL status: BEGIN in 0.080 seconds
[0m12:05:49.327768 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m12:05:49.328221 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_customers"} */

  create view "delivery_analytics"."analytics"."stg_customers__dbt_tmp"
    
    
  as (
    with source as (
    select
        customer_id,
        name,
        email,
        phone,
        address,
        city
    from "delivery_analytics2"."raw"."customers"
)

select * from source
  );
[0m12:05:49.331261 [debug] [Thread-2 (]: Postgres adapter: Postgres error: cross-database references are not implemented: "delivery_analytics2.raw.customers"
LINE 15:     from "delivery_analytics2"."raw"."customers"
                  ^

[0m12:05:49.331833 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: ROLLBACK
[0m12:05:49.332662 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: Close
[0m12:05:49.345559 [debug] [Thread-2 (]: Database Error in model stg_customers (models\staging\stg_customers.sql)
  cross-database references are not implemented: "delivery_analytics2.raw.customers"
  LINE 15:     from "delivery_analytics2"."raw"."customers"
                    ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_customers.sql
[0m12:05:49.347778 [debug] [Thread-1 (]: SQL status: BEGIN in 0.100 seconds
[0m12:05:49.348474 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m12:05:49.349006 [debug] [Thread-3 (]: SQL status: BEGIN in 0.100 seconds
[0m12:05:49.349685 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_couriers"} */

  create view "delivery_analytics"."analytics"."stg_couriers__dbt_tmp"
    
    
  as (
    -- stg_couriers.sql
with source as (
    select
        courier_id,
        name,
        vehicle_type,
        city
    from "delivery_analytics2"."raw"."couriers"
)

select * from source
  );
[0m12:05:49.350339 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m12:05:49.351155 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipments"} */

  create view "delivery_analytics"."analytics"."stg_shipments__dbt_tmp"
    
    
  as (
    with source as (
    select
        shipment_id,
        sender_id,
        receiver_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        completed_at,
        price,
        delivery_city,
        delivery_type,
        distance_km
    from "delivery_analytics2"."raw"."shipments"
),

status_logic as (
    select
        *,
        case
            when shipment_date is null then 'pending'
            when shipment_date is not null and completed_at is null then 'picked_up'
            when completed_at is not null then 'delivered'
        end as current_status
    from source
)

select * from status_logic
  );
[0m12:05:49.351857 [debug] [Thread-1 (]: Postgres adapter: Postgres error: cross-database references are not implemented: "delivery_analytics2.raw.couriers"
LINE 14:     from "delivery_analytics2"."raw"."couriers"
                  ^

[0m12:05:49.352357 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: ROLLBACK
[0m12:05:49.352817 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '509df1a8-0c27-48bd-bdcc-070a93c6df95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002385747C350>]}
[0m12:05:49.353437 [debug] [Thread-3 (]: Postgres adapter: Postgres error: cross-database references are not implemented: "delivery_analytics2.raw.shipments"
LINE 20:     from "delivery_analytics2"."raw"."shipments"
                  ^

[0m12:05:49.354995 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: Close
[0m12:05:49.354364 [error] [Thread-2 (]: 2 of 3 ERROR creating sql view model analytics.stg_customers ................... [[31mERROR[0m in 0.19s]
[0m12:05:49.355942 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: ROLLBACK
[0m12:05:49.356861 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.stg_customers
[0m12:05:49.358735 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_customers' to be skipped because of status 'error'.  Reason: Database Error in model stg_customers (models\staging\stg_customers.sql)
  cross-database references are not implemented: "delivery_analytics2.raw.customers"
  LINE 15:     from "delivery_analytics2"."raw"."customers"
                    ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_customers.sql.
[0m12:05:49.359350 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: Close
[0m12:05:49.366565 [debug] [Thread-1 (]: Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  cross-database references are not implemented: "delivery_analytics2.raw.couriers"
  LINE 14:     from "delivery_analytics2"."raw"."couriers"
                    ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_couriers.sql
[0m12:05:49.367364 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '509df1a8-0c27-48bd-bdcc-070a93c6df95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002385734D860>]}
[0m12:05:49.368180 [error] [Thread-1 (]: 1 of 3 ERROR creating sql view model analytics.stg_couriers .................... [[31mERROR[0m in 0.22s]
[0m12:05:49.369673 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.stg_couriers
[0m12:05:49.373282 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_couriers' to be skipped because of status 'error'.  Reason: Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  cross-database references are not implemented: "delivery_analytics2.raw.couriers"
  LINE 14:     from "delivery_analytics2"."raw"."couriers"
                    ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_couriers.sql.
[0m12:05:49.374711 [debug] [Thread-3 (]: Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  cross-database references are not implemented: "delivery_analytics2.raw.shipments"
  LINE 20:     from "delivery_analytics2"."raw"."shipments"
                    ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipments.sql
[0m12:05:49.375370 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '509df1a8-0c27-48bd-bdcc-070a93c6df95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002385484CC30>]}
[0m12:05:49.376151 [error] [Thread-3 (]: 3 of 3 ERROR creating sql view model analytics.stg_shipments ................... [[31mERROR[0m in 0.22s]
[0m12:05:49.377329 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.stg_shipments
[0m12:05:49.378285 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.stg_shipments' to be skipped because of status 'error'.  Reason: Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  cross-database references are not implemented: "delivery_analytics2.raw.shipments"
  LINE 20:     from "delivery_analytics2"."raw"."shipments"
                    ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipments.sql.
[0m12:05:49.380628 [debug] [MainThread]: Using postgres connection "master"
[0m12:05:49.381200 [debug] [MainThread]: On master: BEGIN
[0m12:05:49.381588 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:05:49.447343 [debug] [MainThread]: SQL status: BEGIN in 0.066 seconds
[0m12:05:49.448004 [debug] [MainThread]: On master: COMMIT
[0m12:05:49.448474 [debug] [MainThread]: Using postgres connection "master"
[0m12:05:49.448928 [debug] [MainThread]: On master: COMMIT
[0m12:05:49.449633 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:05:49.450256 [debug] [MainThread]: On master: Close
[0m12:05:49.451046 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:05:49.451452 [debug] [MainThread]: Connection 'list_delivery_analytics' was properly closed.
[0m12:05:49.451836 [debug] [MainThread]: Connection 'list_delivery_analytics_analytics' was properly closed.
[0m12:05:49.452191 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_couriers' was properly closed.
[0m12:05:49.452524 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_customers' was properly closed.
[0m12:05:49.452930 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_shipments' was properly closed.
[0m12:05:49.453433 [info ] [MainThread]: 
[0m12:05:49.454245 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 0.79 seconds (0.79s).
[0m12:05:49.455988 [debug] [MainThread]: Command end result
[0m12:05:49.486373 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:05:49.490308 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:05:49.499920 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m12:05:49.500454 [info ] [MainThread]: 
[0m12:05:49.501201 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m12:05:49.501946 [info ] [MainThread]: 
[0m12:05:49.502885 [error] [MainThread]: [31mFailure in model stg_customers (models\staging\stg_customers.sql)[0m
[0m12:05:49.503737 [error] [MainThread]:   Database Error in model stg_customers (models\staging\stg_customers.sql)
  cross-database references are not implemented: "delivery_analytics2.raw.customers"
  LINE 15:     from "delivery_analytics2"."raw"."customers"
                    ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_customers.sql
[0m12:05:49.504475 [info ] [MainThread]: 
[0m12:05:49.505353 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_customers.sql
[0m12:05:49.506075 [info ] [MainThread]: 
[0m12:05:49.506826 [error] [MainThread]: [31mFailure in model stg_couriers (models\staging\stg_couriers.sql)[0m
[0m12:05:49.507763 [error] [MainThread]:   Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  cross-database references are not implemented: "delivery_analytics2.raw.couriers"
  LINE 14:     from "delivery_analytics2"."raw"."couriers"
                    ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_couriers.sql
[0m12:05:49.508606 [info ] [MainThread]: 
[0m12:05:49.509506 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_couriers.sql
[0m12:05:49.510328 [info ] [MainThread]: 
[0m12:05:49.511143 [error] [MainThread]: [31mFailure in model stg_shipments (models\staging\stg_shipments.sql)[0m
[0m12:05:49.512315 [error] [MainThread]:   Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  cross-database references are not implemented: "delivery_analytics2.raw.shipments"
  LINE 20:     from "delivery_analytics2"."raw"."shipments"
                    ^
  compiled code at target\run\dbt_delivery_analytics\models\staging\stg_shipments.sql
[0m12:05:49.513085 [info ] [MainThread]: 
[0m12:05:49.514055 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\staging\stg_shipments.sql
[0m12:05:49.514858 [info ] [MainThread]: 
[0m12:05:49.515668 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=3
[0m12:05:49.517259 [debug] [MainThread]: Command `dbt run` failed at 12:05:49.517097 after 2.72 seconds
[0m12:05:49.517768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238570F0F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023856B14550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238555E0C90>]}
[0m12:05:49.518255 [debug] [MainThread]: Flushing usage events
[0m12:05:50.635595 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:09:00.359241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A709208980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A709CBCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A70A2FFD90>]}


============================== 12:09:00.364883 | 454d402f-4bb1-4e7a-aa7e-8183a0b2905d ==============================
[0m12:09:00.364883 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:09:00.366304 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt run --select staging', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:09:00.632181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '454d402f-4bb1-4e7a-aa7e-8183a0b2905d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A709670770>]}
[0m12:09:00.713105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '454d402f-4bb1-4e7a-aa7e-8183a0b2905d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A70A1A6E00>]}
[0m12:09:00.714445 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:09:01.154708 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:09:01.314424 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m12:09:01.315282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '454d402f-4bb1-4e7a-aa7e-8183a0b2905d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A70BC8CA50>]}
[0m12:09:02.660912 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'shipments_by_hour_day' in the 'models' section of file 'models\schema.yml'
[0m12:09:02.664171 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'shipments_by_city_vehicle' in the 'models' section of file 'models\schema.yml'
[0m12:09:02.947807 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_delivery_analytics.not_null_shipments_by_hour_day_day_of_week.e2d126a571' (models\schema.yml) depends on a node named 'shipments_by_hour_day' in package '' which was not found
[0m12:09:02.949034 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_delivery_analytics.not_null_shipments_by_hour_day_shipment_hour.6ee57859e9' (models\schema.yml) depends on a node named 'shipments_by_hour_day' in package '' which was not found
[0m12:09:02.949857 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_delivery_analytics.not_null_shipments_by_city_vehicle_city.1d36860158' (models\schema.yml) depends on a node named 'shipments_by_city_vehicle' in package '' which was not found
[0m12:09:02.950627 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_delivery_analytics.not_null_shipments_by_city_vehicle_vehicle_type.51a805e811' (models\schema.yml) depends on a node named 'shipments_by_city_vehicle' in package '' which was not found
[0m12:09:03.085703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '454d402f-4bb1-4e7a-aa7e-8183a0b2905d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A70BB45040>]}
[0m12:09:03.188883 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:09:03.193167 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:09:03.222095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '454d402f-4bb1-4e7a-aa7e-8183a0b2905d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A70BDC0E50>]}
[0m12:09:03.222798 [info ] [MainThread]: Found 12 models, 5 data tests, 3 sources, 459 macros
[0m12:09:03.223607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '454d402f-4bb1-4e7a-aa7e-8183a0b2905d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A70BBDF380>]}
[0m12:09:03.226249 [info ] [MainThread]: 
[0m12:09:03.227068 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:09:03.227856 [info ] [MainThread]: 
[0m12:09:03.228932 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:09:03.235080 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2'
[0m12:09:03.375303 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2"
[0m12:09:03.375913 [debug] [ThreadPool]: On list_delivery_analytics2: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2"} */

    select distinct nspname from pg_namespace
  
[0m12:09:03.376298 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:09:03.487520 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.111 seconds
[0m12:09:03.490138 [debug] [ThreadPool]: On list_delivery_analytics2: Close
[0m12:09:03.491244 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_delivery_analytics2, now create_delivery_analytics2_analytics)
[0m12:09:03.491965 [debug] [ThreadPool]: Creating schema "database: "delivery_analytics2"
schema: "analytics"
"
[0m12:09:03.499203 [debug] [ThreadPool]: Using postgres connection "create_delivery_analytics2_analytics"
[0m12:09:03.499750 [debug] [ThreadPool]: On create_delivery_analytics2_analytics: BEGIN
[0m12:09:03.500285 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:09:03.562234 [debug] [ThreadPool]: SQL status: BEGIN in 0.062 seconds
[0m12:09:03.562833 [debug] [ThreadPool]: Using postgres connection "create_delivery_analytics2_analytics"
[0m12:09:03.563204 [debug] [ThreadPool]: On create_delivery_analytics2_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "create_delivery_analytics2_analytics"} */
create schema if not exists "analytics"
[0m12:09:03.573829 [debug] [ThreadPool]: SQL status: CREATE SCHEMA in 0.010 seconds
[0m12:09:03.575770 [debug] [ThreadPool]: On create_delivery_analytics2_analytics: COMMIT
[0m12:09:03.576189 [debug] [ThreadPool]: Using postgres connection "create_delivery_analytics2_analytics"
[0m12:09:03.576548 [debug] [ThreadPool]: On create_delivery_analytics2_analytics: COMMIT
[0m12:09:03.578612 [debug] [ThreadPool]: SQL status: COMMIT in 0.002 seconds
[0m12:09:03.579323 [debug] [ThreadPool]: On create_delivery_analytics2_analytics: Close
[0m12:09:03.582768 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2_analytics'
[0m12:09:03.591248 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m12:09:03.591799 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: BEGIN
[0m12:09:03.592218 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:09:03.654125 [debug] [ThreadPool]: SQL status: BEGIN in 0.062 seconds
[0m12:09:03.654810 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m12:09:03.655506 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2_analytics"} */
select
      'delivery_analytics2' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m12:09:03.665977 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.010 seconds
[0m12:09:03.667649 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: ROLLBACK
[0m12:09:03.668371 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: Close
[0m12:09:03.675890 [debug] [MainThread]: Using postgres connection "master"
[0m12:09:03.676406 [debug] [MainThread]: On master: BEGIN
[0m12:09:03.676783 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:09:03.738158 [debug] [MainThread]: SQL status: BEGIN in 0.061 seconds
[0m12:09:03.738799 [debug] [MainThread]: Using postgres connection "master"
[0m12:09:03.739543 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m12:09:03.753559 [debug] [MainThread]: SQL status: SELECT 0 in 0.013 seconds
[0m12:09:03.755571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '454d402f-4bb1-4e7a-aa7e-8183a0b2905d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A70BE2BDD0>]}
[0m12:09:03.756118 [debug] [MainThread]: On master: ROLLBACK
[0m12:09:03.756848 [debug] [MainThread]: Using postgres connection "master"
[0m12:09:03.757252 [debug] [MainThread]: On master: BEGIN
[0m12:09:03.758507 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m12:09:03.759023 [debug] [MainThread]: On master: COMMIT
[0m12:09:03.759418 [debug] [MainThread]: Using postgres connection "master"
[0m12:09:03.759788 [debug] [MainThread]: On master: COMMIT
[0m12:09:03.760435 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:09:03.760848 [debug] [MainThread]: On master: Close
[0m12:09:03.766677 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.stg_customers
[0m12:09:03.767288 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.stg_couriers
[0m12:09:03.767869 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.stg_shipments
[0m12:09:03.768564 [info ] [Thread-2 (]: 2 of 3 START sql view model analytics.stg_customers ............................ [RUN]
[0m12:09:03.769300 [info ] [Thread-1 (]: 1 of 3 START sql view model analytics.stg_couriers ............................. [RUN]
[0m12:09:03.771276 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_customers'
[0m12:09:03.770303 [info ] [Thread-3 (]: 3 of 3 START sql view model analytics.stg_shipments ............................ [RUN]
[0m12:09:03.772118 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_couriers'
[0m12:09:03.772702 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.stg_customers
[0m12:09:03.773532 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_shipments'
[0m12:09:03.774145 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.stg_couriers
[0m12:09:03.783887 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_customers"
[0m12:09:03.784630 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.stg_shipments
[0m12:09:03.788390 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_couriers"
[0m12:09:03.793001 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_shipments"
[0m12:09:03.795142 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.stg_customers
[0m12:09:03.827321 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.stg_couriers
[0m12:09:03.850772 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_couriers"
[0m12:09:03.854229 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_customers"
[0m12:09:03.854915 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.stg_shipments
[0m12:09:03.860504 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_shipments"
[0m12:09:03.863790 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m12:09:03.864617 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m12:09:03.865310 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: BEGIN
[0m12:09:03.866097 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: BEGIN
[0m12:09:03.866877 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m12:09:03.867471 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:09:03.868119 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:09:03.868722 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: BEGIN
[0m12:09:03.869787 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m12:09:03.942491 [debug] [Thread-1 (]: SQL status: BEGIN in 0.075 seconds
[0m12:09:03.943098 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m12:09:03.943560 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_couriers"} */

  create view "delivery_analytics2"."analytics"."stg_couriers__dbt_tmp"
    
    
  as (
    -- stg_couriers.sql
with source as (
    select
        courier_id,
        name,
        vehicle_type,
        city
    from "delivery_analytics2"."raw"."couriers"
)

select * from source
  );
[0m12:09:03.960485 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.016 seconds
[0m12:09:03.968508 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m12:09:03.969852 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_couriers"} */
alter table "delivery_analytics2"."analytics"."stg_couriers__dbt_tmp" rename to "stg_couriers"
[0m12:09:03.969094 [debug] [Thread-2 (]: SQL status: BEGIN in 0.101 seconds
[0m12:09:03.971030 [debug] [Thread-3 (]: SQL status: BEGIN in 0.101 seconds
[0m12:09:03.971577 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m12:09:03.972162 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m12:09:03.972818 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m12:09:03.973429 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_customers"} */

  create view "delivery_analytics2"."analytics"."stg_customers__dbt_tmp"
    
    
  as (
    with source as (
    select
        customer_id,
        name,
        email,
        phone,
        address,
        city
    from "delivery_analytics2"."raw"."customers"
)

select * from source
  );
[0m12:09:03.974129 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipments"} */

  create view "delivery_analytics2"."analytics"."stg_shipments__dbt_tmp"
    
    
  as (
    with source as (
    select
        shipment_id,
        sender_id,
        receiver_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        completed_at,
        price,
        delivery_city,
        delivery_type,
        distance_km
    from "delivery_analytics2"."raw"."shipments"
),

status_logic as (
    select
        *,
        case
            when shipment_date is null then 'pending'
            when shipment_date is not null and completed_at is null then 'picked_up'
            when completed_at is not null then 'delivered'
        end as current_status
    from source
)

select * from status_logic
  );
[0m12:09:03.993623 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: COMMIT
[0m12:09:03.994239 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.008 seconds
[0m12:09:03.994862 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m12:09:03.998422 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m12:09:03.999108 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: COMMIT
[0m12:09:03.999756 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_customers"} */
alter table "delivery_analytics2"."analytics"."stg_customers__dbt_tmp" rename to "stg_customers"
[0m12:09:04.000972 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:09:04.001463 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m12:09:04.003800 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: COMMIT
[0m12:09:04.011885 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."stg_couriers__dbt_backup"
[0m12:09:04.013137 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.019 seconds
[0m12:09:04.013709 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m12:09:04.020652 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m12:09:04.024336 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m12:09:04.025068 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: COMMIT
[0m12:09:04.025907 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_couriers"} */
drop view if exists "delivery_analytics2"."analytics"."stg_couriers__dbt_backup" cascade
[0m12:09:04.026680 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipments"} */
alter table "delivery_analytics2"."analytics"."stg_shipments__dbt_tmp" rename to "stg_shipments"
[0m12:09:04.028480 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:09:04.029006 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:09:04.029525 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m12:09:04.031240 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: COMMIT
[0m12:09:04.034639 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: Close
[0m12:09:04.038146 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics2"."analytics"."stg_customers__dbt_backup"
[0m12:09:04.038788 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m12:09:04.039908 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m12:09:04.042465 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: COMMIT
[0m12:09:04.043124 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_customers"} */
drop view if exists "delivery_analytics2"."analytics"."stg_customers__dbt_backup" cascade
[0m12:09:04.044139 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '454d402f-4bb1-4e7a-aa7e-8183a0b2905d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A70C261860>]}
[0m12:09:04.044704 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:09:04.046050 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m12:09:04.045595 [info ] [Thread-1 (]: 1 of 3 OK created sql view model analytics.stg_couriers ........................ [[32mCREATE VIEW[0m in 0.27s]
[0m12:09:04.048181 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: Close
[0m12:09:04.051602 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics2"."analytics"."stg_shipments__dbt_backup"
[0m12:09:04.052555 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.stg_couriers
[0m12:09:04.053689 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m12:09:04.054551 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '454d402f-4bb1-4e7a-aa7e-8183a0b2905d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A70C469D10>]}
[0m12:09:04.055413 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipments"} */
drop view if exists "delivery_analytics2"."analytics"."stg_shipments__dbt_backup" cascade
[0m12:09:04.056464 [info ] [Thread-2 (]: 2 of 3 OK created sql view model analytics.stg_customers ....................... [[32mCREATE VIEW[0m in 0.28s]
[0m12:09:04.057755 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.stg_customers
[0m12:09:04.058553 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.001 seconds
[0m12:09:04.060719 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: Close
[0m12:09:04.061698 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '454d402f-4bb1-4e7a-aa7e-8183a0b2905d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A70C312950>]}
[0m12:09:04.062705 [info ] [Thread-3 (]: 3 of 3 OK created sql view model analytics.stg_shipments ....................... [[32mCREATE VIEW[0m in 0.29s]
[0m12:09:04.064057 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.stg_shipments
[0m12:09:04.066620 [debug] [MainThread]: Using postgres connection "master"
[0m12:09:04.067291 [debug] [MainThread]: On master: BEGIN
[0m12:09:04.067831 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:09:04.135834 [debug] [MainThread]: SQL status: BEGIN in 0.068 seconds
[0m12:09:04.136533 [debug] [MainThread]: On master: COMMIT
[0m12:09:04.137007 [debug] [MainThread]: Using postgres connection "master"
[0m12:09:04.137495 [debug] [MainThread]: On master: COMMIT
[0m12:09:04.138190 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:09:04.138691 [debug] [MainThread]: On master: Close
[0m12:09:04.139320 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:09:04.139703 [debug] [MainThread]: Connection 'create_delivery_analytics2_analytics' was properly closed.
[0m12:09:04.140075 [debug] [MainThread]: Connection 'list_delivery_analytics2_analytics' was properly closed.
[0m12:09:04.140655 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_customers' was properly closed.
[0m12:09:04.141261 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_couriers' was properly closed.
[0m12:09:04.141912 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.stg_shipments' was properly closed.
[0m12:09:04.142659 [info ] [MainThread]: 
[0m12:09:04.143517 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 0.91 seconds (0.91s).
[0m12:09:04.145219 [debug] [MainThread]: Command end result
[0m12:09:04.178162 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:09:04.182449 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:09:04.191254 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m12:09:04.191956 [info ] [MainThread]: 
[0m12:09:04.192878 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:09:04.193662 [info ] [MainThread]: 
[0m12:09:04.194373 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m12:09:04.195803 [debug] [MainThread]: Command `dbt run` succeeded at 12:09:04.195647 after 4.05 seconds
[0m12:09:04.196361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A70A37CC90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A703BF1780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A70BD267B0>]}
[0m12:09:04.196848 [debug] [MainThread]: Flushing usage events
[0m12:09:05.259699 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:19:09.250568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0EFF3C980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0F09ECB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0F102FD90>]}


============================== 12:19:09.256097 | 415d5c3f-4e55-43df-8d71-c63356b4b322 ==============================
[0m12:19:09.256097 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:19:09.257686 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run --select fct_shipments', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:19:09.585340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '415d5c3f-4e55-43df-8d71-c63356b4b322', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0F03A0770>]}
[0m12:19:09.676938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '415d5c3f-4e55-43df-8d71-c63356b4b322', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0F0ED6E00>]}
[0m12:19:09.678856 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:19:10.154033 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:19:10.495330 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m12:19:10.496390 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\facts\fct_courier_load.sql
[0m12:19:10.497089 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\facts\fct_courier_performance.sql
[0m12:19:10.899666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '415d5c3f-4e55-43df-8d71-c63356b4b322', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0F291D550>]}
[0m12:19:11.052530 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:19:11.058814 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:19:11.196124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '415d5c3f-4e55-43df-8d71-c63356b4b322', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0F0EC3D40>]}
[0m12:19:11.211668 [info ] [MainThread]: Found 12 models, 5 data tests, 3 sources, 459 macros
[0m12:19:11.214309 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '415d5c3f-4e55-43df-8d71-c63356b4b322', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0F292EC10>]}
[0m12:19:11.220339 [info ] [MainThread]: 
[0m12:19:11.222891 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:19:11.227554 [info ] [MainThread]: 
[0m12:19:11.229328 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:19:11.233164 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2'
[0m12:19:11.395634 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2"
[0m12:19:11.396174 [debug] [ThreadPool]: On list_delivery_analytics2: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2"} */

    select distinct nspname from pg_namespace
  
[0m12:19:11.396580 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:19:11.485125 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.088 seconds
[0m12:19:11.486870 [debug] [ThreadPool]: On list_delivery_analytics2: Close
[0m12:19:11.495237 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2_analytics'
[0m12:19:11.502905 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m12:19:11.503700 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: BEGIN
[0m12:19:11.504196 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:19:11.567460 [debug] [ThreadPool]: SQL status: BEGIN in 0.063 seconds
[0m12:19:11.568100 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m12:19:11.568779 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2_analytics"} */
select
      'delivery_analytics2' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m12:19:11.577604 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.008 seconds
[0m12:19:11.579273 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: ROLLBACK
[0m12:19:11.579904 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: Close
[0m12:19:11.588122 [debug] [MainThread]: Using postgres connection "master"
[0m12:19:11.588712 [debug] [MainThread]: On master: BEGIN
[0m12:19:11.589349 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:19:11.652441 [debug] [MainThread]: SQL status: BEGIN in 0.063 seconds
[0m12:19:11.653072 [debug] [MainThread]: Using postgres connection "master"
[0m12:19:11.653814 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m12:19:11.665996 [debug] [MainThread]: SQL status: SELECT 3 in 0.011 seconds
[0m12:19:11.667579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '415d5c3f-4e55-43df-8d71-c63356b4b322', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0F2C436C0>]}
[0m12:19:11.668144 [debug] [MainThread]: On master: ROLLBACK
[0m12:19:11.668795 [debug] [MainThread]: Using postgres connection "master"
[0m12:19:11.669185 [debug] [MainThread]: On master: BEGIN
[0m12:19:11.670029 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m12:19:11.670533 [debug] [MainThread]: On master: COMMIT
[0m12:19:11.671014 [debug] [MainThread]: Using postgres connection "master"
[0m12:19:11.671537 [debug] [MainThread]: On master: COMMIT
[0m12:19:11.672557 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:19:11.672965 [debug] [MainThread]: On master: Close
[0m12:19:11.680139 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m12:19:11.681140 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics.fct_shipments ........................... [RUN]
[0m12:19:11.682477 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m12:19:11.683148 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m12:19:11.692750 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m12:19:11.695711 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m12:19:11.753384 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m12:19:11.757052 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:19:11.757647 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m12:19:11.758093 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:19:11.827484 [debug] [Thread-1 (]: SQL status: BEGIN in 0.069 seconds
[0m12:19:11.828092 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:19:11.828686 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics2"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (

    select *
    from "delivery_analytics2"."analytics"."stg_shipments"

),

customers as (

    select *
    from "delivery_analytics2"."analytics"."stg_customers"

),

couriers as (

    select *
    from "delivery_analytics2"."analytics"."stg_couriers"

)

select
    s.shipment_id,

    -- keys
    s.sender_id,
    s.receiver_id,
    s.courier_id,

    -- dates
    s.shipment_date,
    s.expected_delivery_date,
    s.completed_at,

    -- metrics
    s.price,
    s.distance_km,

    -- attributes
    s.delivery_city,
    s.delivery_type,
    s.current_status,

    -- derived metrics
    case
        when s.completed_at is not null
        then (s.completed_at::date - s.shipment_date)
    end as delivery_duration_days,

    case
        when s.completed_at > s.expected_delivery_date then true
        else false
    end as is_late_delivery

from shipments s




/*
    FACT TABLE: fct_shipments

    Svaki red u ovoj tabeli predstavlja jednu pošiljku (shipment) iz naše baze.

    Svrha:
    - Centralizovati sve metrike vezane za pošiljke na jednom mjestu
    - Omogućiti analitiku po vremenu, kuriru, tipu pošiljke i gradu
    - Poslužiti kao ulaz za BI dashboard ili dalju analitiku u marts layer-u

    Šta se ovdje računa:
    1. Ključne kolone:
        - shipment_id
        - sender_id, receiver_id
        - courier_id
    2. Datumi:
        - shipment_date (kada je pošiljka poslana)
        - expected_delivery_date (planirani datum isporuke)
        - completed_at (kada je pošiljka zaista dostavljena)
    3. Metričke kolone:
        - price
        - distance_km
    4. Atributi:
        - delivery_city
        - delivery_type (standard / express)
        - current_status (pending, picked_up, delivered)
    5. Derived metrics (izvedene kolone):
        - delivery_duration_days = koliko je dana trajala pošiljka
        - is_late_delivery = boolean, true ako je pošiljka dostavljena nakon expected_delivery_date

    Napomena:
    - Ova tabela služi kao "fact" jer sadrži sve mjere koje se mogu agregirati
    - Za dimenzije ćemo imati posebne dim_tables: dim_customers, dim_couriers, dim_cities
    - Može se koristiti za: analizu performansi kurira, kašnjenja po gradu, tip pošiljke, revenue po gradu itd.

*/
  );
  
[0m12:19:11.854200 [debug] [Thread-1 (]: SQL status: SELECT 50 in 0.025 seconds
[0m12:19:11.872812 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:19:11.873700 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics2"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m12:19:11.875758 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:19:11.903737 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m12:19:11.904374 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:19:11.905488 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m12:19:11.908048 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m12:19:11.916834 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."fct_shipments__dbt_backup"
[0m12:19:11.924348 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m12:19:11.924886 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics2"."analytics"."fct_shipments__dbt_backup" cascade
[0m12:19:11.926148 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m12:19:11.930396 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m12:19:11.934299 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '415d5c3f-4e55-43df-8d71-c63356b4b322', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0F2EDC1D0>]}
[0m12:19:11.935550 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 50[0m in 0.25s]
[0m12:19:11.936834 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m12:19:11.939087 [debug] [MainThread]: Using postgres connection "master"
[0m12:19:11.939570 [debug] [MainThread]: On master: BEGIN
[0m12:19:11.939993 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:19:12.005467 [debug] [MainThread]: SQL status: BEGIN in 0.065 seconds
[0m12:19:12.006077 [debug] [MainThread]: On master: COMMIT
[0m12:19:12.006781 [debug] [MainThread]: Using postgres connection "master"
[0m12:19:12.007480 [debug] [MainThread]: On master: COMMIT
[0m12:19:12.008119 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:19:12.008548 [debug] [MainThread]: On master: Close
[0m12:19:12.009303 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:19:12.009861 [debug] [MainThread]: Connection 'list_delivery_analytics2' was properly closed.
[0m12:19:12.010537 [debug] [MainThread]: Connection 'list_delivery_analytics2_analytics' was properly closed.
[0m12:19:12.011139 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments' was properly closed.
[0m12:19:12.011748 [info ] [MainThread]: 
[0m12:19:12.012775 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.78 seconds (0.78s).
[0m12:19:12.014300 [debug] [MainThread]: Command end result
[0m12:19:12.053440 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:19:12.059746 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:19:12.070105 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m12:19:12.070631 [info ] [MainThread]: 
[0m12:19:12.071553 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:19:12.072313 [info ] [MainThread]: 
[0m12:19:12.073134 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m12:19:12.074669 [debug] [MainThread]: Command `dbt run` succeeded at 12:19:12.074501 after 3.16 seconds
[0m12:19:12.075167 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0F28B1FF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0F27C2A30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0F2578550>]}
[0m12:19:12.075642 [debug] [MainThread]: Flushing usage events
[0m12:19:13.197859 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:19:18.474688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186C507C980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186C5B30B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186C616FD90>]}


============================== 12:19:18.479246 | 86488c48-c69a-4509-bd31-64236b6a93fc ==============================
[0m12:19:18.479246 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:19:18.480364 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select fct_courier_performance fct_courier_load', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:19:18.667113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '86488c48-c69a-4509-bd31-64236b6a93fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186C54E0770>]}
[0m12:19:18.729423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '86488c48-c69a-4509-bd31-64236b6a93fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186C6016E00>]}
[0m12:19:18.730460 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:19:19.087833 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:19:19.272916 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:19:19.273416 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:19:19.340445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '86488c48-c69a-4509-bd31-64236b6a93fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186C7AFCF50>]}
[0m12:19:19.455282 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:19:19.459950 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:19:19.493045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '86488c48-c69a-4509-bd31-64236b6a93fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186C6003D40>]}
[0m12:19:19.493632 [info ] [MainThread]: Found 12 models, 5 data tests, 3 sources, 459 macros
[0m12:19:19.494247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '86488c48-c69a-4509-bd31-64236b6a93fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186C72CCF30>]}
[0m12:19:19.496917 [info ] [MainThread]: 
[0m12:19:19.497602 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:19:19.498340 [info ] [MainThread]: 
[0m12:19:19.499097 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:19:19.504769 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2'
[0m12:19:19.614443 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2"
[0m12:19:19.614915 [debug] [ThreadPool]: On list_delivery_analytics2: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2"} */

    select distinct nspname from pg_namespace
  
[0m12:19:19.615225 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:19:19.753816 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.138 seconds
[0m12:19:19.755223 [debug] [ThreadPool]: On list_delivery_analytics2: Close
[0m12:19:19.758023 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2_analytics'
[0m12:19:19.764979 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m12:19:19.765394 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: BEGIN
[0m12:19:19.765673 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:19:19.827125 [debug] [ThreadPool]: SQL status: BEGIN in 0.061 seconds
[0m12:19:19.827634 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m12:19:19.828003 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2_analytics"} */
select
      'delivery_analytics2' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m12:19:19.837973 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.010 seconds
[0m12:19:19.839330 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: ROLLBACK
[0m12:19:19.839890 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: Close
[0m12:19:19.846142 [debug] [MainThread]: Using postgres connection "master"
[0m12:19:19.846543 [debug] [MainThread]: On master: BEGIN
[0m12:19:19.846827 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:19:19.906343 [debug] [MainThread]: SQL status: BEGIN in 0.059 seconds
[0m12:19:19.906873 [debug] [MainThread]: Using postgres connection "master"
[0m12:19:19.907281 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m12:19:19.918825 [debug] [MainThread]: SQL status: SELECT 3 in 0.011 seconds
[0m12:19:19.920327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '86488c48-c69a-4509-bd31-64236b6a93fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186C7EC8870>]}
[0m12:19:19.920849 [debug] [MainThread]: On master: ROLLBACK
[0m12:19:19.921373 [debug] [MainThread]: Using postgres connection "master"
[0m12:19:19.921673 [debug] [MainThread]: On master: BEGIN
[0m12:19:19.922363 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m12:19:19.922743 [debug] [MainThread]: On master: COMMIT
[0m12:19:19.923168 [debug] [MainThread]: Using postgres connection "master"
[0m12:19:19.923630 [debug] [MainThread]: On master: COMMIT
[0m12:19:19.924282 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:19:19.924603 [debug] [MainThread]: On master: Close
[0m12:19:19.929987 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_courier_load
[0m12:19:19.930539 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_courier_performance
[0m12:19:19.931114 [info ] [Thread-1 (]: 1 of 2 START sql table model analytics.fct_courier_load ........................ [RUN]
[0m12:19:19.932296 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_courier_load'
[0m12:19:19.931628 [info ] [Thread-2 (]: 2 of 2 START sql table model analytics.fct_courier_performance ................. [RUN]
[0m12:19:19.932863 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_courier_load
[0m12:19:19.933507 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_courier_performance'
[0m12:19:19.941026 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_courier_load"
[0m12:19:19.941656 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_courier_performance
[0m12:19:19.944231 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_courier_performance"
[0m12:19:19.946662 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_courier_load
[0m12:19:19.947177 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_courier_performance
[0m12:19:19.995789 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_courier_load"
[0m12:19:19.996604 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_courier_performance"
[0m12:19:19.998823 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_load"
[0m12:19:19.999395 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_courier_load: BEGIN
[0m12:19:20.000043 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:19:20.000654 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_performance"
[0m12:19:20.001345 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_courier_performance: BEGIN
[0m12:19:20.001899 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m12:19:20.064230 [debug] [Thread-1 (]: SQL status: BEGIN in 0.064 seconds
[0m12:19:20.064980 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_load"
[0m12:19:20.065665 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_courier_load"} */

  
    

  create  table "delivery_analytics2"."analytics"."fct_courier_load__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
    where shipment_date is not null
)

select
    courier_id,
    shipment_date,
    count(*) as shipments_per_day,
    sum(distance_km) as total_distance_per_day,
    sum(price) as revenue_per_day

from shipments
group by courier_id, shipment_date
order by courier_id, shipment_date



/*

Daily load po kuriru


*/
  );
  
[0m12:19:20.069342 [debug] [Thread-2 (]: SQL status: BEGIN in 0.067 seconds
[0m12:19:20.069763 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_performance"
[0m12:19:20.070144 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_courier_performance"} */

  
    

  create  table "delivery_analytics2"."analytics"."fct_courier_performance__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    courier_id,

    count(*) as total_shipments,
    count(case when current_status = 'delivered' then 1 end) as delivered_shipments,
    avg(delivery_duration_days) as avg_delivery_duration_days,
    sum(price) as total_revenue,
    sum(case when is_late_delivery then 1 else 0 end)::float / nullif(count(*),0) as pct_late_deliveries

from shipments
group by courier_id
  );
  
[0m12:19:20.080228 [debug] [Thread-1 (]: SQL status: SELECT 50 in 0.014 seconds
[0m12:19:20.092270 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_load"
[0m12:19:20.092939 [debug] [Thread-2 (]: SQL status: SELECT 10 in 0.022 seconds
[0m12:19:20.093496 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_courier_load"} */
alter table "delivery_analytics2"."analytics"."fct_courier_load__dbt_tmp" rename to "fct_courier_load"
[0m12:19:20.099930 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_performance"
[0m12:19:20.100638 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_courier_performance"} */
alter table "delivery_analytics2"."analytics"."fct_courier_performance__dbt_tmp" rename to "fct_courier_performance"
[0m12:19:20.101662 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:19:20.102185 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:19:20.117054 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_courier_load: COMMIT
[0m12:19:20.118856 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_courier_performance: COMMIT
[0m12:19:20.119391 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_load"
[0m12:19:20.119819 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_performance"
[0m12:19:20.120259 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_courier_load: COMMIT
[0m12:19:20.120722 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_courier_performance: COMMIT
[0m12:19:20.122647 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m12:19:20.123111 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m12:19:20.129336 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."fct_courier_load__dbt_backup"
[0m12:19:20.201132 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics2"."analytics"."fct_courier_performance__dbt_backup"
[0m12:19:20.206495 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_load"
[0m12:19:20.207362 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_performance"
[0m12:19:20.207825 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_courier_load"} */
drop table if exists "delivery_analytics2"."analytics"."fct_courier_load__dbt_backup" cascade
[0m12:19:20.208243 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_courier_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_courier_performance"} */
drop table if exists "delivery_analytics2"."analytics"."fct_courier_performance__dbt_backup" cascade
[0m12:19:20.209224 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.000 seconds
[0m12:19:20.209650 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m12:19:20.211938 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_courier_performance: Close
[0m12:19:20.213525 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_courier_load: Close
[0m12:19:20.216167 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86488c48-c69a-4509-bd31-64236b6a93fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186C7BB77D0>]}
[0m12:19:20.216793 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86488c48-c69a-4509-bd31-64236b6a93fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186C7BF6DB0>]}
[0m12:19:20.217689 [info ] [Thread-2 (]: 2 of 2 OK created sql table model analytics.fct_courier_performance ............ [[32mSELECT 10[0m in 0.28s]
[0m12:19:20.219442 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_courier_performance
[0m12:19:20.218495 [info ] [Thread-1 (]: 1 of 2 OK created sql table model analytics.fct_courier_load ................... [[32mSELECT 50[0m in 0.28s]
[0m12:19:20.220528 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_courier_load
[0m12:19:20.222095 [debug] [MainThread]: Using postgres connection "master"
[0m12:19:20.222452 [debug] [MainThread]: On master: BEGIN
[0m12:19:20.222815 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:19:20.289913 [debug] [MainThread]: SQL status: BEGIN in 0.067 seconds
[0m12:19:20.290371 [debug] [MainThread]: On master: COMMIT
[0m12:19:20.290679 [debug] [MainThread]: Using postgres connection "master"
[0m12:19:20.290950 [debug] [MainThread]: On master: COMMIT
[0m12:19:20.291395 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:19:20.291709 [debug] [MainThread]: On master: Close
[0m12:19:20.292201 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:19:20.292469 [debug] [MainThread]: Connection 'list_delivery_analytics2' was properly closed.
[0m12:19:20.292729 [debug] [MainThread]: Connection 'list_delivery_analytics2_analytics' was properly closed.
[0m12:19:20.292982 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_courier_load' was properly closed.
[0m12:19:20.293412 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_courier_performance' was properly closed.
[0m12:19:20.293858 [info ] [MainThread]: 
[0m12:19:20.294446 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 0.79 seconds (0.79s).
[0m12:19:20.295833 [debug] [MainThread]: Command end result
[0m12:19:20.326902 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:19:20.331117 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:19:20.341417 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m12:19:20.341873 [info ] [MainThread]: 
[0m12:19:20.342445 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:19:20.342931 [info ] [MainThread]: 
[0m12:19:20.343441 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=2
[0m12:19:20.344605 [debug] [MainThread]: Command `dbt run` succeeded at 12:19:20.344481 after 2.02 seconds
[0m12:19:20.344979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186C7ED6D50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186C7BD0680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186C7863150>]}
[0m12:19:20.345920 [debug] [MainThread]: Flushing usage events
[0m12:19:21.260430 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:26:56.654190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E03624C980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E036CFCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E03733FD90>]}


============================== 12:26:56.659710 | 55a4129e-1997-46c1-9a76-ddc033b4efb6 ==============================
[0m12:26:56.659710 [info ] [MainThread]: Running with dbt=1.10.15
[0m12:26:56.660890 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run --select fct_shipments_by_time', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m12:26:56.916368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '55a4129e-1997-46c1-9a76-ddc033b4efb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0366B0770>]}
[0m12:26:57.001313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '55a4129e-1997-46c1-9a76-ddc033b4efb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0371E6E00>]}
[0m12:26:57.002844 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m12:26:57.424052 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m12:26:57.626544 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 2 files added, 4 files changed.
[0m12:26:57.627531 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\facts\fct_shipments_by_time.sql
[0m12:26:57.628166 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\dimensions\dim_cities.sql
[0m12:26:57.628694 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\dimensions\dim_city.sql
[0m12:26:57.629361 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\intermediate\int_shipment_delays.sql
[0m12:26:57.630039 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\dimensions\dim_couriers.sql
[0m12:26:57.630675 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\intermediate\int_shipment_status.sql
[0m12:26:57.631319 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\intermediate\int_active_delays.sql
[0m12:26:58.044410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '55a4129e-1997-46c1-9a76-ddc033b4efb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E038C5D350>]}
[0m12:26:58.173157 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:26:58.178095 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:26:58.283747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '55a4129e-1997-46c1-9a76-ddc033b4efb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0371D3D40>]}
[0m12:26:58.284504 [info ] [MainThread]: Found 13 models, 5 data tests, 3 sources, 459 macros
[0m12:26:58.285302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '55a4129e-1997-46c1-9a76-ddc033b4efb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E038C6D630>]}
[0m12:26:58.288179 [info ] [MainThread]: 
[0m12:26:58.289108 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m12:26:58.289798 [info ] [MainThread]: 
[0m12:26:58.290992 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m12:26:58.292343 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2'
[0m12:26:58.434010 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2"
[0m12:26:58.434558 [debug] [ThreadPool]: On list_delivery_analytics2: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2"} */

    select distinct nspname from pg_namespace
  
[0m12:26:58.434958 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:26:58.511856 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.077 seconds
[0m12:26:58.513581 [debug] [ThreadPool]: On list_delivery_analytics2: Close
[0m12:26:58.521619 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2_analytics'
[0m12:26:58.528648 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m12:26:58.529096 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: BEGIN
[0m12:26:58.529473 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:26:58.592013 [debug] [ThreadPool]: SQL status: BEGIN in 0.062 seconds
[0m12:26:58.592585 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m12:26:58.592985 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2_analytics"} */
select
      'delivery_analytics2' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m12:26:58.601485 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.008 seconds
[0m12:26:58.604055 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: ROLLBACK
[0m12:26:58.604769 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: Close
[0m12:26:58.613589 [debug] [MainThread]: Using postgres connection "master"
[0m12:26:58.614110 [debug] [MainThread]: On master: BEGIN
[0m12:26:58.614594 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:26:58.675709 [debug] [MainThread]: SQL status: BEGIN in 0.061 seconds
[0m12:26:58.676345 [debug] [MainThread]: Using postgres connection "master"
[0m12:26:58.676916 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m12:26:58.688016 [debug] [MainThread]: SQL status: SELECT 3 in 0.010 seconds
[0m12:26:58.689663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '55a4129e-1997-46c1-9a76-ddc033b4efb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E038F6F6C0>]}
[0m12:26:58.690224 [debug] [MainThread]: On master: ROLLBACK
[0m12:26:58.691006 [debug] [MainThread]: Using postgres connection "master"
[0m12:26:58.691670 [debug] [MainThread]: On master: BEGIN
[0m12:26:58.693510 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m12:26:58.693932 [debug] [MainThread]: On master: COMMIT
[0m12:26:58.694322 [debug] [MainThread]: Using postgres connection "master"
[0m12:26:58.694662 [debug] [MainThread]: On master: COMMIT
[0m12:26:58.695269 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:26:58.695685 [debug] [MainThread]: On master: Close
[0m12:26:58.702825 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments_by_time
[0m12:26:58.703612 [info ] [Thread-1 (]: 1 of 1 START sql table model analytics.fct_shipments_by_time ................... [RUN]
[0m12:26:58.704713 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments_by_time'
[0m12:26:58.705301 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments_by_time
[0m12:26:58.714915 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments_by_time"
[0m12:26:58.717276 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments_by_time
[0m12:26:58.767258 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments_by_time"
[0m12:26:58.769483 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_by_time"
[0m12:26:58.769988 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_by_time: BEGIN
[0m12:26:58.770473 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:26:58.834705 [debug] [Thread-1 (]: SQL status: BEGIN in 0.064 seconds
[0m12:26:58.835339 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_by_time"
[0m12:26:58.835812 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_by_time"} */

  
    

  create  table "delivery_analytics2"."analytics"."fct_shipments_by_time__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    date_trunc('day', shipment_date) as day,
    date_trunc('week', shipment_date) as week,
    date_trunc('month', shipment_date) as month,
    delivery_city,
    courier_id,
    delivery_type,
    count(*) as total_shipments,
    count(case when is_late_delivery = false then 1 end) as on_time_shipments,
    avg(delivery_duration_days) as avg_delivery_days
from shipments
group by 1,2,3,4,5,6
order by day
  );
  
[0m12:26:58.855427 [debug] [Thread-1 (]: SQL status: SELECT 50 in 0.019 seconds
[0m12:26:58.867386 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_by_time"
[0m12:26:58.867891 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_by_time"} */
alter table "delivery_analytics2"."analytics"."fct_shipments_by_time__dbt_tmp" rename to "fct_shipments_by_time"
[0m12:26:58.869294 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m12:26:58.887002 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_by_time: COMMIT
[0m12:26:58.887547 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_by_time"
[0m12:26:58.887989 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_by_time: COMMIT
[0m12:26:58.889606 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m12:26:58.896918 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."fct_shipments_by_time__dbt_backup"
[0m12:26:58.903838 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments_by_time"
[0m12:26:58.904341 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments_by_time"} */
drop table if exists "delivery_analytics2"."analytics"."fct_shipments_by_time__dbt_backup" cascade
[0m12:26:58.905306 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.000 seconds
[0m12:26:58.908188 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments_by_time: Close
[0m12:26:58.910707 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '55a4129e-1997-46c1-9a76-ddc033b4efb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0391F01D0>]}
[0m12:26:58.911641 [info ] [Thread-1 (]: 1 of 1 OK created sql table model analytics.fct_shipments_by_time .............. [[32mSELECT 50[0m in 0.20s]
[0m12:26:58.913266 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments_by_time
[0m12:26:58.915535 [debug] [MainThread]: Using postgres connection "master"
[0m12:26:58.915953 [debug] [MainThread]: On master: BEGIN
[0m12:26:58.916348 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m12:26:58.984007 [debug] [MainThread]: SQL status: BEGIN in 0.068 seconds
[0m12:26:58.984648 [debug] [MainThread]: On master: COMMIT
[0m12:26:58.985136 [debug] [MainThread]: Using postgres connection "master"
[0m12:26:58.985565 [debug] [MainThread]: On master: COMMIT
[0m12:26:58.986268 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m12:26:58.986781 [debug] [MainThread]: On master: Close
[0m12:26:58.987498 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:26:58.987978 [debug] [MainThread]: Connection 'list_delivery_analytics2' was properly closed.
[0m12:26:58.988501 [debug] [MainThread]: Connection 'list_delivery_analytics2_analytics' was properly closed.
[0m12:26:58.988954 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.fct_shipments_by_time' was properly closed.
[0m12:26:58.989533 [info ] [MainThread]: 
[0m12:26:58.990391 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.70 seconds (0.70s).
[0m12:26:58.991904 [debug] [MainThread]: Command end result
[0m12:26:59.026810 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m12:26:59.030763 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m12:26:59.040183 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m12:26:59.041058 [info ] [MainThread]: 
[0m12:26:59.041866 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:26:59.042502 [info ] [MainThread]: 
[0m12:26:59.043612 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m12:26:59.044984 [debug] [MainThread]: Command `dbt run` succeeded at 12:26:59.044831 after 2.69 seconds
[0m12:26:59.045467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E038BDA410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E03898DD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E03898DEF0>]}
[0m12:26:59.045963 [debug] [MainThread]: Flushing usage events
[0m12:27:00.146805 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:52:40.719433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BA49AC980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BA5460B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BA5A9FD90>]}


============================== 13:52:40.725548 | c6f29efd-8bb6-403d-9266-a00f9a895e17 ==============================
[0m13:52:40.725548 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:52:40.727069 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt run --select int', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:52:40.992820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c6f29efd-8bb6-403d-9266-a00f9a895e17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BA4E10770>]}
[0m13:52:41.101968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c6f29efd-8bb6-403d-9266-a00f9a895e17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BA5946E00>]}
[0m13:52:41.103743 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:52:41.637351 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:52:41.881575 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m13:52:41.882432 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\dimensions\dim_couriers.sql
[0m13:52:41.882980 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\dimensions\dim_customers.sql
[0m13:52:41.883460 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\marts\dimensions\dim_cities.sql
[0m13:52:42.244610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c6f29efd-8bb6-403d-9266-a00f9a895e17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BA73AD450>]}
[0m13:52:42.365565 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:52:42.370316 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:52:42.479457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c6f29efd-8bb6-403d-9266-a00f9a895e17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BA5AF6A80>]}
[0m13:52:42.480364 [info ] [MainThread]: Found 13 models, 5 data tests, 3 sources, 459 macros
[0m13:52:42.481230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c6f29efd-8bb6-403d-9266-a00f9a895e17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BA739EDD0>]}
[0m13:52:42.482134 [warn ] [MainThread]: The selection criterion 'int' does not match any enabled nodes
[0m13:52:42.484099 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m13:52:42.485134 [debug] [MainThread]: Command end result
[0m13:52:42.535980 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:52:42.540895 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:52:42.546399 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m13:52:42.547378 [debug] [MainThread]: Command `dbt run` succeeded at 13:52:42.547228 after 2.04 seconds
[0m13:52:42.547889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BA7543C70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BA586B590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BA758A5D0>]}
[0m13:52:42.548342 [debug] [MainThread]: Flushing usage events
[0m13:52:43.616097 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:53:04.605382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B4C6A4980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B4D15CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B4D79FD90>]}


============================== 13:53:04.610842 | f3bd38ed-4e7e-4f33-a743-c49f5d56dd17 ==============================
[0m13:53:04.610842 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:53:04.612260 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt run --select intermediate', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:53:04.889495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f3bd38ed-4e7e-4f33-a743-c49f5d56dd17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B4CB10770>]}
[0m13:53:04.976263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f3bd38ed-4e7e-4f33-a743-c49f5d56dd17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B4D646E00>]}
[0m13:53:04.978174 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:53:05.415604 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:53:05.619426 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:53:05.619940 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:53:05.682054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f3bd38ed-4e7e-4f33-a743-c49f5d56dd17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B4F138F50>]}
[0m13:53:05.806529 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:53:05.810700 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:53:05.843245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f3bd38ed-4e7e-4f33-a743-c49f5d56dd17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B4D633D40>]}
[0m13:53:05.844036 [info ] [MainThread]: Found 13 models, 5 data tests, 3 sources, 459 macros
[0m13:53:05.844854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f3bd38ed-4e7e-4f33-a743-c49f5d56dd17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B4D92E5F0>]}
[0m13:53:05.847669 [info ] [MainThread]: 
[0m13:53:05.848287 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:53:05.848830 [info ] [MainThread]: 
[0m13:53:05.849725 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m13:53:05.854681 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2'
[0m13:53:05.993529 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2"
[0m13:53:05.994072 [debug] [ThreadPool]: On list_delivery_analytics2: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2"} */

    select distinct nspname from pg_namespace
  
[0m13:53:05.994448 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:53:06.142057 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.147 seconds
[0m13:53:06.143769 [debug] [ThreadPool]: On list_delivery_analytics2: Close
[0m13:53:06.147493 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2_analytics'
[0m13:53:06.157600 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m13:53:06.158112 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: BEGIN
[0m13:53:06.158610 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:53:06.224714 [debug] [ThreadPool]: SQL status: BEGIN in 0.066 seconds
[0m13:53:06.225285 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m13:53:06.225693 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2_analytics"} */
select
      'delivery_analytics2' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m13:53:06.237971 [debug] [ThreadPool]: SQL status: SELECT 7 in 0.012 seconds
[0m13:53:06.240511 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: ROLLBACK
[0m13:53:06.241332 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: Close
[0m13:53:06.250683 [debug] [MainThread]: Using postgres connection "master"
[0m13:53:06.251373 [debug] [MainThread]: On master: BEGIN
[0m13:53:06.252041 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:53:06.318024 [debug] [MainThread]: SQL status: BEGIN in 0.066 seconds
[0m13:53:06.318638 [debug] [MainThread]: Using postgres connection "master"
[0m13:53:06.319203 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m13:53:06.330621 [debug] [MainThread]: SQL status: SELECT 3 in 0.011 seconds
[0m13:53:06.332807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f3bd38ed-4e7e-4f33-a743-c49f5d56dd17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B4F50C870>]}
[0m13:53:06.333383 [debug] [MainThread]: On master: ROLLBACK
[0m13:53:06.334118 [debug] [MainThread]: Using postgres connection "master"
[0m13:53:06.334526 [debug] [MainThread]: On master: BEGIN
[0m13:53:06.335536 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m13:53:06.335945 [debug] [MainThread]: On master: COMMIT
[0m13:53:06.336313 [debug] [MainThread]: Using postgres connection "master"
[0m13:53:06.336677 [debug] [MainThread]: On master: COMMIT
[0m13:53:06.337309 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:53:06.337745 [debug] [MainThread]: On master: Close
[0m13:53:06.345011 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.int_active_delays
[0m13:53:06.345650 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.int_shipment_delays
[0m13:53:06.346210 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.int_shipment_status
[0m13:53:06.346918 [info ] [Thread-1 (]: 1 of 3 START sql view model analytics.int_active_delays ........................ [RUN]
[0m13:53:06.347716 [info ] [Thread-2 (]: 2 of 3 START sql view model analytics.int_shipment_delays ...................... [RUN]
[0m13:53:06.349458 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.int_active_delays'
[0m13:53:06.348593 [info ] [Thread-3 (]: 3 of 3 START sql view model analytics.int_shipment_status ...................... [RUN]
[0m13:53:06.350328 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.int_shipment_delays'
[0m13:53:06.350913 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.int_active_delays
[0m13:53:06.351621 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.int_shipment_status'
[0m13:53:06.352283 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.int_shipment_delays
[0m13:53:06.362004 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.int_active_delays"
[0m13:53:06.362682 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.int_shipment_status
[0m13:53:06.366468 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.int_shipment_delays"
[0m13:53:06.369785 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.int_shipment_status"
[0m13:53:06.374886 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.int_shipment_status
[0m13:53:06.376136 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.int_active_delays
[0m13:53:06.404083 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.int_shipment_delays
[0m13:53:06.427316 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.int_shipment_status"
[0m13:53:06.431648 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.int_active_delays"
[0m13:53:06.439173 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.int_shipment_delays"
[0m13:53:06.444055 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_status"
[0m13:53:06.444713 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_delays"
[0m13:53:06.445329 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.int_active_delays"
[0m13:53:06.445945 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.int_shipment_status: BEGIN
[0m13:53:06.446499 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.int_shipment_delays: BEGIN
[0m13:53:06.447116 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.int_active_delays: BEGIN
[0m13:53:06.447827 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:53:06.448384 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:53:06.449001 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:53:06.530772 [debug] [Thread-3 (]: SQL status: BEGIN in 0.083 seconds
[0m13:53:06.531418 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_status"
[0m13:53:06.532040 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.int_shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_shipment_status"} */

  create view "delivery_analytics2"."analytics"."int_shipment_status__dbt_tmp"
    
    
  as (
    select
    shipment_id,
    current_status,
    delivery_type,
    delivery_city,
    courier_id
from "delivery_analytics2"."analytics"."fct_shipments"




/*

Koji je status po posiljci je li 
pending, picked up, delivered


*/
  );
[0m13:53:06.541008 [debug] [Thread-2 (]: SQL status: BEGIN in 0.093 seconds
[0m13:53:06.541601 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_delays"
[0m13:53:06.542094 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.int_shipment_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_shipment_delays"} */

  create view "delivery_analytics2"."analytics"."int_shipment_delays__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    shipment_id,
    shipment_date,
    expected_delivery_date,
    completed_at,
    case 
        when completed_at is not null then (completed_at::date - expected_delivery_date)
        else (current_date - expected_delivery_date)
    end as delay_days
from shipments



/*

Koliko je kasnjenje po posiljci

*/
  );
[0m13:53:06.543852 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.011 seconds
[0m13:53:06.552229 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_status"
[0m13:53:06.552849 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.010 seconds
[0m13:53:06.553413 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.int_shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_shipment_status"} */
alter table "delivery_analytics2"."analytics"."int_shipment_status__dbt_tmp" rename to "int_shipment_status"
[0m13:53:06.556999 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_delays"
[0m13:53:06.557609 [debug] [Thread-1 (]: SQL status: BEGIN in 0.109 seconds
[0m13:53:06.558229 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.int_shipment_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_shipment_delays"} */
alter table "delivery_analytics2"."analytics"."int_shipment_delays__dbt_tmp" rename to "int_shipment_delays"
[0m13:53:06.558791 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:53:06.559268 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.int_active_delays"
[0m13:53:06.638181 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.078 seconds
[0m13:53:06.649534 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.int_shipment_status: COMMIT
[0m13:53:06.650244 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.int_active_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_active_delays"} */

  create view "delivery_analytics2"."analytics"."int_active_delays__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select *
from shipments
where current_status in ('pending','picked_up')
   or (current_status='delivered' and is_late_delivery)



/*

Koje posiljke trenutno kasne

*/
  );
[0m13:53:06.653532 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.int_shipment_delays: COMMIT
[0m13:53:06.654171 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_status"
[0m13:53:06.655372 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_delays"
[0m13:53:06.656008 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.int_shipment_status: COMMIT
[0m13:53:06.656565 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.int_shipment_delays: COMMIT
[0m13:53:06.666090 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.011 seconds
[0m13:53:06.670029 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.int_active_delays"
[0m13:53:06.670520 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.int_active_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_active_delays"} */
alter table "delivery_analytics2"."analytics"."int_active_delays__dbt_tmp" rename to "int_active_delays"
[0m13:53:06.671931 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:53:06.673724 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.int_active_delays: COMMIT
[0m13:53:06.674393 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.int_active_delays"
[0m13:53:06.674901 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.int_active_delays: COMMIT
[0m13:53:06.691799 [debug] [Thread-3 (]: SQL status: COMMIT in 0.035 seconds
[0m13:53:06.692529 [debug] [Thread-1 (]: SQL status: COMMIT in 0.017 seconds
[0m13:53:06.693088 [debug] [Thread-2 (]: SQL status: COMMIT in 0.036 seconds
[0m13:53:06.701653 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics2"."analytics"."int_shipment_status__dbt_backup"
[0m13:53:06.704359 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."int_active_delays__dbt_backup"
[0m13:53:06.706950 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics2"."analytics"."int_shipment_delays__dbt_backup"
[0m13:53:06.713499 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_status"
[0m13:53:06.714741 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.int_active_delays"
[0m13:53:06.716215 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_delays"
[0m13:53:06.716908 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.int_shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_shipment_status"} */
drop view if exists "delivery_analytics2"."analytics"."int_shipment_status__dbt_backup" cascade
[0m13:53:06.717664 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.int_active_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_active_delays"} */
drop view if exists "delivery_analytics2"."analytics"."int_active_delays__dbt_backup" cascade
[0m13:53:06.718527 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.int_shipment_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_shipment_delays"} */
drop view if exists "delivery_analytics2"."analytics"."int_shipment_delays__dbt_backup" cascade
[0m13:53:06.719972 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.001 seconds
[0m13:53:06.720561 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.001 seconds
[0m13:53:06.723769 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.int_active_delays: Close
[0m13:53:06.724341 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.005 seconds
[0m13:53:06.726340 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.int_shipment_status: Close
[0m13:53:06.728216 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.int_shipment_delays: Close
[0m13:53:06.732231 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f3bd38ed-4e7e-4f33-a743-c49f5d56dd17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B4F1FFD10>]}
[0m13:53:06.732813 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f3bd38ed-4e7e-4f33-a743-c49f5d56dd17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B4F24A780>]}
[0m13:53:06.734418 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f3bd38ed-4e7e-4f33-a743-c49f5d56dd17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B4CA8CC30>]}
[0m13:53:06.733864 [info ] [Thread-1 (]: 1 of 3 OK created sql view model analytics.int_active_delays ................... [[32mCREATE VIEW[0m in 0.38s]
[0m13:53:06.735617 [info ] [Thread-3 (]: 3 of 3 OK created sql view model analytics.int_shipment_status ................. [[32mCREATE VIEW[0m in 0.38s]
[0m13:53:06.737584 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.int_active_delays
[0m13:53:06.738603 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.int_shipment_status
[0m13:53:06.736480 [info ] [Thread-2 (]: 2 of 3 OK created sql view model analytics.int_shipment_delays ................. [[32mCREATE VIEW[0m in 0.38s]
[0m13:53:06.740174 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.int_shipment_delays
[0m13:53:06.742306 [debug] [MainThread]: Using postgres connection "master"
[0m13:53:06.742748 [debug] [MainThread]: On master: BEGIN
[0m13:53:06.743125 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:53:06.810739 [debug] [MainThread]: SQL status: BEGIN in 0.067 seconds
[0m13:53:06.811379 [debug] [MainThread]: On master: COMMIT
[0m13:53:06.811874 [debug] [MainThread]: Using postgres connection "master"
[0m13:53:06.812428 [debug] [MainThread]: On master: COMMIT
[0m13:53:06.813156 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:53:06.813708 [debug] [MainThread]: On master: Close
[0m13:53:06.814485 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:53:06.814865 [debug] [MainThread]: Connection 'list_delivery_analytics2' was properly closed.
[0m13:53:06.815226 [debug] [MainThread]: Connection 'list_delivery_analytics2_analytics' was properly closed.
[0m13:53:06.815533 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.int_active_delays' was properly closed.
[0m13:53:06.815939 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.int_shipment_delays' was properly closed.
[0m13:53:06.816243 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.int_shipment_status' was properly closed.
[0m13:53:06.816683 [info ] [MainThread]: 
[0m13:53:06.817540 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 0.97 seconds (0.97s).
[0m13:53:06.819292 [debug] [MainThread]: Command end result
[0m13:53:06.851669 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:53:06.856474 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:53:06.865935 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m13:53:06.866454 [info ] [MainThread]: 
[0m13:53:06.867373 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:53:06.868276 [info ] [MainThread]: 
[0m13:53:06.869052 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3
[0m13:53:06.870776 [debug] [MainThread]: Command `dbt run` succeeded at 13:53:06.870590 after 2.47 seconds
[0m13:53:06.871335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B4F0AA6D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B4EEA2F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B46FF1780>]}
[0m13:53:06.871919 [debug] [MainThread]: Flushing usage events
[0m13:53:07.854178 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:53:25.305686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AB6FC8980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AB7A7CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AB80BFD90>]}


============================== 13:53:25.310971 | 4c94eeeb-cf88-464d-bc9d-90bbd34b55e3 ==============================
[0m13:53:25.310971 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:53:25.311917 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run --select dimensions', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:53:25.531575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4c94eeeb-cf88-464d-bc9d-90bbd34b55e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AB7430770>]}
[0m13:53:25.599022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4c94eeeb-cf88-464d-bc9d-90bbd34b55e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AB7F66E00>]}
[0m13:53:25.600153 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:53:25.933313 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:53:26.096823 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:53:26.097259 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:53:26.145647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4c94eeeb-cf88-464d-bc9d-90bbd34b55e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AB9A9CF50>]}
[0m13:53:26.236475 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:53:26.239590 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:53:26.262546 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4c94eeeb-cf88-464d-bc9d-90bbd34b55e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AB7F53D40>]}
[0m13:53:26.263261 [info ] [MainThread]: Found 13 models, 5 data tests, 3 sources, 459 macros
[0m13:53:26.263852 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4c94eeeb-cf88-464d-bc9d-90bbd34b55e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AB91CE5F0>]}
[0m13:53:26.264611 [warn ] [MainThread]: The selection criterion 'dimensions' does not match any enabled nodes
[0m13:53:26.266222 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m13:53:26.268357 [debug] [MainThread]: Command end result
[0m13:53:26.298840 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:53:26.301698 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:53:26.305759 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m13:53:26.306509 [debug] [MainThread]: Command `dbt run` succeeded at 13:53:26.306394 after 1.19 seconds
[0m13:53:26.306906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AB9B67A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AB7E8B590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AB9B6DD90>]}
[0m13:53:26.307260 [debug] [MainThread]: Flushing usage events
[0m13:53:27.324305 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:54:01.691949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C290CB8980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C29176CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C291DAFD90>]}


============================== 13:54:01.696448 | 16a20028-24cc-4dc2-8d5c-807c3cb20a98 ==============================
[0m13:54:01.696448 [info ] [MainThread]: Running with dbt=1.10.15
[0m13:54:01.697516 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run --select dimensions', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:54:01.888987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '16a20028-24cc-4dc2-8d5c-807c3cb20a98', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C291120770>]}
[0m13:54:01.951418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '16a20028-24cc-4dc2-8d5c-807c3cb20a98', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C291C56E00>]}
[0m13:54:01.952592 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:54:02.277834 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m13:54:02.484195 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:54:02.484776 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:54:02.538933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '16a20028-24cc-4dc2-8d5c-807c3cb20a98', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2936A4F50>]}
[0m13:54:02.643278 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:54:02.646608 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:54:02.669297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '16a20028-24cc-4dc2-8d5c-807c3cb20a98', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C291C43D40>]}
[0m13:54:02.669895 [info ] [MainThread]: Found 13 models, 5 data tests, 3 sources, 459 macros
[0m13:54:02.670673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '16a20028-24cc-4dc2-8d5c-807c3cb20a98', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C292F0E5F0>]}
[0m13:54:02.671457 [warn ] [MainThread]: The selection criterion 'dimensions' does not match any enabled nodes
[0m13:54:02.673067 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m13:54:02.675145 [debug] [MainThread]: Command end result
[0m13:54:02.699472 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:54:02.702663 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:54:02.706770 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m13:54:02.707568 [debug] [MainThread]: Command `dbt run` succeeded at 13:54:02.707451 after 1.18 seconds
[0m13:54:02.707939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C293763A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C291B7B590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C293769D90>]}
[0m13:54:02.708289 [debug] [MainThread]: Flushing usage events
[0m13:54:03.639155 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:03:01.075859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA72C08980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA736C0B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA73CFFD90>]}


============================== 14:03:01.082099 | 1b1c8a27-e0a7-4eec-a674-60f3b605334f ==============================
[0m14:03:01.082099 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:03:01.083339 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select fct_shipments fct_shipment_delays couriers_performance courier_load shipments_by_time delays_by_city_type', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:03:01.347349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1b1c8a27-e0a7-4eec-a674-60f3b605334f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA73070770>]}
[0m14:03:01.434113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1b1c8a27-e0a7-4eec-a674-60f3b605334f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA73BA6E00>]}
[0m14:03:01.435518 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:03:01.911858 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:03:02.125889 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 5 files added, 0 files changed.
[0m14:03:02.155797 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\views\courier_load.sql
[0m14:03:02.156744 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\views\delays_by_city_type.sql
[0m14:03:02.157171 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\views\shipments_by_time.sql
[0m14:03:02.157576 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\views\couriers_performance.sql
[0m14:03:02.157977 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\facts\fct_shipments_delays.sql
[0m14:03:02.158323 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\facts\fct_shipments_by_time.sql
[0m14:03:02.439175 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_delivery_analytics.delays_by_city_type' (models\views\delays_by_city_type.sql) depends on a node named 'fct_shipment_delays' which was not found
[0m14:03:02.440788 [debug] [MainThread]: Command `dbt run` failed at 14:03:02.440616 after 1.57 seconds
[0m14:03:02.441277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA75171250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA75648450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BA757316D0>]}
[0m14:03:02.441934 [debug] [MainThread]: Flushing usage events
[0m14:03:03.593208 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:03:32.994119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDD787C980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDD8320B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDD895FD90>]}


============================== 14:03:32.999549 | 67d17801-09ac-432a-81a9-60422350c395 ==============================
[0m14:03:32.999549 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:03:33.000950 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt run --select fct_shipments fct_shipment_delays couriers_performance courier_load shipments_by_time delays_by_city_type', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:03:33.261093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '67d17801-09ac-432a-81a9-60422350c395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDD7CE0770>]}
[0m14:03:33.352251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '67d17801-09ac-432a-81a9-60422350c395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDD8806E00>]}
[0m14:03:33.353569 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:03:33.807744 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:03:34.018499 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 5 files added, 0 files changed.
[0m14:03:34.019273 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\views\shipments_by_time.sql
[0m14:03:34.019716 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\views\couriers_performance.sql
[0m14:03:34.020155 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\views\courier_load.sql
[0m14:03:34.020551 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\marts\facts\fct_shipment_delays.sql
[0m14:03:34.020959 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\views\delays_by_city_type.sql
[0m14:03:34.021313 [debug] [MainThread]: Partial parsing: deleted file: dbt_delivery_analytics://models\marts\facts\fct_shipments_by_time.sql
[0m14:03:34.409812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '67d17801-09ac-432a-81a9-60422350c395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDDA25D550>]}
[0m14:03:34.615198 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:03:34.620836 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:03:34.653659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '67d17801-09ac-432a-81a9-60422350c395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDD87F3D40>]}
[0m14:03:34.654333 [info ] [MainThread]: Found 17 models, 5 data tests, 3 sources, 459 macros
[0m14:03:34.655375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67d17801-09ac-432a-81a9-60422350c395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDD482F150>]}
[0m14:03:34.659138 [info ] [MainThread]: 
[0m14:03:34.659934 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:03:34.660552 [info ] [MainThread]: 
[0m14:03:34.661415 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:03:34.667084 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2'
[0m14:03:34.805488 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2"
[0m14:03:34.806140 [debug] [ThreadPool]: On list_delivery_analytics2: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2"} */

    select distinct nspname from pg_namespace
  
[0m14:03:34.806964 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:03:34.904529 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.097 seconds
[0m14:03:34.906796 [debug] [ThreadPool]: On list_delivery_analytics2: Close
[0m14:03:34.910300 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2_analytics'
[0m14:03:34.917987 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m14:03:34.918533 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: BEGIN
[0m14:03:34.918898 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:03:35.001044 [debug] [ThreadPool]: SQL status: BEGIN in 0.082 seconds
[0m14:03:35.001690 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m14:03:35.002142 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2_analytics"} */
select
      'delivery_analytics2' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m14:03:35.017702 [debug] [ThreadPool]: SQL status: SELECT 10 in 0.015 seconds
[0m14:03:35.019714 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: ROLLBACK
[0m14:03:35.020404 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: Close
[0m14:03:35.030222 [debug] [MainThread]: Using postgres connection "master"
[0m14:03:35.030745 [debug] [MainThread]: On master: BEGIN
[0m14:03:35.031139 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:03:35.104240 [debug] [MainThread]: SQL status: BEGIN in 0.073 seconds
[0m14:03:35.104884 [debug] [MainThread]: Using postgres connection "master"
[0m14:03:35.105482 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:03:35.116348 [debug] [MainThread]: SQL status: SELECT 6 in 0.010 seconds
[0m14:03:35.119687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67d17801-09ac-432a-81a9-60422350c395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDDA577790>]}
[0m14:03:35.120239 [debug] [MainThread]: On master: ROLLBACK
[0m14:03:35.121010 [debug] [MainThread]: Using postgres connection "master"
[0m14:03:35.121411 [debug] [MainThread]: On master: BEGIN
[0m14:03:35.122377 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m14:03:35.123106 [debug] [MainThread]: On master: COMMIT
[0m14:03:35.123786 [debug] [MainThread]: Using postgres connection "master"
[0m14:03:35.124339 [debug] [MainThread]: On master: COMMIT
[0m14:03:35.124968 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:03:35.125422 [debug] [MainThread]: On master: Close
[0m14:03:35.131455 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m14:03:35.132179 [info ] [Thread-1 (]: 1 of 6 START sql table model analytics.fct_shipments ........................... [RUN]
[0m14:03:35.133198 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipments'
[0m14:03:35.133697 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m14:03:35.146317 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:03:35.149465 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m14:03:35.199462 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m14:03:35.201944 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:03:35.202497 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m14:03:35.202994 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:03:35.276555 [debug] [Thread-1 (]: SQL status: BEGIN in 0.073 seconds
[0m14:03:35.277146 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:03:35.277751 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics2"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (

    select *
    from "delivery_analytics2"."analytics"."stg_shipments"

),

customers as (

    select *
    from "delivery_analytics2"."analytics"."stg_customers"

),

couriers as (

    select *
    from "delivery_analytics2"."analytics"."stg_couriers"

)

select
    s.shipment_id,

    -- keys
    s.sender_id,
    s.receiver_id,
    s.courier_id,

    -- dates
    s.shipment_date,
    s.expected_delivery_date,
    s.completed_at,

    -- metrics
    s.price,
    s.distance_km,

    -- attributes
    s.delivery_city,
    s.delivery_type,
    s.current_status,

    -- derived metrics
    case
        when s.completed_at is not null
        then (s.completed_at::date - s.shipment_date)
    end as delivery_duration_days,

    case
        when s.completed_at > s.expected_delivery_date then true
        else false
    end as is_late_delivery

from shipments s




/*
    FACT TABLE: fct_shipments

    Svaki red u ovoj tabeli predstavlja jednu pošiljku (shipment) iz naše baze.

    Svrha:
    - Centralizovati sve metrike vezane za pošiljke na jednom mjestu
    - Omogućiti analitiku po vremenu, kuriru, tipu pošiljke i gradu
    - Poslužiti kao ulaz za BI dashboard ili dalju analitiku u marts layer-u

    Šta se ovdje računa:
    1. Ključne kolone:
        - shipment_id
        - sender_id, receiver_id
        - courier_id
    2. Datumi:
        - shipment_date (kada je pošiljka poslana)
        - expected_delivery_date (planirani datum isporuke)
        - completed_at (kada je pošiljka zaista dostavljena)
    3. Metričke kolone:
        - price
        - distance_km
    4. Atributi:
        - delivery_city
        - delivery_type (standard / express)
        - current_status (pending, picked_up, delivered)
    5. Derived metrics (izvedene kolone):
        - delivery_duration_days = koliko je dana trajala pošiljka
        - is_late_delivery = boolean, true ako je pošiljka dostavljena nakon expected_delivery_date

    Napomena:
    - Ova tabela služi kao "fact" jer sadrži sve mjere koje se mogu agregirati
    - Za dimenzije ćemo imati posebne dim_tables: dim_customers, dim_couriers, dim_cities
    - Može se koristiti za: analizu performansi kurira, kašnjenja po gradu, tip pošiljke, revenue po gradu itd.

*/
  );
  
[0m14:03:35.297140 [debug] [Thread-1 (]: SQL status: SELECT 50 in 0.019 seconds
[0m14:03:35.310240 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:03:35.310978 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics2"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m14:03:35.312292 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:03:35.315499 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:03:35.315969 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics2"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m14:03:35.317248 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:03:35.338682 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:03:35.339498 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:03:35.340137 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m14:03:35.342513 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m14:03:35.350318 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."fct_shipments__dbt_backup"
[0m14:03:35.356890 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m14:03:35.357433 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics2"."analytics"."fct_shipments__dbt_backup" cascade
[0m14:03:35.375451 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.017 seconds
[0m14:03:35.378465 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m14:03:35.380878 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67d17801-09ac-432a-81a9-60422350c395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDDA7BC710>]}
[0m14:03:35.381842 [info ] [Thread-1 (]: 1 of 6 OK created sql table model analytics.fct_shipments ...................... [[32mSELECT 50[0m in 0.25s]
[0m14:03:35.383322 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m14:03:35.384612 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.shipments_by_time
[0m14:03:35.385209 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.couriers_performance
[0m14:03:35.385952 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_load
[0m14:03:35.387294 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.fct_shipment_delays
[0m14:03:35.386720 [info ] [Thread-1 (]: 5 of 6 START sql view model analytics.shipments_by_time ........................ [RUN]
[0m14:03:35.388575 [info ] [Thread-3 (]: 3 of 6 START sql view model analytics.couriers_performance ..................... [RUN]
[0m14:03:35.389908 [info ] [Thread-2 (]: 2 of 6 START sql view model analytics.courier_load ............................. [RUN]
[0m14:03:35.392377 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.shipments_by_time)
[0m14:03:35.393324 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.couriers_performance'
[0m14:03:35.391453 [info ] [Thread-4 (]: 4 of 6 START sql table model analytics.fct_shipment_delays ..................... [RUN]
[0m14:03:35.394187 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_load'
[0m14:03:35.394948 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_time
[0m14:03:35.395680 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.couriers_performance
[0m14:03:35.396407 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.fct_shipment_delays'
[0m14:03:35.396958 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_load
[0m14:03:35.400121 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_time"
[0m14:03:35.402814 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.couriers_performance"
[0m14:03:35.403422 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.fct_shipment_delays
[0m14:03:35.409267 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_load"
[0m14:03:35.414604 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipment_delays"
[0m14:03:35.417059 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.shipments_by_time
[0m14:03:35.417853 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_load
[0m14:03:35.418451 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.fct_shipment_delays
[0m14:03:35.445226 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_load"
[0m14:03:35.447604 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_time"
[0m14:03:35.452032 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipment_delays"
[0m14:03:35.452570 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.couriers_performance
[0m14:03:35.457490 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.couriers_performance"
[0m14:03:35.459405 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipment_delays"
[0m14:03:35.460181 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:03:35.460656 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipment_delays: BEGIN
[0m14:03:35.461128 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: BEGIN
[0m14:03:35.461684 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:03:35.462227 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:03:35.462665 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:03:35.463184 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:03:35.463796 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: BEGIN
[0m14:03:35.464525 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: BEGIN
[0m14:03:35.465149 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:03:35.465764 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:03:35.540087 [debug] [Thread-4 (]: SQL status: BEGIN in 0.078 seconds
[0m14:03:35.540929 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipment_delays"
[0m14:03:35.541531 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipment_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipment_delays"} */

  
    

  create  table "delivery_analytics2"."analytics"."fct_shipment_delays__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    shipment_id,
    shipment_date,
    expected_delivery_date,
    completed_at,
    delivery_city,
    delivery_type,
    courier_id,

    -- kašnjenje u danima
    case
        when completed_at is not null then (completed_at::date - expected_delivery_date::date)
        else (current_date - expected_delivery_date::date)
    end as delay_days,

    -- da li je pošiljka kasnila
    case
        when completed_at > expected_delivery_date then true
        when completed_at is null and current_date > expected_delivery_date then true
        else false
    end as is_late_delivery

from shipments

/*
FACT TABLE: fct_shipment_delays

Svrha:
- Prati kašnjenja po pošiljci
- Može se koristiti za analizu kašnjenja po gradu, tipu pošiljke i kuriru
- Ulaz za view-eve: delays_by_city_type, courier_load, couriers_performance itd.

Kolone:
- shipment_id: jedinstveni identifikator pošiljke
- shipment_date, expected_delivery_date, completed_at: datumi
- courier_id, delivery_city, delivery_type: atributi pošiljke
- delay_days: broj dana kašnjenja
- is_late_delivery: boolean, true ako je pošiljka kasnila
*/
  );
  
[0m14:03:35.546474 [debug] [Thread-1 (]: SQL status: BEGIN in 0.084 seconds
[0m14:03:35.547097 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:03:35.547731 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */

  create view "delivery_analytics2"."analytics"."shipments_by_time__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
    where shipment_date is not null
)

select
    date_trunc('day', shipment_date) as day,
    date_trunc('week', shipment_date) as week,
    date_trunc('month', shipment_date) as month,
    delivery_city,
    courier_id,
    delivery_type,
    count(*) as total_shipments,
    count(case when is_late_delivery = false then 1 end) as on_time_shipments,
    avg(delivery_duration_days) as avg_delivery_days,
    sum(price) as total_revenue,
    sum(distance_km) as total_distance

from shipments
group by 1,2,3,4,5,6
order by day, delivery_city, courier_id, delivery_type

/*
VIEW: shipments_by_time

Svrha:
- Analiza pošiljki po danu, sedmici i mjesecu
- Segmentacija po delivery_city, courier_id, delivery_type (standard/express)
- Metričke kolone: total_shipments, on_time_shipments, avg_delivery_days, total_revenue, total_distance
- Može se koristiti za trend analizu i dashboard vizualizacije
*/
  );
[0m14:03:35.551985 [debug] [Thread-3 (]: SQL status: BEGIN in 0.087 seconds
[0m14:03:35.552475 [debug] [Thread-4 (]: SQL status: SELECT 50 in 0.010 seconds
[0m14:03:35.552992 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:03:35.561671 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipment_delays"
[0m14:03:35.562196 [debug] [Thread-2 (]: SQL status: BEGIN in 0.096 seconds
[0m14:03:35.562636 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.014 seconds
[0m14:03:35.563246 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */

  create view "delivery_analytics2"."analytics"."couriers_performance__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    courier_id,
    count(*) as total_shipments,
    count(case when current_status = 'delivered' then 1 end) as delivered_shipments,
    avg(delivery_duration_days) as avg_delivery_duration_days,
    sum(case when delivery_type = 'express' then 1 else 0 end) as express_shipments_count,
    avg(distance_km) as avg_distance_per_shipment,
    sum(distance_km) as total_distance_per_courier,
    (count(case when is_late_delivery = false then 1 end)::float / nullif(count(*),0))*100 as success_rate_percent,
    count(case when current_status != 'delivered' then 1 end) as pending_shipments
from shipments
group by courier_id
order by courier_id

/*
VIEW: couriers_performance

Svrha:
- Prikazuje performanse kurira
- Metričke kolone: total_shipments, delivered_shipments, avg_delivery_duration_days,
  express_shipments_count, avg_distance_per_shipment, total_distance_per_courier, success_rate_percent, pending_shipments
*/
  );
[0m14:03:35.563889 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipment_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipment_delays"} */
alter table "delivery_analytics2"."analytics"."fct_shipment_delays__dbt_tmp" rename to "fct_shipment_delays"
[0m14:03:35.564414 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:03:35.567278 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:03:35.567966 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */

  create view "delivery_analytics2"."analytics"."courier_load__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    courier_id,
    shipment_date,
    count(*) as total_shipments_per_day,
    count(case when current_status != 'delivered' then 1 end) as active_shipments
from shipments
group by courier_id, shipment_date
order by courier_id, shipment_date

/*
VIEW: courier_load

Svrha:
- Analiza radnog opterećenja kurira po danu
- active_shipments = pošiljke koje nisu još dostavljene
*/
  );
[0m14:03:35.568474 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */
alter table "delivery_analytics2"."analytics"."shipments_by_time__dbt_tmp" rename to "shipments_by_time"
[0m14:03:35.568975 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:03:35.570668 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipment_delays: COMMIT
[0m14:03:35.571129 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m14:03:35.571670 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipment_delays"
[0m14:03:35.573354 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: COMMIT
[0m14:03:35.573859 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipment_delays: COMMIT
[0m14:03:35.574343 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:03:35.574908 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: COMMIT
[0m14:03:35.576015 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m14:03:35.578609 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics2"."analytics"."fct_shipment_delays__dbt_backup"
[0m14:03:35.579097 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.011 seconds
[0m14:03:35.579602 [debug] [Thread-1 (]: SQL status: COMMIT in 0.004 seconds
[0m14:03:35.580073 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.011 seconds
[0m14:03:35.580956 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipment_delays"
[0m14:03:35.583540 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:03:35.585865 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."shipments_by_time__dbt_backup"
[0m14:03:35.588754 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:03:35.589314 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipment_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipment_delays"} */
drop table if exists "delivery_analytics2"."analytics"."fct_shipment_delays__dbt_backup" cascade
[0m14:03:35.589960 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */
alter table "delivery_analytics2"."analytics"."couriers_performance__dbt_tmp" rename to "couriers_performance"
[0m14:03:35.593208 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:03:35.593720 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */
alter table "delivery_analytics2"."analytics"."courier_load__dbt_tmp" rename to "courier_load"
[0m14:03:35.594469 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */
drop view if exists "delivery_analytics2"."analytics"."shipments_by_time__dbt_backup" cascade
[0m14:03:35.595001 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:03:35.595435 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.001 seconds
[0m14:03:35.597040 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: COMMIT
[0m14:03:35.597514 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.002 seconds
[0m14:03:35.597924 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m14:03:35.599211 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.fct_shipment_delays: Close
[0m14:03:35.599703 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:03:35.601289 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: Close
[0m14:03:35.602708 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: COMMIT
[0m14:03:35.603929 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: COMMIT
[0m14:03:35.604652 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67d17801-09ac-432a-81a9-60422350c395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDDA790AA0>]}
[0m14:03:35.605305 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:03:35.606121 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67d17801-09ac-432a-81a9-60422350c395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDDA3CD4F0>]}
[0m14:03:35.608037 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m14:03:35.607318 [info ] [Thread-4 (]: 4 of 6 OK created sql table model analytics.fct_shipment_delays ................ [[32mSELECT 50[0m in 0.21s]
[0m14:03:35.608852 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: COMMIT
[0m14:03:35.612009 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics2"."analytics"."couriers_performance__dbt_backup"
[0m14:03:35.609630 [info ] [Thread-1 (]: 5 of 6 OK created sql view model analytics.shipments_by_time ................... [[32mCREATE VIEW[0m in 0.21s]
[0m14:03:35.612947 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.fct_shipment_delays
[0m14:03:35.613918 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:03:35.614678 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.shipments_by_time
[0m14:03:35.615187 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m14:03:35.615951 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */
drop view if exists "delivery_analytics2"."analytics"."couriers_performance__dbt_backup" cascade
[0m14:03:35.616584 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.delays_by_city_type
[0m14:03:35.619343 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics2"."analytics"."courier_load__dbt_backup"
[0m14:03:35.620516 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.001 seconds
[0m14:03:35.621328 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:03:35.620055 [info ] [Thread-4 (]: 6 of 6 START sql view model analytics.delays_by_city_type ...................... [RUN]
[0m14:03:35.623754 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: Close
[0m14:03:35.624328 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */
drop view if exists "delivery_analytics2"."analytics"."courier_load__dbt_backup" cascade
[0m14:03:35.624975 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipment_delays, now model.dbt_delivery_analytics.delays_by_city_type)
[0m14:03:35.625727 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.delays_by_city_type
[0m14:03:35.626367 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67d17801-09ac-432a-81a9-60422350c395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDDA6464D0>]}
[0m14:03:35.626874 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.001 seconds
[0m14:03:35.629909 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:03:35.632843 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: Close
[0m14:03:35.630708 [info ] [Thread-3 (]: 3 of 6 OK created sql view model analytics.couriers_performance ................ [[32mCREATE VIEW[0m in 0.23s]
[0m14:03:35.633997 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.couriers_performance
[0m14:03:35.634708 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67d17801-09ac-432a-81a9-60422350c395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDDA828360>]}
[0m14:03:35.636380 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.delays_by_city_type
[0m14:03:35.635978 [info ] [Thread-2 (]: 2 of 6 OK created sql view model analytics.courier_load ........................ [[32mCREATE VIEW[0m in 0.24s]
[0m14:03:35.641564 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:03:35.642482 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_load
[0m14:03:35.644490 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:03:35.644948 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: BEGIN
[0m14:03:35.645308 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:03:35.713608 [debug] [Thread-4 (]: SQL status: BEGIN in 0.068 seconds
[0m14:03:35.714205 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:03:35.714763 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */

  create view "delivery_analytics2"."analytics"."delays_by_city_type__dbt_tmp"
    
    
  as (
    with delays as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipment_delays"
)

select
    delivery_city,
    delivery_type,
    count(*) as total_shipments,
    count(case when is_late_delivery = false then 1 end) as on_time_shipments,
    avg(delay_days) as avg_delay_days
from delays
group by delivery_city, delivery_type
order by delivery_city, delivery_type

/*
VIEW: delays_by_city_type

Svrha:
- Agregacija kašnjenja po delivery_city i delivery_type
- Metričke kolone: total_shipments, on_time_shipments, avg_delay_days
*/
  );
[0m14:03:35.721159 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.006 seconds
[0m14:03:35.725495 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:03:35.725913 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */
alter table "delivery_analytics2"."analytics"."delays_by_city_type__dbt_tmp" rename to "delays_by_city_type"
[0m14:03:35.726862 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:03:35.728160 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: COMMIT
[0m14:03:35.728538 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:03:35.728880 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: COMMIT
[0m14:03:35.730531 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m14:03:35.732528 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics2"."analytics"."delays_by_city_type__dbt_backup"
[0m14:03:35.733182 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:03:35.733541 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */
drop view if exists "delivery_analytics2"."analytics"."delays_by_city_type__dbt_backup" cascade
[0m14:03:35.734336 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.000 seconds
[0m14:03:35.735936 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: Close
[0m14:03:35.736633 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67d17801-09ac-432a-81a9-60422350c395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDDA7BBD10>]}
[0m14:03:35.737245 [info ] [Thread-4 (]: 6 of 6 OK created sql view model analytics.delays_by_city_type ................. [[32mCREATE VIEW[0m in 0.11s]
[0m14:03:35.738182 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.delays_by_city_type
[0m14:03:35.740111 [debug] [MainThread]: Using postgres connection "master"
[0m14:03:35.740562 [debug] [MainThread]: On master: BEGIN
[0m14:03:35.740945 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:03:35.801149 [debug] [MainThread]: SQL status: BEGIN in 0.060 seconds
[0m14:03:35.801668 [debug] [MainThread]: On master: COMMIT
[0m14:03:35.801968 [debug] [MainThread]: Using postgres connection "master"
[0m14:03:35.802243 [debug] [MainThread]: On master: COMMIT
[0m14:03:35.802725 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:03:35.803116 [debug] [MainThread]: On master: Close
[0m14:03:35.803690 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:03:35.803985 [debug] [MainThread]: Connection 'list_delivery_analytics2' was properly closed.
[0m14:03:35.804229 [debug] [MainThread]: Connection 'list_delivery_analytics2_analytics' was properly closed.
[0m14:03:35.804628 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipments_by_time' was properly closed.
[0m14:03:35.805002 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.couriers_performance' was properly closed.
[0m14:03:35.805319 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.courier_load' was properly closed.
[0m14:03:35.805722 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.delays_by_city_type' was properly closed.
[0m14:03:35.806311 [info ] [MainThread]: 
[0m14:03:35.806931 [info ] [MainThread]: Finished running 2 table models, 4 view models in 0 hours 0 minutes and 1.15 seconds (1.15s).
[0m14:03:35.808700 [debug] [MainThread]: Command end result
[0m14:03:35.834269 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:03:35.837261 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:03:35.845655 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m14:03:35.846055 [info ] [MainThread]: 
[0m14:03:35.846589 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:03:35.847084 [info ] [MainThread]: 
[0m14:03:35.847628 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m14:03:35.848703 [debug] [MainThread]: Command `dbt run` succeeded at 14:03:35.848586 after 3.05 seconds
[0m14:03:35.849069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDD750AB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDDA300350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CDDA7B9BB0>]}
[0m14:03:35.849439 [debug] [MainThread]: Flushing usage events
[0m14:03:36.881864 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:09:56.616148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568C684980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568D13CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568D77FD90>]}


============================== 14:09:56.622076 | 143876d6-b613-42ee-abfb-1c92a4ee0f24 ==============================
[0m14:09:56.622076 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:09:56.623421 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run --select views', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:09:56.905132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '143876d6-b613-42ee-abfb-1c92a4ee0f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568CAF0770>]}
[0m14:09:57.030732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '143876d6-b613-42ee-abfb-1c92a4ee0f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568D622E00>]}
[0m14:09:57.032663 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:09:57.552313 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:09:57.778340 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 3 files added, 0 files changed.
[0m14:09:57.779139 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\views\shipment_delays.sql
[0m14:09:57.779593 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\views\active_delays.sql
[0m14:09:57.780002 [debug] [MainThread]: Partial parsing: added file: dbt_delivery_analytics://models\views\shipment_status.sql
[0m14:09:58.269834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '143876d6-b613-42ee-abfb-1c92a4ee0f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568F125550>]}
[0m14:09:58.407870 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:09:58.413846 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:09:58.528375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '143876d6-b613-42ee-abfb-1c92a4ee0f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568D613D40>]}
[0m14:09:58.529187 [info ] [MainThread]: Found 20 models, 5 data tests, 3 sources, 459 macros
[0m14:09:58.529915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '143876d6-b613-42ee-abfb-1c92a4ee0f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568EE64E50>]}
[0m14:09:58.532702 [info ] [MainThread]: 
[0m14:09:58.533463 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:09:58.534187 [info ] [MainThread]: 
[0m14:09:58.535066 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:09:58.542565 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2'
[0m14:09:58.691262 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2"
[0m14:09:58.691829 [debug] [ThreadPool]: On list_delivery_analytics2: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2"} */

    select distinct nspname from pg_namespace
  
[0m14:09:58.692325 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:09:58.771140 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.079 seconds
[0m14:09:58.773594 [debug] [ThreadPool]: On list_delivery_analytics2: Close
[0m14:09:58.777534 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2_analytics'
[0m14:09:58.785809 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m14:09:58.786454 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: BEGIN
[0m14:09:58.787116 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:09:58.852588 [debug] [ThreadPool]: SQL status: BEGIN in 0.065 seconds
[0m14:09:58.853218 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m14:09:58.853683 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2_analytics"} */
select
      'delivery_analytics2' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m14:09:58.862580 [debug] [ThreadPool]: SQL status: SELECT 12 in 0.008 seconds
[0m14:09:58.864470 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: ROLLBACK
[0m14:09:58.865533 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: Close
[0m14:09:58.877323 [debug] [MainThread]: Using postgres connection "master"
[0m14:09:58.877996 [debug] [MainThread]: On master: BEGIN
[0m14:09:58.878502 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:09:58.945833 [debug] [MainThread]: SQL status: BEGIN in 0.067 seconds
[0m14:09:58.946394 [debug] [MainThread]: Using postgres connection "master"
[0m14:09:58.946893 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:09:58.957431 [debug] [MainThread]: SQL status: SELECT 7 in 0.010 seconds
[0m14:09:58.961027 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '143876d6-b613-42ee-abfb-1c92a4ee0f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568F44F930>]}
[0m14:09:58.961590 [debug] [MainThread]: On master: ROLLBACK
[0m14:09:58.962295 [debug] [MainThread]: Using postgres connection "master"
[0m14:09:58.962692 [debug] [MainThread]: On master: BEGIN
[0m14:09:58.963571 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m14:09:58.964078 [debug] [MainThread]: On master: COMMIT
[0m14:09:58.964570 [debug] [MainThread]: Using postgres connection "master"
[0m14:09:58.964995 [debug] [MainThread]: On master: COMMIT
[0m14:09:58.965728 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:09:58.966138 [debug] [MainThread]: On master: Close
[0m14:09:58.972580 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.delays_by_city_type
[0m14:09:58.973401 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_load
[0m14:09:58.974044 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.active_delays
[0m14:09:58.974538 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.couriers_performance
[0m14:09:58.975214 [info ] [Thread-4 (]: 4 of 7 START sql view model analytics.delays_by_city_type ...................... [RUN]
[0m14:09:58.975983 [info ] [Thread-2 (]: 2 of 7 START sql view model analytics.courier_load ............................. [RUN]
[0m14:09:58.978541 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.delays_by_city_type'
[0m14:09:58.979295 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_load'
[0m14:09:58.976875 [info ] [Thread-1 (]: 1 of 7 START sql view model analytics.active_delays ............................ [RUN]
[0m14:09:58.979980 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.delays_by_city_type
[0m14:09:58.980618 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_load
[0m14:09:58.977620 [info ] [Thread-3 (]: 3 of 7 START sql view model analytics.couriers_performance ..................... [RUN]
[0m14:09:58.981441 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.active_delays'
[0m14:09:58.993974 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_load"
[0m14:09:58.995459 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:09:58.996446 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.couriers_performance'
[0m14:09:58.997203 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.active_delays
[0m14:09:58.998356 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.couriers_performance
[0m14:09:59.002833 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.active_delays"
[0m14:09:59.009467 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.couriers_performance"
[0m14:09:59.011289 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.delays_by_city_type
[0m14:09:59.012131 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_load
[0m14:09:59.085963 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:09:59.087454 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_load"
[0m14:09:59.088840 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.couriers_performance
[0m14:09:59.095300 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.couriers_performance"
[0m14:09:59.096011 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.active_delays
[0m14:09:59.100149 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.active_delays"
[0m14:09:59.101847 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:09:59.102402 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: BEGIN
[0m14:09:59.102909 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:09:59.103589 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:09:59.104771 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: BEGIN
[0m14:09:59.105607 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:09:59.106427 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:09:59.107089 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:09:59.107835 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: BEGIN
[0m14:09:59.108879 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: BEGIN
[0m14:09:59.109592 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:09:59.110187 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:09:59.194846 [debug] [Thread-2 (]: SQL status: BEGIN in 0.092 seconds
[0m14:09:59.195730 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:09:59.196484 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */

  create view "delivery_analytics2"."analytics"."courier_load__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    courier_id,
    shipment_date,
    count(*) as total_shipments_per_day,
    count(case when current_status != 'delivered' then 1 end) as active_shipments
from shipments
group by courier_id, shipment_date
order by courier_id, shipment_date

/*
VIEW: courier_load

Svrha:
- Analiza radnog opterećenja kurira po danu
- active_shipments = pošiljke koje nisu još dostavljene
*/
  );
[0m14:09:59.201507 [debug] [Thread-4 (]: SQL status: BEGIN in 0.095 seconds
[0m14:09:59.202405 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:09:59.203180 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */

  create view "delivery_analytics2"."analytics"."delays_by_city_type__dbt_tmp"
    
    
  as (
    with delays as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipment_delays"
)

select
    delivery_city,
    delivery_type,
    count(*) as total_shipments,
    count(case when is_late_delivery = false then 1 end) as on_time_shipments,
    avg(delay_days) as avg_delay_days
from delays
group by delivery_city, delivery_type
order by delivery_city, delivery_type

/*
VIEW: delays_by_city_type

Svrha:
- Agregacija kašnjenja po delivery_city i delivery_type
- Metričke kolone: total_shipments, on_time_shipments, avg_delay_days
*/
  );
[0m14:09:59.210168 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.013 seconds
[0m14:09:59.223483 [debug] [Thread-1 (]: SQL status: BEGIN in 0.113 seconds
[0m14:09:59.224320 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.020 seconds
[0m14:09:59.224956 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:09:59.225461 [debug] [Thread-3 (]: SQL status: BEGIN in 0.116 seconds
[0m14:09:59.226063 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:09:59.230743 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:09:59.231368 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */
alter table "delivery_analytics2"."analytics"."courier_load" rename to "courier_load__dbt_backup"
[0m14:09:59.231889 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:09:59.232620 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.active_delays"} */

  create view "delivery_analytics2"."analytics"."active_delays__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
    where current_status != 'delivered'
)

select
    shipment_id,
    courier_id,
    delivery_city,
    delivery_type,
    shipment_date,
    expected_delivery_date,
    current_status,
    (current_date - expected_delivery_date) as delay_days
from shipments
where current_date > expected_delivery_date
order by delay_days desc

/*
VIEW: active_delays
Svrha:
- Prikazuje sve aktivne pošiljke koje kasne
- Metričke kolone: delay_days
- Koristi se za monitoring kašnjenja i upozorenja
*/
  );
[0m14:09:59.233406 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */
alter table "delivery_analytics2"."analytics"."delays_by_city_type" rename to "delays_by_city_type__dbt_backup"
[0m14:09:59.234461 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */

  create view "delivery_analytics2"."analytics"."couriers_performance__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    courier_id,
    count(*) as total_shipments,
    count(case when current_status = 'delivered' then 1 end) as delivered_shipments,
    avg(delivery_duration_days) as avg_delivery_duration_days,
    sum(case when delivery_type = 'express' then 1 else 0 end) as express_shipments_count,
    avg(distance_km) as avg_distance_per_shipment,
    sum(distance_km) as total_distance_per_courier,
    (count(case when is_late_delivery = false then 1 end)::float / nullif(count(*),0))*100 as success_rate_percent,
    count(case when current_status != 'delivered' then 1 end) as pending_shipments
from shipments
group by courier_id
order by courier_id

/*
VIEW: couriers_performance

Svrha:
- Prikazuje performanse kurira
- Metričke kolone: total_shipments, delivered_shipments, avg_delivery_duration_days,
  express_shipments_count, avg_distance_per_shipment, total_distance_per_courier, success_rate_percent, pending_shipments
*/
  );
[0m14:09:59.235429 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:09:59.240596 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:09:59.241387 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m14:09:59.242037 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */
alter table "delivery_analytics2"."analytics"."courier_load__dbt_tmp" rename to "courier_load"
[0m14:09:59.246660 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:09:59.247714 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.011 seconds
[0m14:09:59.248474 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:09:59.248964 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.013 seconds
[0m14:09:59.249476 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */
alter table "delivery_analytics2"."analytics"."delays_by_city_type__dbt_tmp" rename to "delays_by_city_type"
[0m14:09:59.254690 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:09:59.285780 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: COMMIT
[0m14:09:59.290862 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:09:59.291674 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.active_delays"} */
alter table "delivery_analytics2"."analytics"."active_delays__dbt_tmp" rename to "active_delays"
[0m14:09:59.292254 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:09:59.292758 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:09:59.293256 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */
alter table "delivery_analytics2"."analytics"."couriers_performance" rename to "couriers_performance__dbt_backup"
[0m14:09:59.293860 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: COMMIT
[0m14:09:59.295867 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: COMMIT
[0m14:09:59.296773 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m14:09:59.297958 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:09:59.298761 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:09:59.301090 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: COMMIT
[0m14:09:59.301604 [debug] [Thread-2 (]: SQL status: COMMIT in 0.004 seconds
[0m14:09:59.302109 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: COMMIT
[0m14:09:59.305768 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:09:59.306699 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:09:59.314730 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics2"."analytics"."courier_load__dbt_backup"
[0m14:09:59.315718 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */
alter table "delivery_analytics2"."analytics"."couriers_performance__dbt_tmp" rename to "couriers_performance"
[0m14:09:59.316540 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: COMMIT
[0m14:09:59.317352 [debug] [Thread-4 (]: SQL status: COMMIT in 0.002 seconds
[0m14:09:59.326053 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:09:59.330440 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics2"."analytics"."delays_by_city_type__dbt_backup"
[0m14:09:59.331176 [debug] [Thread-1 (]: SQL status: COMMIT in 0.004 seconds
[0m14:09:59.331647 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m14:09:59.332173 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */
drop view if exists "delivery_analytics2"."analytics"."courier_load__dbt_backup" cascade
[0m14:09:59.333078 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:09:59.335628 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."active_delays__dbt_backup"
[0m14:09:59.337637 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: COMMIT
[0m14:09:59.338465 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */
drop view if exists "delivery_analytics2"."analytics"."delays_by_city_type__dbt_backup" cascade
[0m14:09:59.339737 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:09:59.340456 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:09:59.341168 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.active_delays"} */
drop view if exists "delivery_analytics2"."analytics"."active_delays__dbt_backup" cascade
[0m14:09:59.341752 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.004 seconds
[0m14:09:59.342354 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: COMMIT
[0m14:09:59.346984 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: Close
[0m14:09:59.348077 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.007 seconds
[0m14:09:59.348661 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.006 seconds
[0m14:09:59.351634 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: Close
[0m14:09:59.352478 [debug] [Thread-3 (]: SQL status: COMMIT in 0.003 seconds
[0m14:09:59.358094 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: Close
[0m14:09:59.373103 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics2"."analytics"."couriers_performance__dbt_backup"
[0m14:09:59.375977 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:09:59.377549 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */
drop view if exists "delivery_analytics2"."analytics"."couriers_performance__dbt_backup" cascade
[0m14:09:59.378375 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '143876d6-b613-42ee-abfb-1c92a4ee0f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568F30BA10>]}
[0m14:09:59.378860 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '143876d6-b613-42ee-abfb-1c92a4ee0f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568F5E1FF0>]}
[0m14:09:59.379414 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '143876d6-b613-42ee-abfb-1c92a4ee0f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568F5E20A0>]}
[0m14:09:59.381450 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.003 seconds
[0m14:09:59.380653 [info ] [Thread-2 (]: 2 of 7 OK created sql view model analytics.courier_load ........................ [[32mCREATE VIEW[0m in 0.37s]
[0m14:09:59.382345 [info ] [Thread-4 (]: 4 of 7 OK created sql view model analytics.delays_by_city_type ................. [[32mCREATE VIEW[0m in 0.40s]
[0m14:09:59.385767 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: Close
[0m14:09:59.383463 [info ] [Thread-1 (]: 1 of 7 OK created sql view model analytics.active_delays ....................... [[32mCREATE VIEW[0m in 0.40s]
[0m14:09:59.387005 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_load
[0m14:09:59.388203 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.delays_by_city_type
[0m14:09:59.389327 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.active_delays
[0m14:09:59.390299 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '143876d6-b613-42ee-abfb-1c92a4ee0f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568F5423D0>]}
[0m14:09:59.391232 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.shipment_delays
[0m14:09:59.392314 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.shipment_status
[0m14:09:59.393278 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.shipments_by_time
[0m14:09:59.394595 [info ] [Thread-3 (]: 3 of 7 OK created sql view model analytics.couriers_performance ................ [[32mCREATE VIEW[0m in 0.39s]
[0m14:09:59.395514 [info ] [Thread-2 (]: 5 of 7 START sql view model analytics.shipment_delays .......................... [RUN]
[0m14:09:59.398735 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.couriers_performance
[0m14:09:59.396507 [info ] [Thread-4 (]: 6 of 7 START sql view model analytics.shipment_status .......................... [RUN]
[0m14:09:59.399677 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.courier_load, now model.dbt_delivery_analytics.shipment_delays)
[0m14:09:59.397619 [info ] [Thread-1 (]: 7 of 7 START sql view model analytics.shipments_by_time ........................ [RUN]
[0m14:09:59.401143 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.delays_by_city_type, now model.dbt_delivery_analytics.shipment_status)
[0m14:09:59.401904 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.shipment_delays
[0m14:09:59.402745 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.active_delays, now model.dbt_delivery_analytics.shipments_by_time)
[0m14:09:59.403424 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.shipment_status
[0m14:09:59.408781 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipment_delays"
[0m14:09:59.409798 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_time
[0m14:09:59.412923 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipment_status"
[0m14:09:59.416670 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_time"
[0m14:09:59.419557 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.shipment_delays
[0m14:09:59.420302 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.shipment_status
[0m14:09:59.426714 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipment_delays"
[0m14:09:59.427733 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.shipments_by_time
[0m14:09:59.432668 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipment_status"
[0m14:09:59.437212 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_time"
[0m14:09:59.439506 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_delays"
[0m14:09:59.440675 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: BEGIN
[0m14:09:59.441611 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:09:59.442488 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:09:59.443517 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: BEGIN
[0m14:09:59.444306 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:09:59.445005 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:09:59.445639 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: BEGIN
[0m14:09:59.446387 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:09:59.530489 [debug] [Thread-2 (]: SQL status: BEGIN in 0.089 seconds
[0m14:09:59.531199 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_delays"
[0m14:09:59.531731 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_delays"} */

  create view "delivery_analytics2"."analytics"."shipment_delays__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    shipment_id,
    courier_id,
    delivery_city,
    delivery_type,
    shipment_date,
    expected_delivery_date,
    completed_at,
    case
        when completed_at is not null then (completed_at - expected_delivery_date)
        else (current_date - expected_delivery_date)
    end as delay_days,
    case
        when completed_at > expected_delivery_date then true
        else false
    end as is_late_delivery
from shipments
order by delay_days desc

/*
VIEW: fct_shipment_delays
Svrha:
- Kašnjenje po pošiljci
- Metričke kolone: delay_days, is_late_delivery
- Može se agregirati po gradu, tipu pošiljke, kuriru
*/
  );
[0m14:09:59.534585 [debug] [Thread-2 (]: Postgres adapter: Postgres error: CASE types integer and interval cannot be matched
LINE 21:         when completed_at is not null then (completed_at - e...
                                                     ^

[0m14:09:59.535240 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: ROLLBACK
[0m14:09:59.536047 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: Close
[0m14:09:59.541404 [debug] [Thread-4 (]: SQL status: BEGIN in 0.096 seconds
[0m14:09:59.542440 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:09:59.543198 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_status"} */

  create view "delivery_analytics2"."analytics"."shipment_status__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    current_status,
    count(*) as total_shipments,
    sum(case when current_status != 'delivered' then 1 else 0 end) as active_shipments
from shipments
group by current_status

/*
VIEW: shipment_status
Svrha:
- Prikazuje broj pošiljki po statusu: pending, picked_up, delivered
- Koristi se za monitoring opterećenja i trenutnog statusa pošiljki
*/
  );
[0m14:09:59.552742 [debug] [Thread-2 (]: Database Error in model shipment_delays (models\views\shipment_delays.sql)
  CASE types integer and interval cannot be matched
  LINE 21:         when completed_at is not null then (completed_at - e...
                                                       ^
  compiled code at target\run\dbt_delivery_analytics\models\views\shipment_delays.sql
[0m14:09:59.553472 [debug] [Thread-1 (]: SQL status: BEGIN in 0.107 seconds
[0m14:09:59.554093 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '143876d6-b613-42ee-abfb-1c92a4ee0f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568F769A20>]}
[0m14:09:59.554749 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:09:59.555328 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.012 seconds
[0m14:09:59.557505 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */

  create view "delivery_analytics2"."analytics"."shipments_by_time__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
    where shipment_date is not null
)

select
    date_trunc('day', shipment_date) as day,
    date_trunc('week', shipment_date) as week,
    date_trunc('month', shipment_date) as month,
    delivery_city,
    courier_id,
    delivery_type,
    count(*) as total_shipments,
    count(case when is_late_delivery = false then 1 end) as on_time_shipments,
    avg(delivery_duration_days) as avg_delivery_days,
    sum(price) as total_revenue,
    sum(distance_km) as total_distance

from shipments
group by 1,2,3,4,5,6
order by day, delivery_city, courier_id, delivery_type

/*
VIEW: shipments_by_time

Svrha:
- Analiza pošiljki po danu, sedmici i mjesecu
- Segmentacija po delivery_city, courier_id, delivery_type (standard/express)
- Metričke kolone: total_shipments, on_time_shipments, avg_delivery_days, total_revenue, total_distance
- Može se koristiti za trend analizu i dashboard vizualizacije
*/
  );
[0m14:09:59.556450 [error] [Thread-2 (]: 5 of 7 ERROR creating sql view model analytics.shipment_delays ................. [[31mERROR[0m in 0.15s]
[0m14:09:59.561947 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:09:59.563044 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.shipment_delays
[0m14:09:59.563712 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_status"} */
alter table "delivery_analytics2"."analytics"."shipment_status__dbt_tmp" rename to "shipment_status"
[0m14:09:59.564564 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.shipment_delays' to be skipped because of status 'error'.  Reason: Database Error in model shipment_delays (models\views\shipment_delays.sql)
  CASE types integer and interval cannot be matched
  LINE 21:         when completed_at is not null then (completed_at - e...
                                                       ^
  compiled code at target\run\dbt_delivery_analytics\models\views\shipment_delays.sql.
[0m14:09:59.566116 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:09:59.568711 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: COMMIT
[0m14:09:59.569442 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:09:59.570129 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: COMMIT
[0m14:09:59.572182 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m14:09:59.575601 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics2"."analytics"."shipment_status__dbt_backup"
[0m14:09:59.576165 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.014 seconds
[0m14:09:59.577260 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:09:59.581790 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:09:59.582615 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_status"} */
drop view if exists "delivery_analytics2"."analytics"."shipment_status__dbt_backup" cascade
[0m14:09:59.583395 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */
alter table "delivery_analytics2"."analytics"."shipments_by_time" rename to "shipments_by_time__dbt_backup"
[0m14:09:59.584935 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.001 seconds
[0m14:09:59.585777 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:09:59.588109 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: Close
[0m14:09:59.593241 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:09:59.594171 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */
alter table "delivery_analytics2"."analytics"."shipments_by_time__dbt_tmp" rename to "shipments_by_time"
[0m14:09:59.595066 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '143876d6-b613-42ee-abfb-1c92a4ee0f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568F6CD730>]}
[0m14:09:59.596840 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:09:59.596165 [info ] [Thread-4 (]: 6 of 7 OK created sql view model analytics.shipment_status ..................... [[32mCREATE VIEW[0m in 0.19s]
[0m14:09:59.599595 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: COMMIT
[0m14:09:59.600449 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.shipment_status
[0m14:09:59.601016 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:09:59.601883 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: COMMIT
[0m14:09:59.604374 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m14:09:59.608139 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."shipments_by_time__dbt_backup"
[0m14:09:59.609386 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:09:59.609999 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */
drop view if exists "delivery_analytics2"."analytics"."shipments_by_time__dbt_backup" cascade
[0m14:09:59.613679 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.003 seconds
[0m14:09:59.615662 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: Close
[0m14:09:59.616896 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '143876d6-b613-42ee-abfb-1c92a4ee0f24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568F366330>]}
[0m14:09:59.618053 [info ] [Thread-1 (]: 7 of 7 OK created sql view model analytics.shipments_by_time ................... [[32mCREATE VIEW[0m in 0.21s]
[0m14:09:59.619401 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.shipments_by_time
[0m14:09:59.622090 [debug] [MainThread]: Using postgres connection "master"
[0m14:09:59.623049 [debug] [MainThread]: On master: BEGIN
[0m14:09:59.623851 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:09:59.696402 [debug] [MainThread]: SQL status: BEGIN in 0.072 seconds
[0m14:09:59.697048 [debug] [MainThread]: On master: COMMIT
[0m14:09:59.697522 [debug] [MainThread]: Using postgres connection "master"
[0m14:09:59.697982 [debug] [MainThread]: On master: COMMIT
[0m14:09:59.698619 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:09:59.699016 [debug] [MainThread]: On master: Close
[0m14:09:59.699772 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:09:59.700353 [debug] [MainThread]: Connection 'list_delivery_analytics2' was properly closed.
[0m14:09:59.700925 [debug] [MainThread]: Connection 'list_delivery_analytics2_analytics' was properly closed.
[0m14:09:59.701391 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipment_status' was properly closed.
[0m14:09:59.701859 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipment_delays' was properly closed.
[0m14:09:59.702383 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipments_by_time' was properly closed.
[0m14:09:59.702932 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.couriers_performance' was properly closed.
[0m14:09:59.703615 [info ] [MainThread]: 
[0m14:09:59.704553 [info ] [MainThread]: Finished running 7 view models in 0 hours 0 minutes and 1.17 seconds (1.17s).
[0m14:09:59.708107 [debug] [MainThread]: Command end result
[0m14:09:59.754719 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:09:59.760793 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:09:59.771428 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m14:09:59.772092 [info ] [MainThread]: 
[0m14:09:59.773279 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:09:59.774305 [info ] [MainThread]: 
[0m14:09:59.775094 [error] [MainThread]: [31mFailure in model shipment_delays (models\views\shipment_delays.sql)[0m
[0m14:09:59.775813 [error] [MainThread]:   Database Error in model shipment_delays (models\views\shipment_delays.sql)
  CASE types integer and interval cannot be matched
  LINE 21:         when completed_at is not null then (completed_at - e...
                                                       ^
  compiled code at target\run\dbt_delivery_analytics\models\views\shipment_delays.sql
[0m14:09:59.776557 [info ] [MainThread]: 
[0m14:09:59.777362 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\views\shipment_delays.sql
[0m14:09:59.778116 [info ] [MainThread]: 
[0m14:09:59.778869 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=7
[0m14:09:59.780540 [debug] [MainThread]: Command `dbt run` failed at 14:09:59.780379 after 3.38 seconds
[0m14:09:59.781180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568C33AF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568F6FFB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002568F6FFEF0>]}
[0m14:09:59.782014 [debug] [MainThread]: Flushing usage events
[0m14:10:01.013588 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:14:41.446225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FA87C8980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FA927CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FA98BFD90>]}


============================== 14:14:41.453173 | 3b4aef24-38f2-412e-9fbc-9eecb78c2f8c ==============================
[0m14:14:41.453173 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:14:41.454490 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'indirect_selection': 'eager', 'use_colors': 'True', 'invocation_command': 'dbt run --select views', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:14:41.759984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3b4aef24-38f2-412e-9fbc-9eecb78c2f8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FA8C30770>]}
[0m14:14:41.854147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3b4aef24-38f2-412e-9fbc-9eecb78c2f8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FA9766E00>]}
[0m14:14:41.856234 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:14:42.443508 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:14:42.674162 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:14:42.674689 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:14:42.754691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3b4aef24-38f2-412e-9fbc-9eecb78c2f8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FAB2ACF50>]}
[0m14:14:42.882390 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:14:42.887598 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:14:42.921148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3b4aef24-38f2-412e-9fbc-9eecb78c2f8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FA9753D40>]}
[0m14:14:42.921874 [info ] [MainThread]: Found 20 models, 5 data tests, 3 sources, 459 macros
[0m14:14:42.922728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3b4aef24-38f2-412e-9fbc-9eecb78c2f8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FA98C60B0>]}
[0m14:14:42.925433 [info ] [MainThread]: 
[0m14:14:42.926188 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:14:42.926912 [info ] [MainThread]: 
[0m14:14:42.927886 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:14:42.934829 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2'
[0m14:14:43.126159 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2"
[0m14:14:43.126690 [debug] [ThreadPool]: On list_delivery_analytics2: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2"} */

    select distinct nspname from pg_namespace
  
[0m14:14:43.127095 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:14:43.269873 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.143 seconds
[0m14:14:43.271724 [debug] [ThreadPool]: On list_delivery_analytics2: Close
[0m14:14:43.276322 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2_analytics'
[0m14:14:43.284228 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m14:14:43.284792 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: BEGIN
[0m14:14:43.285211 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:14:43.352798 [debug] [ThreadPool]: SQL status: BEGIN in 0.067 seconds
[0m14:14:43.353353 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m14:14:43.353763 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2_analytics"} */
select
      'delivery_analytics2' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m14:14:43.362326 [debug] [ThreadPool]: SQL status: SELECT 14 in 0.008 seconds
[0m14:14:43.365010 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: ROLLBACK
[0m14:14:43.365681 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: Close
[0m14:14:43.376190 [debug] [MainThread]: Using postgres connection "master"
[0m14:14:43.376720 [debug] [MainThread]: On master: BEGIN
[0m14:14:43.377076 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:14:43.442424 [debug] [MainThread]: SQL status: BEGIN in 0.065 seconds
[0m14:14:43.442979 [debug] [MainThread]: Using postgres connection "master"
[0m14:14:43.443465 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:14:43.454452 [debug] [MainThread]: SQL status: SELECT 9 in 0.010 seconds
[0m14:14:43.458076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3b4aef24-38f2-412e-9fbc-9eecb78c2f8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FAB694D50>]}
[0m14:14:43.458810 [debug] [MainThread]: On master: ROLLBACK
[0m14:14:43.459594 [debug] [MainThread]: Using postgres connection "master"
[0m14:14:43.460061 [debug] [MainThread]: On master: BEGIN
[0m14:14:43.461212 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m14:14:43.461957 [debug] [MainThread]: On master: COMMIT
[0m14:14:43.462332 [debug] [MainThread]: Using postgres connection "master"
[0m14:14:43.462674 [debug] [MainThread]: On master: COMMIT
[0m14:14:43.463303 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:14:43.463721 [debug] [MainThread]: On master: Close
[0m14:14:43.470829 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.couriers_performance
[0m14:14:43.471488 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.delays_by_city_type
[0m14:14:43.472098 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.active_delays
[0m14:14:43.472687 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_load
[0m14:14:43.473395 [info ] [Thread-3 (]: 3 of 7 START sql view model analytics.couriers_performance ..................... [RUN]
[0m14:14:43.474489 [info ] [Thread-4 (]: 4 of 7 START sql view model analytics.delays_by_city_type ...................... [RUN]
[0m14:14:43.475365 [info ] [Thread-1 (]: 1 of 7 START sql view model analytics.active_delays ............................ [RUN]
[0m14:14:43.477072 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.couriers_performance'
[0m14:14:43.477927 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.delays_by_city_type'
[0m14:14:43.476133 [info ] [Thread-2 (]: 2 of 7 START sql view model analytics.courier_load ............................. [RUN]
[0m14:14:43.478806 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.active_delays'
[0m14:14:43.479436 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.couriers_performance
[0m14:14:43.479969 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.delays_by_city_type
[0m14:14:43.480670 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_load'
[0m14:14:43.481265 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.active_delays
[0m14:14:43.492996 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.couriers_performance"
[0m14:14:43.496170 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:14:43.496837 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_load
[0m14:14:43.501750 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.active_delays"
[0m14:14:43.505708 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_load"
[0m14:14:43.508350 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.delays_by_city_type
[0m14:14:43.532533 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.couriers_performance
[0m14:14:43.648501 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.couriers_performance"
[0m14:14:43.650191 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:14:43.650949 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.active_delays
[0m14:14:43.651577 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_load
[0m14:14:43.656307 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.active_delays"
[0m14:14:43.661173 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_load"
[0m14:14:43.662431 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:14:43.663385 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: BEGIN
[0m14:14:43.664170 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:14:43.664728 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:14:43.665400 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: BEGIN
[0m14:14:43.666709 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:14:43.667499 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:14:43.668053 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:14:43.668828 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: BEGIN
[0m14:14:43.669348 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: BEGIN
[0m14:14:43.669932 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:14:43.670521 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:14:43.752003 [debug] [Thread-3 (]: SQL status: BEGIN in 0.087 seconds
[0m14:14:43.752618 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:14:43.753200 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */

  create view "delivery_analytics2"."analytics"."couriers_performance__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    courier_id,
    count(*) as total_shipments,
    count(case when current_status = 'delivered' then 1 end) as delivered_shipments,
    avg(delivery_duration_days) as avg_delivery_duration_days,
    sum(case when delivery_type = 'express' then 1 else 0 end) as express_shipments_count,
    avg(distance_km) as avg_distance_per_shipment,
    sum(distance_km) as total_distance_per_courier,
    (count(case when is_late_delivery = false then 1 end)::float / nullif(count(*),0))*100 as success_rate_percent,
    count(case when current_status != 'delivered' then 1 end) as pending_shipments
from shipments
group by courier_id
order by courier_id

/*
VIEW: couriers_performance

Svrha:
- Prikazuje performanse kurira
- Metričke kolone: total_shipments, delivered_shipments, avg_delivery_duration_days,
  express_shipments_count, avg_distance_per_shipment, total_distance_per_courier, success_rate_percent, pending_shipments
*/
  );
[0m14:14:43.762164 [debug] [Thread-4 (]: SQL status: BEGIN in 0.095 seconds
[0m14:14:43.762914 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:14:43.763511 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */

  create view "delivery_analytics2"."analytics"."delays_by_city_type__dbt_tmp"
    
    
  as (
    with delays as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipment_delays"
)

select
    delivery_city,
    delivery_type,
    count(*) as total_shipments,
    count(case when is_late_delivery = false then 1 end) as on_time_shipments,
    avg(delay_days) as avg_delay_days
from delays
group by delivery_city, delivery_type
order by delivery_city, delivery_type

/*
VIEW: delays_by_city_type

Svrha:
- Agregacija kašnjenja po delivery_city i delivery_type
- Metričke kolone: total_shipments, on_time_shipments, avg_delay_days
*/
  );
[0m14:14:43.767753 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.014 seconds
[0m14:14:43.779849 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:14:43.780508 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.016 seconds
[0m14:14:43.781173 [debug] [Thread-1 (]: SQL status: BEGIN in 0.111 seconds
[0m14:14:43.781968 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */
alter table "delivery_analytics2"."analytics"."couriers_performance" rename to "couriers_performance__dbt_backup"
[0m14:14:43.782778 [debug] [Thread-2 (]: SQL status: BEGIN in 0.113 seconds
[0m14:14:43.788925 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:14:43.789564 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:14:43.790238 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:14:43.790775 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */
alter table "delivery_analytics2"."analytics"."delays_by_city_type" rename to "delays_by_city_type__dbt_backup"
[0m14:14:43.791390 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:14:43.792003 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.active_delays"} */

  create view "delivery_analytics2"."analytics"."active_delays__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
    where current_status != 'delivered'
)

select
    shipment_id,
    courier_id,
    delivery_city,
    delivery_type,
    shipment_date,
    expected_delivery_date,
    current_status,
    (current_date - expected_delivery_date) as delay_days
from shipments
where current_date > expected_delivery_date
order by delay_days desc

/*
VIEW: active_delays
Svrha:
- Prikazuje sve aktivne pošiljke koje kasne
- Metričke kolone: delay_days
- Koristi se za monitoring kašnjenja i upozorenja
*/
  );
[0m14:14:43.792692 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */

  create view "delivery_analytics2"."analytics"."courier_load__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    courier_id,
    shipment_date,
    count(*) as total_shipments_per_day,
    count(case when current_status != 'delivered' then 1 end) as active_shipments
from shipments
group by courier_id, shipment_date
order by courier_id, shipment_date

/*
VIEW: courier_load

Svrha:
- Analiza radnog opterećenja kurira po danu
- active_shipments = pošiljke koje nisu još dostavljene
*/
  );
[0m14:14:43.796349 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:14:43.796994 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m14:14:43.797873 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */
alter table "delivery_analytics2"."analytics"."couriers_performance__dbt_tmp" rename to "couriers_performance"
[0m14:14:43.803315 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:14:43.804427 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */
alter table "delivery_analytics2"."analytics"."delays_by_city_type__dbt_tmp" rename to "delays_by_city_type"
[0m14:14:43.805149 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:14:43.813632 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.008 seconds
[0m14:14:43.814389 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.017 seconds
[0m14:14:43.830851 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: COMMIT
[0m14:14:43.832983 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: COMMIT
[0m14:14:43.833636 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.036 seconds
[0m14:14:43.837583 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:14:43.838202 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:14:43.838983 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:14:43.843215 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:14:43.843846 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */
alter table "delivery_analytics2"."analytics"."courier_load" rename to "courier_load__dbt_backup"
[0m14:14:43.844562 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: COMMIT
[0m14:14:43.845515 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: COMMIT
[0m14:14:43.846177 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.active_delays"} */
alter table "delivery_analytics2"."analytics"."active_delays" rename to "active_delays__dbt_backup"
[0m14:14:43.847959 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:14:43.848539 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:14:43.853121 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:14:43.853792 [debug] [Thread-3 (]: SQL status: COMMIT in 0.007 seconds
[0m14:14:43.854574 [debug] [Thread-4 (]: SQL status: COMMIT in 0.007 seconds
[0m14:14:43.858082 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:14:43.858957 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */
alter table "delivery_analytics2"."analytics"."courier_load__dbt_tmp" rename to "courier_load"
[0m14:14:43.868567 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics2"."analytics"."couriers_performance__dbt_backup"
[0m14:14:43.871554 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics2"."analytics"."delays_by_city_type__dbt_backup"
[0m14:14:43.872570 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.active_delays"} */
alter table "delivery_analytics2"."analytics"."active_delays__dbt_tmp" rename to "active_delays"
[0m14:14:43.881437 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:14:43.882246 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.009 seconds
[0m14:14:43.883369 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:14:43.884255 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */
drop view if exists "delivery_analytics2"."analytics"."couriers_performance__dbt_backup" cascade
[0m14:14:43.886285 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: COMMIT
[0m14:14:43.886985 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m14:14:43.887653 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */
drop view if exists "delivery_analytics2"."analytics"."delays_by_city_type__dbt_backup" cascade
[0m14:14:43.888565 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:14:43.890708 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: COMMIT
[0m14:14:43.891347 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.003 seconds
[0m14:14:43.892048 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: COMMIT
[0m14:14:43.892603 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:14:43.896037 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: Close
[0m14:14:43.896660 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.005 seconds
[0m14:14:43.897344 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: COMMIT
[0m14:14:43.899607 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: Close
[0m14:14:43.900377 [debug] [Thread-2 (]: SQL status: COMMIT in 0.003 seconds
[0m14:14:43.905225 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m14:14:43.907860 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics2"."analytics"."courier_load__dbt_backup"
[0m14:14:43.911068 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."active_delays__dbt_backup"
[0m14:14:43.912287 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:14:43.913763 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:14:43.914411 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */
drop view if exists "delivery_analytics2"."analytics"."courier_load__dbt_backup" cascade
[0m14:14:43.915095 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.active_delays"} */
drop view if exists "delivery_analytics2"."analytics"."active_delays__dbt_backup" cascade
[0m14:14:43.915674 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3b4aef24-38f2-412e-9fbc-9eecb78c2f8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FAB387A10>]}
[0m14:14:43.916324 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3b4aef24-38f2-412e-9fbc-9eecb78c2f8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FAB3D8520>]}
[0m14:14:43.917694 [info ] [Thread-3 (]: 3 of 7 OK created sql view model analytics.couriers_performance ................ [[32mCREATE VIEW[0m in 0.42s]
[0m14:14:43.919767 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.couriers_performance
[0m14:14:43.918697 [info ] [Thread-4 (]: 4 of 7 OK created sql view model analytics.delays_by_city_type ................. [[32mCREATE VIEW[0m in 0.43s]
[0m14:14:43.920549 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.004 seconds
[0m14:14:43.921266 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.shipment_delays
[0m14:14:43.921829 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.005 seconds
[0m14:14:43.922877 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.delays_by_city_type
[0m14:14:43.925215 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: Close
[0m14:14:43.928675 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: Close
[0m14:14:43.926087 [info ] [Thread-3 (]: 5 of 7 START sql view model analytics.shipment_delays .......................... [RUN]
[0m14:14:43.929502 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.shipment_status
[0m14:14:43.930515 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.couriers_performance, now model.dbt_delivery_analytics.shipment_delays)
[0m14:14:43.931209 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3b4aef24-38f2-412e-9fbc-9eecb78c2f8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FAAF5BE30>]}
[0m14:14:43.932088 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3b4aef24-38f2-412e-9fbc-9eecb78c2f8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FAB766C30>]}
[0m14:14:43.933784 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.shipment_delays
[0m14:14:43.932972 [info ] [Thread-4 (]: 6 of 7 START sql view model analytics.shipment_status .......................... [RUN]
[0m14:14:43.935182 [info ] [Thread-2 (]: 2 of 7 OK created sql view model analytics.courier_load ........................ [[32mCREATE VIEW[0m in 0.45s]
[0m14:14:43.939992 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipment_delays"
[0m14:14:43.940772 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.delays_by_city_type, now model.dbt_delivery_analytics.shipment_status)
[0m14:14:43.936271 [info ] [Thread-1 (]: 1 of 7 OK created sql view model analytics.active_delays ....................... [[32mCREATE VIEW[0m in 0.45s]
[0m14:14:43.942082 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_load
[0m14:14:43.942891 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.shipment_status
[0m14:14:43.944083 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.active_delays
[0m14:14:43.944761 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.shipments_by_time
[0m14:14:43.949686 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipment_status"
[0m14:14:43.951734 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.shipment_delays
[0m14:14:43.951133 [info ] [Thread-2 (]: 7 of 7 START sql view model analytics.shipments_by_time ........................ [RUN]
[0m14:14:43.956567 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipment_delays"
[0m14:14:43.957279 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.courier_load, now model.dbt_delivery_analytics.shipments_by_time)
[0m14:14:43.958213 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_time
[0m14:14:43.958782 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.shipment_status
[0m14:14:43.962014 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_time"
[0m14:14:43.966978 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipment_status"
[0m14:14:43.968844 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_delays"
[0m14:14:43.969638 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipment_delays: BEGIN
[0m14:14:43.970444 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:14:43.971465 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:14:43.972064 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.shipments_by_time
[0m14:14:43.972607 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: BEGIN
[0m14:14:43.977307 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_time"
[0m14:14:43.978047 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:14:43.980218 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:14:43.981295 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_by_time: BEGIN
[0m14:14:43.982076 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:14:44.051376 [debug] [Thread-3 (]: SQL status: BEGIN in 0.081 seconds
[0m14:14:44.052121 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_delays"
[0m14:14:44.052640 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipment_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_delays"} */

  create view "delivery_analytics2"."analytics"."shipment_delays__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    shipment_id,
    courier_id,
    delivery_city,
    delivery_type,
    shipment_date,
    expected_delivery_date,
    completed_at,
    case
        when completed_at is not null then (completed_at - expected_delivery_date)
        else (current_date - expected_delivery_date)
    end as delay_days,
    case
        when completed_at > expected_delivery_date then true
        else false
    end as is_late_delivery
from shipments
order by delay_days desc

/*
VIEW: fct_shipment_delays
Svrha:
- Kašnjenje po pošiljci
- Metričke kolone: delay_days, is_late_delivery
- Može se agregirati po gradu, tipu pošiljke, kuriru
*/
  );
[0m14:14:44.054828 [debug] [Thread-3 (]: Postgres adapter: Postgres error: CASE types integer and interval cannot be matched
LINE 21:         when completed_at is not null then (completed_at - e...
                                                     ^

[0m14:14:44.055415 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipment_delays: ROLLBACK
[0m14:14:44.056347 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipment_delays: Close
[0m14:14:44.060066 [debug] [Thread-4 (]: SQL status: BEGIN in 0.082 seconds
[0m14:14:44.060614 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:14:44.061208 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_status"} */

  create view "delivery_analytics2"."analytics"."shipment_status__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    current_status,
    count(*) as total_shipments,
    sum(case when current_status != 'delivered' then 1 else 0 end) as active_shipments
from shipments
group by current_status

/*
VIEW: shipment_status
Svrha:
- Prikazuje broj pošiljki po statusu: pending, picked_up, delivered
- Koristi se za monitoring opterećenja i trenutnog statusa pošiljki
*/
  );
[0m14:14:44.069464 [debug] [Thread-3 (]: Database Error in model shipment_delays (models\views\shipment_delays.sql)
  CASE types integer and interval cannot be matched
  LINE 21:         when completed_at is not null then (completed_at - e...
                                                       ^
  compiled code at target\run\dbt_delivery_analytics\models\views\shipment_delays.sql
[0m14:14:44.070226 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3b4aef24-38f2-412e-9fbc-9eecb78c2f8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FAB7FA890>]}
[0m14:14:44.071048 [error] [Thread-3 (]: 5 of 7 ERROR creating sql view model analytics.shipment_delays ................. [[31mERROR[0m in 0.14s]
[0m14:14:44.072280 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.shipment_delays
[0m14:14:44.072866 [debug] [Thread-2 (]: SQL status: BEGIN in 0.091 seconds
[0m14:14:44.073545 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.012 seconds
[0m14:14:44.074351 [debug] [Thread-7 (]: Marking all children of 'model.dbt_delivery_analytics.shipment_delays' to be skipped because of status 'error'.  Reason: Database Error in model shipment_delays (models\views\shipment_delays.sql)
  CASE types integer and interval cannot be matched
  LINE 21:         when completed_at is not null then (completed_at - e...
                                                       ^
  compiled code at target\run\dbt_delivery_analytics\models\views\shipment_delays.sql.
[0m14:14:44.075062 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:14:44.079241 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:14:44.080866 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */

  create view "delivery_analytics2"."analytics"."shipments_by_time__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
    where shipment_date is not null
)

select
    date_trunc('day', shipment_date) as day,
    date_trunc('week', shipment_date) as week,
    date_trunc('month', shipment_date) as month,
    delivery_city,
    courier_id,
    delivery_type,
    count(*) as total_shipments,
    count(case when is_late_delivery = false then 1 end) as on_time_shipments,
    avg(delivery_duration_days) as avg_delivery_days,
    sum(price) as total_revenue,
    sum(distance_km) as total_distance

from shipments
group by 1,2,3,4,5,6
order by day, delivery_city, courier_id, delivery_type

/*
VIEW: shipments_by_time

Svrha:
- Analiza pošiljki po danu, sedmici i mjesecu
- Segmentacija po delivery_city, courier_id, delivery_type (standard/express)
- Metričke kolone: total_shipments, on_time_shipments, avg_delivery_days, total_revenue, total_distance
- Može se koristiti za trend analizu i dashboard vizualizacije
*/
  );
[0m14:14:44.081658 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_status"} */
alter table "delivery_analytics2"."analytics"."shipment_status" rename to "shipment_status__dbt_backup"
[0m14:14:44.083329 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:14:44.087534 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:14:44.088130 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_status"} */
alter table "delivery_analytics2"."analytics"."shipment_status__dbt_tmp" rename to "shipment_status"
[0m14:14:44.089773 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:14:44.091868 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: COMMIT
[0m14:14:44.092393 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:14:44.092848 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: COMMIT
[0m14:14:44.094495 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m14:14:44.097787 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics2"."analytics"."shipment_status__dbt_backup"
[0m14:14:44.098356 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.016 seconds
[0m14:14:44.099554 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:14:44.103990 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:14:44.104600 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_status"} */
drop view if exists "delivery_analytics2"."analytics"."shipment_status__dbt_backup" cascade
[0m14:14:44.105390 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */
alter table "delivery_analytics2"."analytics"."shipments_by_time" rename to "shipments_by_time__dbt_backup"
[0m14:14:44.107340 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:14:44.111030 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:14:44.111674 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.005 seconds
[0m14:14:44.112284 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */
alter table "delivery_analytics2"."analytics"."shipments_by_time__dbt_tmp" rename to "shipments_by_time"
[0m14:14:44.114342 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: Close
[0m14:14:44.115530 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3b4aef24-38f2-412e-9fbc-9eecb78c2f8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FAB75E450>]}
[0m14:14:44.116190 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:14:44.119840 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_by_time: COMMIT
[0m14:14:44.117210 [info ] [Thread-4 (]: 6 of 7 OK created sql view model analytics.shipment_status ..................... [[32mCREATE VIEW[0m in 0.17s]
[0m14:14:44.120854 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:14:44.121985 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.shipment_status
[0m14:14:44.122628 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_by_time: COMMIT
[0m14:14:44.124964 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m14:14:44.128849 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics2"."analytics"."shipments_by_time__dbt_backup"
[0m14:14:44.129684 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:14:44.130148 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */
drop view if exists "delivery_analytics2"."analytics"."shipments_by_time__dbt_backup" cascade
[0m14:14:44.133535 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.003 seconds
[0m14:14:44.135854 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipments_by_time: Close
[0m14:14:44.136744 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3b4aef24-38f2-412e-9fbc-9eecb78c2f8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FAB6365D0>]}
[0m14:14:44.137610 [info ] [Thread-2 (]: 7 of 7 OK created sql view model analytics.shipments_by_time ................... [[32mCREATE VIEW[0m in 0.18s]
[0m14:14:44.138784 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.shipments_by_time
[0m14:14:44.141171 [debug] [MainThread]: Using postgres connection "master"
[0m14:14:44.141655 [debug] [MainThread]: On master: BEGIN
[0m14:14:44.142082 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:14:44.216209 [debug] [MainThread]: SQL status: BEGIN in 0.074 seconds
[0m14:14:44.216923 [debug] [MainThread]: On master: COMMIT
[0m14:14:44.217553 [debug] [MainThread]: Using postgres connection "master"
[0m14:14:44.217991 [debug] [MainThread]: On master: COMMIT
[0m14:14:44.218814 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:14:44.219254 [debug] [MainThread]: On master: Close
[0m14:14:44.219895 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:14:44.220281 [debug] [MainThread]: Connection 'list_delivery_analytics2' was properly closed.
[0m14:14:44.220628 [debug] [MainThread]: Connection 'list_delivery_analytics2_analytics' was properly closed.
[0m14:14:44.220981 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipment_delays' was properly closed.
[0m14:14:44.221391 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipment_status' was properly closed.
[0m14:14:44.221735 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.active_delays' was properly closed.
[0m14:14:44.222071 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipments_by_time' was properly closed.
[0m14:14:44.222524 [info ] [MainThread]: 
[0m14:14:44.223348 [info ] [MainThread]: Finished running 7 view models in 0 hours 0 minutes and 1.29 seconds (1.29s).
[0m14:14:44.226048 [debug] [MainThread]: Command end result
[0m14:14:44.265620 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:14:44.270949 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:14:44.280357 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m14:14:44.280968 [info ] [MainThread]: 
[0m14:14:44.281925 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:14:44.282920 [info ] [MainThread]: 
[0m14:14:44.284063 [error] [MainThread]: [31mFailure in model shipment_delays (models\views\shipment_delays.sql)[0m
[0m14:14:44.285043 [error] [MainThread]:   Database Error in model shipment_delays (models\views\shipment_delays.sql)
  CASE types integer and interval cannot be matched
  LINE 21:         when completed_at is not null then (completed_at - e...
                                                       ^
  compiled code at target\run\dbt_delivery_analytics\models\views\shipment_delays.sql
[0m14:14:44.285825 [info ] [MainThread]: 
[0m14:14:44.286661 [info ] [MainThread]:   compiled code at target\compiled\dbt_delivery_analytics\models\views\shipment_delays.sql
[0m14:14:44.287332 [info ] [MainThread]: 
[0m14:14:44.288108 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=7
[0m14:14:44.289677 [debug] [MainThread]: Command `dbt run` failed at 14:14:44.289520 after 3.06 seconds
[0m14:14:44.290220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FA84B0290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FAAF4A6F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FAAEE9D90>]}
[0m14:14:44.290743 [debug] [MainThread]: Flushing usage events
[0m14:14:45.530115 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:16:58.070404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBA094980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBAB4CB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBB18FD90>]}


============================== 14:16:58.076203 | a170d3cb-1b97-472c-9805-db5edfff3e07 ==============================
[0m14:16:58.076203 [info ] [MainThread]: Running with dbt=1.10.15
[0m14:16:58.077516 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'no_print': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'invocation_command': 'dbt run --select views', 'indirect_selection': 'eager', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m14:16:58.366921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a170d3cb-1b97-472c-9805-db5edfff3e07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBA500770>]}
[0m14:16:58.467432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a170d3cb-1b97-472c-9805-db5edfff3e07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBB036E00>]}
[0m14:16:58.469499 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:16:58.937818 [debug] [MainThread]: checksum: 14a228079bd6df63ba81633f9a3cf44ecfaa31056ac397c602753ec895dbf885, vars: {}, profile: , target: , version: 1.10.15
[0m14:16:59.167679 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:16:59.168868 [debug] [MainThread]: Partial parsing: updated file: dbt_delivery_analytics://models\views\shipment_delays.sql
[0m14:16:59.610638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a170d3cb-1b97-472c-9805-db5edfff3e07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBCAED550>]}
[0m14:16:59.734016 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:16:59.738300 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:16:59.844415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a170d3cb-1b97-472c-9805-db5edfff3e07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBB023D40>]}
[0m14:16:59.845341 [info ] [MainThread]: Found 20 models, 5 data tests, 3 sources, 459 macros
[0m14:16:59.846338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a170d3cb-1b97-472c-9805-db5edfff3e07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBC834E50>]}
[0m14:16:59.849773 [info ] [MainThread]: 
[0m14:16:59.850564 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:16:59.851334 [info ] [MainThread]: 
[0m14:16:59.852402 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:16:59.858650 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2'
[0m14:16:59.992449 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2"
[0m14:16:59.992981 [debug] [ThreadPool]: On list_delivery_analytics2: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2"} */

    select distinct nspname from pg_namespace
  
[0m14:16:59.993367 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:17:00.076009 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.083 seconds
[0m14:17:00.077725 [debug] [ThreadPool]: On list_delivery_analytics2: Close
[0m14:17:00.081740 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2_analytics'
[0m14:17:00.090206 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m14:17:00.090788 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: BEGIN
[0m14:17:00.091158 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:17:00.156453 [debug] [ThreadPool]: SQL status: BEGIN in 0.065 seconds
[0m14:17:00.157119 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m14:17:00.157565 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2_analytics"} */
select
      'delivery_analytics2' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m14:17:00.169205 [debug] [ThreadPool]: SQL status: SELECT 14 in 0.011 seconds
[0m14:17:00.171671 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: ROLLBACK
[0m14:17:00.172428 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: Close
[0m14:17:00.183853 [debug] [MainThread]: Using postgres connection "master"
[0m14:17:00.184386 [debug] [MainThread]: On master: BEGIN
[0m14:17:00.184788 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:17:00.248320 [debug] [MainThread]: SQL status: BEGIN in 0.063 seconds
[0m14:17:00.249040 [debug] [MainThread]: Using postgres connection "master"
[0m14:17:00.249892 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:17:00.262014 [debug] [MainThread]: SQL status: SELECT 9 in 0.011 seconds
[0m14:17:00.265947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a170d3cb-1b97-472c-9805-db5edfff3e07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBCE1B110>]}
[0m14:17:00.266568 [debug] [MainThread]: On master: ROLLBACK
[0m14:17:00.267476 [debug] [MainThread]: Using postgres connection "master"
[0m14:17:00.267977 [debug] [MainThread]: On master: BEGIN
[0m14:17:00.269145 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m14:17:00.269627 [debug] [MainThread]: On master: COMMIT
[0m14:17:00.270047 [debug] [MainThread]: Using postgres connection "master"
[0m14:17:00.270402 [debug] [MainThread]: On master: COMMIT
[0m14:17:00.271049 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:17:00.271493 [debug] [MainThread]: On master: Close
[0m14:17:00.277429 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.courier_load
[0m14:17:00.278125 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.delays_by_city_type
[0m14:17:00.278678 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.active_delays
[0m14:17:00.279222 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.couriers_performance
[0m14:17:00.280001 [info ] [Thread-2 (]: 2 of 7 START sql view model analytics.courier_load ............................. [RUN]
[0m14:17:00.280713 [info ] [Thread-4 (]: 4 of 7 START sql view model analytics.delays_by_city_type ...................... [RUN]
[0m14:17:00.283301 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.courier_load'
[0m14:17:00.284341 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.delays_by_city_type'
[0m14:17:00.281679 [info ] [Thread-1 (]: 1 of 7 START sql view model analytics.active_delays ............................ [RUN]
[0m14:17:00.285196 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.courier_load
[0m14:17:00.285908 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.delays_by_city_type
[0m14:17:00.282438 [info ] [Thread-3 (]: 3 of 7 START sql view model analytics.couriers_performance ..................... [RUN]
[0m14:17:00.286785 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.active_delays'
[0m14:17:00.296106 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_load"
[0m14:17:00.300789 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:17:00.301795 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.couriers_performance'
[0m14:17:00.302457 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.active_delays
[0m14:17:00.303487 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.couriers_performance
[0m14:17:00.307869 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.active_delays"
[0m14:17:00.311076 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.couriers_performance"
[0m14:17:00.312801 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.courier_load
[0m14:17:00.313376 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.delays_by_city_type
[0m14:17:00.361188 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_load"
[0m14:17:00.366184 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:17:00.367920 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.active_delays
[0m14:17:00.368541 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.couriers_performance
[0m14:17:00.372547 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.active_delays"
[0m14:17:00.377093 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.couriers_performance"
[0m14:17:00.378046 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:17:00.378650 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:17:00.379618 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: BEGIN
[0m14:17:00.380178 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: BEGIN
[0m14:17:00.381001 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:17:00.381574 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:17:00.382242 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:17:00.383045 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:17:00.383645 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: BEGIN
[0m14:17:00.384734 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: BEGIN
[0m14:17:00.385606 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:17:00.386361 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:17:00.470405 [debug] [Thread-4 (]: SQL status: BEGIN in 0.089 seconds
[0m14:17:00.471439 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:17:00.472228 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */

  create view "delivery_analytics2"."analytics"."delays_by_city_type__dbt_tmp"
    
    
  as (
    with delays as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipment_delays"
)

select
    delivery_city,
    delivery_type,
    count(*) as total_shipments,
    count(case when is_late_delivery = false then 1 end) as on_time_shipments,
    avg(delay_days) as avg_delay_days
from delays
group by delivery_city, delivery_type
order by delivery_city, delivery_type

/*
VIEW: delays_by_city_type

Svrha:
- Agregacija kašnjenja po delivery_city i delivery_type
- Metričke kolone: total_shipments, on_time_shipments, avg_delay_days
*/
  );
[0m14:17:00.478753 [debug] [Thread-2 (]: SQL status: BEGIN in 0.096 seconds
[0m14:17:00.479707 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:17:00.480639 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */

  create view "delivery_analytics2"."analytics"."courier_load__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    courier_id,
    shipment_date,
    count(*) as total_shipments_per_day,
    count(case when current_status != 'delivered' then 1 end) as active_shipments
from shipments
group by courier_id, shipment_date
order by courier_id, shipment_date

/*
VIEW: courier_load

Svrha:
- Analiza radnog opterećenja kurira po danu
- active_shipments = pošiljke koje nisu još dostavljene
*/
  );
[0m14:17:00.486281 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.013 seconds
[0m14:17:00.496500 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:17:00.497170 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.016 seconds
[0m14:17:00.497928 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */
alter table "delivery_analytics2"."analytics"."delays_by_city_type" rename to "delays_by_city_type__dbt_backup"
[0m14:17:00.503377 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:17:00.504026 [debug] [Thread-3 (]: SQL status: BEGIN in 0.118 seconds
[0m14:17:00.504587 [debug] [Thread-1 (]: SQL status: BEGIN in 0.119 seconds
[0m14:17:00.505176 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */
alter table "delivery_analytics2"."analytics"."courier_load" rename to "courier_load__dbt_backup"
[0m14:17:00.505718 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:17:00.506243 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:17:00.506829 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:17:00.507760 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */

  create view "delivery_analytics2"."analytics"."couriers_performance__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    courier_id,
    count(*) as total_shipments,
    count(case when current_status = 'delivered' then 1 end) as delivered_shipments,
    avg(delivery_duration_days) as avg_delivery_duration_days,
    sum(case when delivery_type = 'express' then 1 else 0 end) as express_shipments_count,
    avg(distance_km) as avg_distance_per_shipment,
    sum(distance_km) as total_distance_per_courier,
    (count(case when is_late_delivery = false then 1 end)::float / nullif(count(*),0))*100 as success_rate_percent,
    count(case when current_status != 'delivered' then 1 end) as pending_shipments
from shipments
group by courier_id
order by courier_id

/*
VIEW: couriers_performance

Svrha:
- Prikazuje performanse kurira
- Metričke kolone: total_shipments, delivered_shipments, avg_delivery_duration_days,
  express_shipments_count, avg_distance_per_shipment, total_distance_per_courier, success_rate_percent, pending_shipments
*/
  );
[0m14:17:00.511618 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:17:00.512161 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m14:17:00.512771 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.active_delays"} */

  create view "delivery_analytics2"."analytics"."active_delays__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
    where current_status != 'delivered'
)

select
    shipment_id,
    courier_id,
    delivery_city,
    delivery_type,
    shipment_date,
    expected_delivery_date,
    current_status,
    (current_date - expected_delivery_date) as delay_days
from shipments
where current_date > expected_delivery_date
order by delay_days desc

/*
VIEW: active_delays
Svrha:
- Prikazuje sve aktivne pošiljke koje kasne
- Metričke kolone: delay_days
- Koristi se za monitoring kašnjenja i upozorenja
*/
  );
[0m14:17:00.513626 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */
alter table "delivery_analytics2"."analytics"."delays_by_city_type__dbt_tmp" rename to "delays_by_city_type"
[0m14:17:00.517282 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:17:00.518259 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */
alter table "delivery_analytics2"."analytics"."courier_load__dbt_tmp" rename to "courier_load"
[0m14:17:00.518996 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:17:00.541988 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: COMMIT
[0m14:17:00.542669 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.024 seconds
[0m14:17:00.543284 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.030 seconds
[0m14:17:00.543911 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.026 seconds
[0m14:17:00.544523 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:17:00.546622 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: COMMIT
[0m14:17:00.551050 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:17:00.554602 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:17:00.555226 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: COMMIT
[0m14:17:00.555907 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:17:00.556524 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */
alter table "delivery_analytics2"."analytics"."couriers_performance" rename to "couriers_performance__dbt_backup"
[0m14:17:00.557176 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.active_delays"} */
alter table "delivery_analytics2"."analytics"."active_delays" rename to "active_delays__dbt_backup"
[0m14:17:00.557969 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: COMMIT
[0m14:17:00.559686 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:17:00.563269 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:17:00.563825 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m14:17:00.564524 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */
alter table "delivery_analytics2"."analytics"."couriers_performance__dbt_tmp" rename to "couriers_performance"
[0m14:17:00.565392 [debug] [Thread-2 (]: SQL status: COMMIT in 0.006 seconds
[0m14:17:00.566253 [debug] [Thread-4 (]: SQL status: COMMIT in 0.008 seconds
[0m14:17:00.569526 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:17:00.577431 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics2"."analytics"."courier_load__dbt_backup"
[0m14:17:00.578090 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.008 seconds
[0m14:17:00.581383 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics2"."analytics"."delays_by_city_type__dbt_backup"
[0m14:17:00.582051 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.active_delays"} */
alter table "delivery_analytics2"."analytics"."active_delays__dbt_tmp" rename to "active_delays"
[0m14:17:00.589254 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m14:17:00.591796 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: COMMIT
[0m14:17:00.593048 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m14:17:00.594211 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */
drop view if exists "delivery_analytics2"."analytics"."courier_load__dbt_backup" cascade
[0m14:17:00.595110 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:17:00.595816 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:17:00.596530 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */
drop view if exists "delivery_analytics2"."analytics"."delays_by_city_type__dbt_backup" cascade
[0m14:17:00.598812 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: COMMIT
[0m14:17:00.599461 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: COMMIT
[0m14:17:00.600284 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:17:00.600954 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: COMMIT
[0m14:17:00.601532 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.004 seconds
[0m14:17:00.602140 [debug] [Thread-3 (]: SQL status: COMMIT in 0.001 seconds
[0m14:17:00.605366 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.courier_load: Close
[0m14:17:00.605942 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.006 seconds
[0m14:17:00.606624 [debug] [Thread-1 (]: SQL status: COMMIT in 0.004 seconds
[0m14:17:00.614978 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics2"."analytics"."couriers_performance__dbt_backup"
[0m14:17:00.617883 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: Close
[0m14:17:00.622640 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."active_delays__dbt_backup"
[0m14:17:00.623838 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m14:17:00.625136 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m14:17:00.625862 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */
drop view if exists "delivery_analytics2"."analytics"."couriers_performance__dbt_backup" cascade
[0m14:17:00.626999 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.active_delays"} */
drop view if exists "delivery_analytics2"."analytics"."active_delays__dbt_backup" cascade
[0m14:17:00.627817 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a170d3cb-1b97-472c-9805-db5edfff3e07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBCCABAD0>]}
[0m14:17:00.628473 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a170d3cb-1b97-472c-9805-db5edfff3e07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBCA1B490>]}
[0m14:17:00.629449 [info ] [Thread-2 (]: 2 of 7 OK created sql view model analytics.courier_load ........................ [[32mCREATE VIEW[0m in 0.34s]
[0m14:17:00.631171 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.003 seconds
[0m14:17:00.631700 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.004 seconds
[0m14:17:00.632564 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.courier_load
[0m14:17:00.634665 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.couriers_performance: Close
[0m14:17:00.630378 [info ] [Thread-4 (]: 4 of 7 OK created sql view model analytics.delays_by_city_type ................. [[32mCREATE VIEW[0m in 0.34s]
[0m14:17:00.637266 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.active_delays: Close
[0m14:17:00.638183 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.shipment_delays
[0m14:17:00.639348 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.delays_by_city_type
[0m14:17:00.640051 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a170d3cb-1b97-472c-9805-db5edfff3e07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBD0EC9B0>]}
[0m14:17:00.641502 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a170d3cb-1b97-472c-9805-db5edfff3e07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBCF6ABA0>]}
[0m14:17:00.640827 [info ] [Thread-2 (]: 5 of 7 START sql view model analytics.shipment_delays .......................... [RUN]
[0m14:17:00.642566 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.shipment_status
[0m14:17:00.643853 [info ] [Thread-3 (]: 3 of 7 OK created sql view model analytics.couriers_performance ................ [[32mCREATE VIEW[0m in 0.34s]
[0m14:17:00.645705 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.courier_load, now model.dbt_delivery_analytics.shipment_delays)
[0m14:17:00.644849 [info ] [Thread-1 (]: 1 of 7 OK created sql view model analytics.active_delays ....................... [[32mCREATE VIEW[0m in 0.35s]
[0m14:17:00.647610 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.couriers_performance
[0m14:17:00.646753 [info ] [Thread-4 (]: 6 of 7 START sql view model analytics.shipment_status .......................... [RUN]
[0m14:17:00.648378 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.shipment_delays
[0m14:17:00.649507 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.active_delays
[0m14:17:00.650413 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.shipments_by_time
[0m14:17:00.651456 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.delays_by_city_type, now model.dbt_delivery_analytics.shipment_status)
[0m14:17:00.655077 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipment_delays"
[0m14:17:00.656756 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.shipment_status
[0m14:17:00.656171 [info ] [Thread-3 (]: 7 of 7 START sql view model analytics.shipments_by_time ........................ [RUN]
[0m14:17:00.661606 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipment_status"
[0m14:17:00.662532 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.couriers_performance, now model.dbt_delivery_analytics.shipments_by_time)
[0m14:17:00.663766 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_time
[0m14:17:00.667960 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_time"
[0m14:17:00.668761 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.shipment_delays
[0m14:17:00.673042 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipment_delays"
[0m14:17:00.674351 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.shipment_status
[0m14:17:00.674991 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.shipments_by_time
[0m14:17:00.679530 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipment_status"
[0m14:17:00.684516 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_time"
[0m14:17:00.685814 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_delays"
[0m14:17:00.686372 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: BEGIN
[0m14:17:00.686859 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m14:17:00.688363 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:17:00.689005 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:17:00.689673 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: BEGIN
[0m14:17:00.690313 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_time: BEGIN
[0m14:17:00.690861 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:17:00.691494 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m14:17:00.772103 [debug] [Thread-2 (]: SQL status: BEGIN in 0.085 seconds
[0m14:17:00.772795 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_delays"
[0m14:17:00.773332 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_delays"} */

  create view "delivery_analytics2"."analytics"."shipment_delays__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    shipment_id,
    shipment_date,
    expected_delivery_date,
    completed_at,

    -- delay_days = difference između expected i actual delivery
    case
        when completed_at is not null then 
            extract(day from (completed_at::timestamp - expected_delivery_date::timestamp))
        else
            extract(day from (current_date - expected_delivery_date::timestamp))
    end as delay_days,

    -- is_late_delivery
    case
        when completed_at is not null and completed_at > expected_delivery_date then true
        when completed_at is null and current_date > expected_delivery_date then true
        else false
    end as is_late_delivery

from shipments

/*
VIEW: fct_shipment_delays
Svrha:
- Kašnjenje po pošiljci
- Metričke kolone: delay_days, is_late_delivery
- Može se agregirati po gradu, tipu pošiljke, kuriru
*/
  );
[0m14:17:00.782459 [debug] [Thread-3 (]: SQL status: BEGIN in 0.091 seconds
[0m14:17:00.783310 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:17:00.783874 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */

  create view "delivery_analytics2"."analytics"."shipments_by_time__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
    where shipment_date is not null
)

select
    date_trunc('day', shipment_date) as day,
    date_trunc('week', shipment_date) as week,
    date_trunc('month', shipment_date) as month,
    delivery_city,
    courier_id,
    delivery_type,
    count(*) as total_shipments,
    count(case when is_late_delivery = false then 1 end) as on_time_shipments,
    avg(delivery_duration_days) as avg_delivery_days,
    sum(price) as total_revenue,
    sum(distance_km) as total_distance

from shipments
group by 1,2,3,4,5,6
order by day, delivery_city, courier_id, delivery_type

/*
VIEW: shipments_by_time

Svrha:
- Analiza pošiljki po danu, sedmici i mjesecu
- Segmentacija po delivery_city, courier_id, delivery_type (standard/express)
- Metričke kolone: total_shipments, on_time_shipments, avg_delivery_days, total_revenue, total_distance
- Može se koristiti za trend analizu i dashboard vizualizacije
*/
  );
[0m14:17:00.784422 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.011 seconds
[0m14:17:00.790680 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_delays"
[0m14:17:00.791532 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_delays"} */
alter table "delivery_analytics2"."analytics"."shipment_delays__dbt_tmp" rename to "shipment_delays"
[0m14:17:00.792384 [debug] [Thread-4 (]: SQL status: BEGIN in 0.101 seconds
[0m14:17:00.792969 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:17:00.793540 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:17:00.794034 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.009 seconds
[0m14:17:00.795853 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: COMMIT
[0m14:17:00.796434 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_status"} */

  create view "delivery_analytics2"."analytics"."shipment_status__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    current_status,
    count(*) as total_shipments,
    sum(case when current_status != 'delivered' then 1 else 0 end) as active_shipments
from shipments
group by current_status

/*
VIEW: shipment_status
Svrha:
- Prikazuje broj pošiljki po statusu: pending, picked_up, delivered
- Koristi se za monitoring opterećenja i trenutnog statusa pošiljki
*/
  );
[0m14:17:00.800192 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:17:00.800793 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_delays"
[0m14:17:00.801588 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */
alter table "delivery_analytics2"."analytics"."shipments_by_time" rename to "shipments_by_time__dbt_backup"
[0m14:17:00.802172 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: COMMIT
[0m14:17:00.803622 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:17:00.804183 [debug] [Thread-2 (]: SQL status: COMMIT in 0.001 seconds
[0m14:17:00.808049 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:17:00.811079 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics2"."analytics"."shipment_delays__dbt_backup"
[0m14:17:00.811574 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.010 seconds
[0m14:17:00.812067 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */
alter table "delivery_analytics2"."analytics"."shipments_by_time__dbt_tmp" rename to "shipments_by_time"
[0m14:17:00.813052 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_delays"
[0m14:17:00.816804 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:17:00.817781 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_delays"} */
drop view if exists "delivery_analytics2"."analytics"."shipment_delays__dbt_backup" cascade
[0m14:17:00.818392 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_status"} */
alter table "delivery_analytics2"."analytics"."shipment_status" rename to "shipment_status__dbt_backup"
[0m14:17:00.819079 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:17:00.821264 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_time: COMMIT
[0m14:17:00.821783 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:17:00.822345 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m14:17:00.822890 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.003 seconds
[0m14:17:00.823411 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_time: COMMIT
[0m14:17:00.827197 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:17:00.828970 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: Close
[0m14:17:00.829709 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_status"} */
alter table "delivery_analytics2"."analytics"."shipment_status__dbt_tmp" rename to "shipment_status"
[0m14:17:00.831183 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a170d3cb-1b97-472c-9805-db5edfff3e07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBC924D70>]}
[0m14:17:00.831789 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m14:17:00.832646 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m14:17:00.836903 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics2"."analytics"."shipments_by_time__dbt_backup"
[0m14:17:00.833688 [info ] [Thread-2 (]: 5 of 7 OK created sql view model analytics.shipment_delays ..................... [[32mCREATE VIEW[0m in 0.19s]
[0m14:17:00.839343 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: COMMIT
[0m14:17:00.840550 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m14:17:00.841480 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.shipment_delays
[0m14:17:00.842042 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:17:00.842656 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */
drop view if exists "delivery_analytics2"."analytics"."shipments_by_time__dbt_backup" cascade
[0m14:17:00.843722 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: COMMIT
[0m14:17:00.845938 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m14:17:00.848918 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics2"."analytics"."shipment_status__dbt_backup"
[0m14:17:00.849617 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.005 seconds
[0m14:17:00.850724 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m14:17:00.852588 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipments_by_time: Close
[0m14:17:00.853331 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: /* {"app": "dbt", "dbt_version": "1.10.15", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_status"} */
drop view if exists "delivery_analytics2"."analytics"."shipment_status__dbt_backup" cascade
[0m14:17:00.854520 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a170d3cb-1b97-472c-9805-db5edfff3e07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBCA7E4B0>]}
[0m14:17:00.855385 [info ] [Thread-3 (]: 7 of 7 OK created sql view model analytics.shipments_by_time ................... [[32mCREATE VIEW[0m in 0.19s]
[0m14:17:00.856591 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.shipments_by_time
[0m14:17:00.857141 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.003 seconds
[0m14:17:00.859443 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.shipment_status: Close
[0m14:17:00.860378 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a170d3cb-1b97-472c-9805-db5edfff3e07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBCD3A450>]}
[0m14:17:00.861195 [info ] [Thread-4 (]: 6 of 7 OK created sql view model analytics.shipment_status ..................... [[32mCREATE VIEW[0m in 0.21s]
[0m14:17:00.862680 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.shipment_status
[0m14:17:00.865309 [debug] [MainThread]: Using postgres connection "master"
[0m14:17:00.866031 [debug] [MainThread]: On master: BEGIN
[0m14:17:00.866627 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:17:00.933463 [debug] [MainThread]: SQL status: BEGIN in 0.067 seconds
[0m14:17:00.934085 [debug] [MainThread]: On master: COMMIT
[0m14:17:00.934499 [debug] [MainThread]: Using postgres connection "master"
[0m14:17:00.934869 [debug] [MainThread]: On master: COMMIT
[0m14:17:00.935555 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:17:00.936118 [debug] [MainThread]: On master: Close
[0m14:17:00.937073 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:17:00.937470 [debug] [MainThread]: Connection 'list_delivery_analytics2' was properly closed.
[0m14:17:00.937820 [debug] [MainThread]: Connection 'list_delivery_analytics2_analytics' was properly closed.
[0m14:17:00.938157 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipment_delays' was properly closed.
[0m14:17:00.938543 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipment_status' was properly closed.
[0m14:17:00.938851 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.active_delays' was properly closed.
[0m14:17:00.939170 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipments_by_time' was properly closed.
[0m14:17:00.939768 [info ] [MainThread]: 
[0m14:17:00.940662 [info ] [MainThread]: Finished running 7 view models in 0 hours 0 minutes and 1.09 seconds (1.09s).
[0m14:17:00.943259 [debug] [MainThread]: Command end result
[0m14:17:00.977290 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m14:17:00.981522 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m14:17:00.992326 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m14:17:00.992970 [info ] [MainThread]: 
[0m14:17:00.993823 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:17:00.994697 [info ] [MainThread]: 
[0m14:17:00.995450 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=7
[0m14:17:00.997036 [debug] [MainThread]: Command `dbt run` succeeded at 14:17:00.996885 after 3.16 seconds
[0m14:17:00.997571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DB8394F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBD09BE30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026DBD098710>]}
[0m14:17:00.998181 [debug] [MainThread]: Flushing usage events
[0m14:17:02.208879 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:39:31.062636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002F7617E4C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002F762947ED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002F762947B10>]}


============================== 13:39:31.069138 | 830db3cc-bbc5-466d-84bd-d49f8a36a855 ==============================
[0m13:39:31.069138 [info ] [MainThread]: Running with dbt=1.11.2
[0m13:39:31.070177 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'empty': 'None', 'log_cache_events': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:39:31.096362 [info ] [MainThread]: dbt version: 1.11.2
[0m13:39:31.097262 [info ] [MainThread]: python version: 3.13.3
[0m13:39:31.097979 [info ] [MainThread]: python path: C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\venv\Scripts\python.exe
[0m13:39:31.098675 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m13:39:31.180164 [info ] [MainThread]: Using profiles dir at C:\Users\Korisnik\.dbt
[0m13:39:31.181218 [info ] [MainThread]: Using profiles.yml file at C:\Users\Korisnik\.dbt\profiles.yml
[0m13:39:31.181878 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\dbt_project.yml
[0m13:39:31.183065 [info ] [MainThread]: adapter type: postgres
[0m13:39:31.183684 [info ] [MainThread]: adapter version: 1.9.1
[0m13:39:31.290669 [info ] [MainThread]: Configuration:
[0m13:39:31.291850 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:39:31.292817 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:39:31.293384 [info ] [MainThread]: Required dependencies:
[0m13:39:31.293954 [debug] [MainThread]: Executing "git --help"
[0m13:39:31.400425 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:39:31.400937 [debug] [MainThread]: STDERR: "b''"
[0m13:39:31.401347 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:39:31.401899 [info ] [MainThread]: Connection:
[0m13:39:31.402473 [info ] [MainThread]:   host: localhost
[0m13:39:31.403027 [info ] [MainThread]:   port: 5433
[0m13:39:31.403459 [info ] [MainThread]:   user: postgres
[0m13:39:31.403906 [info ] [MainThread]:   database: delivery_analytics2
[0m13:39:31.404462 [info ] [MainThread]:   schema: analytics
[0m13:39:31.405006 [info ] [MainThread]:   connect_timeout: 10
[0m13:39:31.405527 [info ] [MainThread]:   role: None
[0m13:39:31.406027 [info ] [MainThread]:   search_path: None
[0m13:39:31.406525 [info ] [MainThread]:   keepalives_idle: 0
[0m13:39:31.407004 [info ] [MainThread]:   sslmode: None
[0m13:39:31.407628 [info ] [MainThread]:   sslcert: None
[0m13:39:31.408163 [info ] [MainThread]:   sslkey: None
[0m13:39:31.408581 [info ] [MainThread]:   sslrootcert: None
[0m13:39:31.409045 [info ] [MainThread]:   application_name: dbt
[0m13:39:31.409513 [info ] [MainThread]:   retries: 1
[0m13:39:31.410261 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:39:31.853718 [debug] [MainThread]: Acquiring new postgres connection 'debug'
[0m13:39:31.986054 [debug] [MainThread]: Using postgres connection "debug"
[0m13:39:31.986473 [debug] [MainThread]: On debug: select 1 as id
[0m13:39:31.986753 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:39:32.125053 [debug] [MainThread]: SQL status: SELECT 1 in 0.138 seconds
[0m13:39:32.126666 [debug] [MainThread]: On debug: Close
[0m13:39:32.127278 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m13:39:32.127872 [info ] [MainThread]: [32mAll checks passed![0m
[0m13:39:32.129001 [debug] [MainThread]: Command `dbt debug` succeeded at 13:39:32.128844 after 1.21 seconds
[0m13:39:32.129465 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:39:32.129932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002F763B67CE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002F763FAFD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002F7629D3CE0>]}
[0m13:39:32.130425 [debug] [MainThread]: Flushing usage events
[0m13:39:33.097835 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:42:32.264190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295A9858C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295AA9B3ED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295AA9B3B10>]}


============================== 13:42:32.268509 | b7ab8c28-e7a6-4b06-ac92-b513be01b716 ==============================
[0m13:42:32.268509 [info ] [MainThread]: Running with dbt=1.11.2
[0m13:42:32.269476 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt debug', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:42:32.284631 [info ] [MainThread]: dbt version: 1.11.2
[0m13:42:32.285278 [info ] [MainThread]: python version: 3.13.3
[0m13:42:32.285834 [info ] [MainThread]: python path: C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\venv\Scripts\python.exe
[0m13:42:32.286415 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m13:42:33.367915 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:42:33.368444 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:42:33.368764 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:42:34.341513 [info ] [MainThread]: Using profiles dir at C:\Users\Korisnik\.dbt
[0m13:42:34.342450 [info ] [MainThread]: Using profiles.yml file at C:\Users\Korisnik\.dbt\profiles.yml
[0m13:42:34.342971 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\dbt_project.yml
[0m13:42:34.343542 [info ] [MainThread]: adapter type: databricks
[0m13:42:34.344078 [info ] [MainThread]: adapter version: 1.11.4
[0m13:42:34.453713 [info ] [MainThread]: Configuration:
[0m13:42:34.454812 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:42:34.455373 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:42:34.455960 [info ] [MainThread]: Required dependencies:
[0m13:42:34.456487 [debug] [MainThread]: Executing "git --help"
[0m13:42:34.597629 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:42:34.598210 [debug] [MainThread]: STDERR: "b''"
[0m13:42:34.598621 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:42:34.599106 [info ] [MainThread]: Connection:
[0m13:42:34.599682 [info ] [MainThread]:   host: dbc-d60709bf-2bd1.cloud.databricks.com
[0m13:42:34.600210 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/c7a1e0fc7c84e77c
[0m13:42:34.600783 [info ] [MainThread]:   catalog: delivery_analytics
[0m13:42:34.601296 [info ] [MainThread]:   schema: staging
[0m13:42:34.602097 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m13:42:35.160426 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:42:35.192567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'b7ab8c28-e7a6-4b06-ac92-b513be01b716', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295D045B230>]}
[0m13:42:35.193759 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m13:42:35.194344 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m13:42:35.194772 [debug] [MainThread]: Using databricks connection "debug"
[0m13:42:35.195227 [debug] [MainThread]: On debug: select 1 as id
[0m13:42:35.195666 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:42:36.642676 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0f07d-56bf-1f12-b9b6-adea2999cf53) - Created
[0m13:42:53.060110 [debug] [MainThread]: SQL status: OK in 17.860 seconds
[0m13:42:53.061594 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0f07d-56bf-1f12-b9b6-adea2999cf53, command-id=01f0f07d-56fa-1e6b-a98c-5612a75486ec) - Closing
[0m13:42:53.374852 [debug] [MainThread]: On debug: Close
[0m13:42:53.375536 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0f07d-56bf-1f12-b9b6-adea2999cf53) - Closing
[0m13:42:53.582084 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m13:42:53.582775 [info ] [MainThread]: [32mAll checks passed![0m
[0m13:42:53.583853 [debug] [MainThread]: Command `dbt debug` succeeded at 13:42:53.583725 after 21.47 seconds
[0m13:42:53.584282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295AD3D5590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295D03729C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295D0305550>]}
[0m13:42:53.584651 [debug] [MainThread]: Flushing usage events
[0m13:42:54.671236 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:43:20.384584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002584D5D8C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002584E733ED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002584E733B10>]}


============================== 13:43:20.388664 | b6c14aee-ac63-4221-80f8-cd3c9beae80f ==============================
[0m13:43:20.388664 [info ] [MainThread]: Running with dbt=1.11.2
[0m13:43:20.389515 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'debug': 'False', 'log_format': 'default', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'use_colors': 'True', 'invocation_command': 'dbt run', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:43:21.440505 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:43:21.441027 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:43:21.441396 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:43:22.316074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b6c14aee-ac63-4221-80f8-cd3c9beae80f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002584DA45220>]}
[0m13:43:22.388834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b6c14aee-ac63-4221-80f8-cd3c9beae80f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002587414E470>]}
[0m13:43:22.389608 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m13:43:22.900691 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:43:22.902006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'b6c14aee-ac63-4221-80f8-cd3c9beae80f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025874199250>]}
[0m13:43:22.914458 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m13:43:23.043550 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m13:43:23.044291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b6c14aee-ac63-4221-80f8-cd3c9beae80f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000258746CC320>]}
[0m13:43:24.898820 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'shipments_by_hour_day' in the 'models' section of file 'models\schema.yml'
[0m13:43:24.900976 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'shipments_by_city_vehicle' in the 'models' section of file 'models\schema.yml'
[0m13:43:25.139733 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.delivery_analytics.not_null_shipments_by_hour_day_day_of_week.e2d126a571' (models\schema.yml) depends on a node named 'shipments_by_hour_day' in package '' which was not found
[0m13:43:25.140592 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.delivery_analytics.not_null_shipments_by_hour_day_shipment_hour.6ee57859e9' (models\schema.yml) depends on a node named 'shipments_by_hour_day' in package '' which was not found
[0m13:43:25.141330 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.delivery_analytics.not_null_shipments_by_city_vehicle_city.1d36860158' (models\schema.yml) depends on a node named 'shipments_by_city_vehicle' in package '' which was not found
[0m13:43:25.142023 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.delivery_analytics.not_null_shipments_by_city_vehicle_vehicle_type.51a805e811' (models\schema.yml) depends on a node named 'shipments_by_city_vehicle' in package '' which was not found
[0m13:43:25.280725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b6c14aee-ac63-4221-80f8-cd3c9beae80f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025875A0FD90>]}
[0m13:43:25.394784 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:43:25.398440 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:43:25.417042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b6c14aee-ac63-4221-80f8-cd3c9beae80f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002587472EDD0>]}
[0m13:43:25.417633 [info ] [MainThread]: Found 20 models, 5 data tests, 3 sources, 727 macros
[0m13:43:25.418183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b6c14aee-ac63-4221-80f8-cd3c9beae80f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025875AAD310>]}
[0m13:43:25.420801 [info ] [MainThread]: 
[0m13:43:25.421311 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:43:25.421838 [info ] [MainThread]: 
[0m13:43:25.422633 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:43:25.422959 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:43:25.431942 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_delivery_analytics) - Creating connection
[0m13:43:25.432521 [debug] [ThreadPool]: Acquiring new databricks connection 'list_delivery_analytics'
[0m13:43:25.445543 [debug] [ThreadPool]: Using databricks connection "list_delivery_analytics"
[0m13:43:25.446204 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    

  SHOW SCHEMAS IN `delivery_analytics`


  
[0m13:43:25.446648 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:43:26.699737 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f07d-74b2-14f4-b26a-e53c2c62b90d) - Created
[0m13:43:27.752036 [debug] [ThreadPool]: SQL status: OK in 2.310 seconds
[0m13:43:27.770442 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f07d-74b2-14f4-b26a-e53c2c62b90d, command-id=01f0f07d-74d1-10e6-9992-b22c3ec6b67a) - Closing
[0m13:43:27.771078 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m13:43:27.771399 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f07d-74b2-14f4-b26a-e53c2c62b90d) - Closing
[0m13:43:27.978856 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_delivery_analytics_staging) - Creating connection
[0m13:43:27.979283 [debug] [ThreadPool]: Acquiring new databricks connection 'create_delivery_analytics_staging'
[0m13:43:28.009775 [debug] [ThreadPool]: Creating schema "database: "delivery_analytics"
schema: "staging"
"
[0m13:43:28.014349 [debug] [ThreadPool]: Using databricks connection "create_delivery_analytics_staging"
[0m13:43:28.014694 [debug] [ThreadPool]: On create_delivery_analytics_staging: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "connection_name": "create_delivery_analytics_staging"} */
create schema if not exists `delivery_analytics`.`staging`
  
[0m13:43:28.014983 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:43:28.904179 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f07d-7606-1846-bf66-0a9427079583) - Created
[0m13:43:30.197957 [debug] [ThreadPool]: SQL status: OK in 2.180 seconds
[0m13:43:30.199037 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f07d-7606-1846-bf66-0a9427079583, command-id=01f0f07d-7621-16f7-8621-63345b014b60) - Closing
[0m13:43:30.199443 [debug] [ThreadPool]: On create_delivery_analytics_staging: Close
[0m13:43:30.199746 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f07d-7606-1846-bf66-0a9427079583) - Closing
[0m13:43:30.430688 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_delivery_analytics_staging) - Creating connection
[0m13:43:30.431249 [debug] [ThreadPool]: Acquiring new databricks connection 'list_delivery_analytics_staging'
[0m13:43:30.440239 [debug] [ThreadPool]: Using databricks connection "list_delivery_analytics_staging"
[0m13:43:30.440625 [debug] [ThreadPool]: On list_delivery_analytics_staging: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'delivery_analytics' 
  AND table_schema = 'staging'

  
[0m13:43:30.440938 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:43:31.316824 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f07d-7772-1cb5-8bc3-99e2dcebd0bb) - Created
[0m13:43:32.561900 [debug] [ThreadPool]: SQL status: OK in 2.120 seconds
[0m13:43:32.594670 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f07d-7772-1cb5-8bc3-99e2dcebd0bb, command-id=01f0f07d-7791-1110-bff1-7bb6f99b292f) - Closing
[0m13:43:32.595222 [debug] [ThreadPool]: On list_delivery_analytics_staging: Close
[0m13:43:32.595544 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f07d-7772-1cb5-8bc3-99e2dcebd0bb) - Closing
[0m13:43:32.797548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b6c14aee-ac63-4221-80f8-cd3c9beae80f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025875AF7EE0>]}
[0m13:43:32.833569 [debug] [Thread-4 (]: Began running node model.delivery_analytics.stg_couriers
[0m13:43:32.834205 [info ] [Thread-4 (]: 1 of 20 START sql view model staging.stg_couriers .............................. [RUN]
[0m13:43:32.835036 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.delivery_analytics.stg_couriers) - Creating connection
[0m13:43:32.835420 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.delivery_analytics.stg_couriers'
[0m13:43:32.835872 [debug] [Thread-4 (]: Began compiling node model.delivery_analytics.stg_couriers
[0m13:43:32.850751 [debug] [Thread-4 (]: Writing injected SQL for node "model.delivery_analytics.stg_couriers"
[0m13:43:32.854653 [debug] [Thread-4 (]: Began executing node model.delivery_analytics.stg_couriers
[0m13:43:32.872804 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m13:43:32.875272 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:43:32.876234 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'b6c14aee-ac63-4221-80f8-cd3c9beae80f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025875BABF70>]}
[0m13:43:32.890631 [debug] [Thread-4 (]: Creating view `delivery_analytics`.`staging`.`stg_couriers`
[0m13:43:32.902373 [debug] [Thread-4 (]: Writing runtime sql for node "model.delivery_analytics.stg_couriers"
[0m13:43:32.905893 [debug] [Thread-4 (]: Using databricks connection "model.delivery_analytics.stg_couriers"
[0m13:43:32.906471 [debug] [Thread-4 (]: On model.delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "node_id": "model.delivery_analytics.stg_couriers"} */

  
  
  create or replace view `delivery_analytics`.`staging`.`stg_couriers`
  
  as (
    -- stg_couriers.sql
with source as (
    select
        courier_id,
        name,
        vehicle_type,
        city
    from `delivery_analytics2`.`raw`.`couriers`
)

select * from source
  )

[0m13:43:32.907102 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:43:33.823060 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f07d-78f1-1447-ad38-df3f13ae45b9) - Created
[0m13:43:34.464009 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "node_id": "model.delivery_analytics.stg_couriers"} */

  
  
  create or replace view `delivery_analytics`.`staging`.`stg_couriers`
  
  as (
    -- stg_couriers.sql
with source as (
    select
        courier_id,
        name,
        vehicle_type,
        city
    from `delivery_analytics2`.`raw`.`couriers`
)

select * from source
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`couriers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 15 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`couriers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 15 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`couriers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 15 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0f07d-7910-1355-83c6-992b1748c313
[0m13:43:34.464936 [debug] [Thread-4 (]: On model.delivery_analytics.stg_couriers: Close
[0m13:43:34.465297 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f07d-78f1-1447-ad38-df3f13ae45b9) - Closing
[0m13:43:34.712338 [debug] [Thread-4 (]: Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`couriers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 15 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_couriers.sql
[0m13:43:34.715024 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6c14aee-ac63-4221-80f8-cd3c9beae80f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002584D9DB6D0>]}
[0m13:43:34.716004 [error] [Thread-4 (]: 1 of 20 ERROR creating sql view model staging.stg_couriers ..................... [[31mERROR[0m in 1.88s]
[0m13:43:34.717096 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.stg_couriers
[0m13:43:34.717636 [debug] [Thread-4 (]: Began running node model.delivery_analytics.stg_customers
[0m13:43:34.718306 [debug] [Thread-7 (]: Marking all children of 'model.delivery_analytics.stg_couriers' to be skipped because of status 'error'.  Reason: Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`couriers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 15 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_couriers.sql.
[0m13:43:34.719021 [info ] [Thread-4 (]: 2 of 20 START sql view model staging.stg_customers ............................. [RUN]
[0m13:43:34.720875 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.delivery_analytics.stg_customers) - Creating connection
[0m13:43:34.721267 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.delivery_analytics.stg_customers'
[0m13:43:34.721615 [debug] [Thread-4 (]: Began compiling node model.delivery_analytics.stg_customers
[0m13:43:34.725957 [debug] [Thread-4 (]: Writing injected SQL for node "model.delivery_analytics.stg_customers"
[0m13:43:34.727821 [debug] [Thread-4 (]: Began executing node model.delivery_analytics.stg_customers
[0m13:43:34.730894 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m13:43:34.732514 [debug] [Thread-4 (]: Creating view `delivery_analytics`.`staging`.`stg_customers`
[0m13:43:34.733331 [debug] [Thread-4 (]: Writing runtime sql for node "model.delivery_analytics.stg_customers"
[0m13:43:34.735457 [debug] [Thread-4 (]: Using databricks connection "model.delivery_analytics.stg_customers"
[0m13:43:34.736150 [debug] [Thread-4 (]: On model.delivery_analytics.stg_customers: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "node_id": "model.delivery_analytics.stg_customers"} */

  
  
  create or replace view `delivery_analytics`.`staging`.`stg_customers`
  
  as (
    with source as (
    select
        customer_id,
        name,
        email,
        phone,
        address,
        city
    from `delivery_analytics2`.`raw`.`customers`
)

select * from source
  )

[0m13:43:34.736514 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:43:35.660777 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f07d-7a0d-1845-b75f-ec6bb0e0a94c) - Created
[0m13:43:36.194368 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "node_id": "model.delivery_analytics.stg_customers"} */

  
  
  create or replace view `delivery_analytics`.`staging`.`stg_customers`
  
  as (
    with source as (
    select
        customer_id,
        name,
        email,
        phone,
        address,
        city
    from `delivery_analytics2`.`raw`.`customers`
)

select * from source
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0f07d-7a28-1cad-b82f-c4cc9874b135
[0m13:43:36.195208 [debug] [Thread-4 (]: On model.delivery_analytics.stg_customers: Close
[0m13:43:36.195559 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f07d-7a0d-1845-b75f-ec6bb0e0a94c) - Closing
[0m13:43:36.393500 [debug] [Thread-4 (]: Database Error in model stg_customers (models\staging\stg_customers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_customers.sql
[0m13:43:36.394125 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6c14aee-ac63-4221-80f8-cd3c9beae80f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025875BA4D00>]}
[0m13:43:36.394795 [error] [Thread-4 (]: 2 of 20 ERROR creating sql view model staging.stg_customers .................... [[31mERROR[0m in 1.67s]
[0m13:43:36.395681 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.stg_customers
[0m13:43:36.396117 [debug] [Thread-4 (]: Began running node model.delivery_analytics.stg_shipments
[0m13:43:36.396700 [debug] [Thread-7 (]: Marking all children of 'model.delivery_analytics.stg_customers' to be skipped because of status 'error'.  Reason: Database Error in model stg_customers (models\staging\stg_customers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_customers.sql.
[0m13:43:36.397334 [info ] [Thread-4 (]: 3 of 20 START sql view model staging.stg_shipments ............................. [RUN]
[0m13:43:36.398357 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.delivery_analytics.stg_shipments) - Creating connection
[0m13:43:36.398739 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.delivery_analytics.stg_shipments'
[0m13:43:36.399085 [debug] [Thread-4 (]: Began compiling node model.delivery_analytics.stg_shipments
[0m13:43:36.402609 [debug] [Thread-4 (]: Writing injected SQL for node "model.delivery_analytics.stg_shipments"
[0m13:43:36.431637 [debug] [Thread-4 (]: Began executing node model.delivery_analytics.stg_shipments
[0m13:43:36.435829 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m13:43:36.436984 [debug] [Thread-4 (]: Creating view `delivery_analytics`.`staging`.`stg_shipments`
[0m13:43:36.438066 [debug] [Thread-4 (]: Writing runtime sql for node "model.delivery_analytics.stg_shipments"
[0m13:43:36.440716 [debug] [Thread-4 (]: Using databricks connection "model.delivery_analytics.stg_shipments"
[0m13:43:36.441951 [debug] [Thread-4 (]: On model.delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "node_id": "model.delivery_analytics.stg_shipments"} */

  
  
  create or replace view `delivery_analytics`.`staging`.`stg_shipments`
  
  as (
    with source as (
    select
        shipment_id,
        sender_id,
        receiver_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        completed_at,
        price,
        delivery_city,
        delivery_type,
        distance_km
    from `delivery_analytics2`.`raw`.`shipments`
),

status_logic as (
    select
        *,
        case
            when shipment_date is null then 'pending'
            when shipment_date is not null and completed_at is null then 'picked_up'
            when completed_at is not null then 'delivered'
        end as current_status
    from source
)

select * from status_logic
  )

[0m13:43:36.442487 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:43:37.380527 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f07d-7b11-1662-a6cd-54ab23dd4b73) - Created
[0m13:43:37.955752 [debug] [Thread-4 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "node_id": "model.delivery_analytics.stg_shipments"} */

  
  
  create or replace view `delivery_analytics`.`staging`.`stg_shipments`
  
  as (
    with source as (
    select
        shipment_id,
        sender_id,
        receiver_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        completed_at,
        price,
        delivery_city,
        delivery_type,
        distance_km
    from `delivery_analytics2`.`raw`.`shipments`
),

status_logic as (
    select
        *,
        case
            when shipment_date is null then 'pending'
            when shipment_date is not null and completed_at is null then 'picked_up'
            when completed_at is not null then 'delivered'
        end as current_status
    from source
)

select * from status_logic
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`shipments` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 21 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`shipments` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 21 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`shipments` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 21 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0f07d-7b2e-1f11-bbc7-56bff9f7630b
[0m13:43:37.956595 [debug] [Thread-4 (]: On model.delivery_analytics.stg_shipments: Close
[0m13:43:37.956943 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f07d-7b11-1662-a6cd-54ab23dd4b73) - Closing
[0m13:43:38.152656 [debug] [Thread-4 (]: Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`shipments` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 21 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_shipments.sql
[0m13:43:38.153249 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b6c14aee-ac63-4221-80f8-cd3c9beae80f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025875C56030>]}
[0m13:43:38.153956 [error] [Thread-4 (]: 3 of 20 ERROR creating sql view model staging.stg_shipments .................... [[31mERROR[0m in 1.76s]
[0m13:43:38.154870 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.stg_shipments
[0m13:43:38.155394 [debug] [Thread-4 (]: Began running node model.delivery_analytics.dim_couriers
[0m13:43:38.155957 [debug] [Thread-7 (]: Marking all children of 'model.delivery_analytics.stg_shipments' to be skipped because of status 'error'.  Reason: Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`shipments` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 21 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_shipments.sql.
[0m13:43:38.156371 [info ] [Thread-4 (]: 4 of 20 SKIP relation staging.dim_couriers ..................................... [[33mSKIP[0m]
[0m13:43:38.157132 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.dim_couriers
[0m13:43:38.157511 [debug] [Thread-4 (]: Began running node model.delivery_analytics.dim_cities
[0m13:43:38.157897 [info ] [Thread-4 (]: 5 of 20 SKIP relation staging.dim_cities ....................................... [[33mSKIP[0m]
[0m13:43:38.158505 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.dim_cities
[0m13:43:38.158864 [debug] [Thread-4 (]: Began running node model.delivery_analytics.dim_customers
[0m13:43:38.159250 [info ] [Thread-4 (]: 6 of 20 SKIP relation staging.dim_customers .................................... [[33mSKIP[0m]
[0m13:43:38.159867 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.dim_customers
[0m13:43:38.160512 [debug] [Thread-4 (]: Began running node model.delivery_analytics.fct_shipments
[0m13:43:38.161134 [info ] [Thread-4 (]: 7 of 20 SKIP relation staging.fct_shipments .................................... [[33mSKIP[0m]
[0m13:43:38.161841 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.fct_shipments
[0m13:43:38.162844 [debug] [Thread-4 (]: Began running node model.delivery_analytics.active_delays
[0m13:43:38.163398 [info ] [Thread-4 (]: 8 of 20 SKIP relation staging.active_delays .................................... [[33mSKIP[0m]
[0m13:43:38.164503 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.active_delays
[0m13:43:38.165544 [debug] [Thread-4 (]: Began running node model.delivery_analytics.courier_load
[0m13:43:38.166181 [info ] [Thread-4 (]: 9 of 20 SKIP relation staging.courier_load ..................................... [[33mSKIP[0m]
[0m13:43:38.168149 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.courier_load
[0m13:43:38.168748 [debug] [Thread-4 (]: Began running node model.delivery_analytics.couriers_performance
[0m13:43:38.169271 [info ] [Thread-4 (]: 10 of 20 SKIP relation staging.couriers_performance ............................ [[33mSKIP[0m]
[0m13:43:38.169916 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.couriers_performance
[0m13:43:38.170595 [debug] [Thread-4 (]: Began running node model.delivery_analytics.fct_courier_load
[0m13:43:38.171127 [info ] [Thread-4 (]: 11 of 20 SKIP relation staging.fct_courier_load ................................ [[33mSKIP[0m]
[0m13:43:38.171880 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.fct_courier_load
[0m13:43:38.172393 [debug] [Thread-4 (]: Began running node model.delivery_analytics.fct_courier_performance
[0m13:43:38.172898 [info ] [Thread-4 (]: 12 of 20 SKIP relation staging.fct_courier_performance ......................... [[33mSKIP[0m]
[0m13:43:38.173752 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.fct_courier_performance
[0m13:43:38.174266 [debug] [Thread-4 (]: Began running node model.delivery_analytics.fct_shipment_delays
[0m13:43:38.174881 [info ] [Thread-4 (]: 13 of 20 SKIP relation staging.fct_shipment_delays ............................. [[33mSKIP[0m]
[0m13:43:38.176875 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.fct_shipment_delays
[0m13:43:38.177366 [debug] [Thread-4 (]: Began running node model.delivery_analytics.int_active_delays
[0m13:43:38.177846 [info ] [Thread-4 (]: 14 of 20 SKIP relation staging.int_active_delays ............................... [[33mSKIP[0m]
[0m13:43:38.178643 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.int_active_delays
[0m13:43:38.179228 [debug] [Thread-4 (]: Began running node model.delivery_analytics.int_shipment_delays
[0m13:43:38.179958 [info ] [Thread-4 (]: 15 of 20 SKIP relation staging.int_shipment_delays ............................. [[33mSKIP[0m]
[0m13:43:38.180866 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.int_shipment_delays
[0m13:43:38.181261 [debug] [Thread-4 (]: Began running node model.delivery_analytics.int_shipment_status
[0m13:43:38.181666 [info ] [Thread-4 (]: 16 of 20 SKIP relation staging.int_shipment_status ............................. [[33mSKIP[0m]
[0m13:43:38.182293 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.int_shipment_status
[0m13:43:38.182938 [debug] [Thread-4 (]: Began running node model.delivery_analytics.shipment_delays
[0m13:43:38.183370 [info ] [Thread-4 (]: 17 of 20 SKIP relation staging.shipment_delays ................................. [[33mSKIP[0m]
[0m13:43:38.183917 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.shipment_delays
[0m13:43:38.184313 [debug] [Thread-4 (]: Began running node model.delivery_analytics.shipment_status
[0m13:43:38.184708 [info ] [Thread-4 (]: 18 of 20 SKIP relation staging.shipment_status ................................. [[33mSKIP[0m]
[0m13:43:38.185318 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.shipment_status
[0m13:43:38.185692 [debug] [Thread-4 (]: Began running node model.delivery_analytics.shipments_by_time
[0m13:43:38.186076 [info ] [Thread-4 (]: 19 of 20 SKIP relation staging.shipments_by_time ............................... [[33mSKIP[0m]
[0m13:43:38.186665 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.shipments_by_time
[0m13:43:38.187370 [debug] [Thread-4 (]: Began running node model.delivery_analytics.delays_by_city_type
[0m13:43:38.187862 [info ] [Thread-4 (]: 20 of 20 SKIP relation staging.delays_by_city_type ............................. [[33mSKIP[0m]
[0m13:43:38.189318 [debug] [Thread-4 (]: Finished running node model.delivery_analytics.delays_by_city_type
[0m13:43:38.191186 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:43:38.191596 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:43:38.192308 [info ] [MainThread]: 
[0m13:43:38.192874 [info ] [MainThread]: Finished running 7 table models, 13 view models in 0 hours 0 minutes and 12.77 seconds (12.77s).
[0m13:43:38.194540 [debug] [MainThread]: Command end result
[0m13:43:38.255345 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:43:38.258985 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:43:38.296836 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m13:43:38.297493 [info ] [MainThread]: 
[0m13:43:38.298114 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m13:43:38.298637 [info ] [MainThread]: 
[0m13:43:38.299201 [error] [MainThread]: [31mFailure in model stg_couriers (models\staging\stg_couriers.sql)[0m
[0m13:43:38.299794 [error] [MainThread]:   Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`couriers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 15 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_couriers.sql
[0m13:43:38.300387 [info ] [MainThread]: 
[0m13:43:38.300978 [info ] [MainThread]:   compiled code at target\compiled\delivery_analytics\models\staging\stg_couriers.sql
[0m13:43:38.301453 [info ] [MainThread]: 
[0m13:43:38.301990 [error] [MainThread]: [31mFailure in model stg_customers (models\staging\stg_customers.sql)[0m
[0m13:43:38.302576 [error] [MainThread]:   Database Error in model stg_customers (models\staging\stg_customers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_customers.sql
[0m13:43:38.303276 [info ] [MainThread]: 
[0m13:43:38.303987 [info ] [MainThread]:   compiled code at target\compiled\delivery_analytics\models\staging\stg_customers.sql
[0m13:43:38.304597 [info ] [MainThread]: 
[0m13:43:38.305182 [error] [MainThread]: [31mFailure in model stg_shipments (models\staging\stg_shipments.sql)[0m
[0m13:43:38.305907 [error] [MainThread]:   Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`shipments` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 21 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_shipments.sql
[0m13:43:38.306713 [info ] [MainThread]: 
[0m13:43:38.307330 [info ] [MainThread]:   compiled code at target\compiled\delivery_analytics\models\staging\stg_shipments.sql
[0m13:43:38.308568 [info ] [MainThread]: 
[0m13:43:38.309705 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=17 NO-OP=0 TOTAL=20
[0m13:43:38.311155 [debug] [MainThread]: Command `dbt run` failed at 13:43:38.311008 after 18.07 seconds
[0m13:43:38.311688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002584D8AC830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000258747A46B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000258746CB5F0>]}
[0m13:43:38.312114 [debug] [MainThread]: Flushing usage events
[0m13:43:39.301250 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:50:41.658975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4A8C38C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4A9D9BED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4A9D9BB10>]}


============================== 13:50:41.664624 | a2c9ee35-1951-4243-9564-1600cfc2d36a ==============================
[0m13:50:41.664624 [info ] [MainThread]: Running with dbt=1.11.2
[0m13:50:41.665880 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'partial_parse': 'True', 'invocation_command': 'dbt run --models staging', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:50:41.666596 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
Usage of `--models`, `--model`, and `-m` is deprecated in favor of `--select` or
`-s`.
[0m13:50:41.668032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'a2c9ee35-1951-4243-9564-1600cfc2d36a', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4A9D9DE00>]}
[0m13:50:43.074441 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:50:43.075069 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:50:43.075476 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:50:44.121203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a2c9ee35-1951-4243-9564-1600cfc2d36a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4CF802470>]}
[0m13:50:44.196523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a2c9ee35-1951-4243-9564-1600cfc2d36a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4CF84C950>]}
[0m13:50:44.197418 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m13:50:44.804404 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:50:44.805406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a2c9ee35-1951-4243-9564-1600cfc2d36a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4AC95AF30>]}
[0m13:50:44.818686 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m13:50:45.003726 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:50:45.004172 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m13:50:45.004456 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:50:45.059214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a2c9ee35-1951-4243-9564-1600cfc2d36a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4CFA3FA10>]}
[0m13:50:45.158620 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:50:45.162518 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:50:45.177014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a2c9ee35-1951-4243-9564-1600cfc2d36a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4D0E113D0>]}
[0m13:50:45.177835 [info ] [MainThread]: Found 20 models, 5 data tests, 3 sources, 727 macros
[0m13:50:45.178453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a2c9ee35-1951-4243-9564-1600cfc2d36a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4D0D93E90>]}
[0m13:50:45.180550 [info ] [MainThread]: 
[0m13:50:45.181040 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:50:45.181575 [info ] [MainThread]: 
[0m13:50:45.182456 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:50:45.182843 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:50:45.191422 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_delivery_analytics) - Creating connection
[0m13:50:45.191917 [debug] [ThreadPool]: Acquiring new databricks connection 'list_delivery_analytics'
[0m13:50:45.211289 [debug] [ThreadPool]: Using databricks connection "list_delivery_analytics"
[0m13:50:45.212456 [debug] [ThreadPool]: On list_delivery_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics"} */

    

  SHOW SCHEMAS IN `delivery_analytics`


  
[0m13:50:45.212867 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:50:46.246396 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f07e-7ab1-17df-ae95-8335f8935435) - Created
[0m13:50:46.779363 [debug] [ThreadPool]: SQL status: OK in 1.570 seconds
[0m13:50:46.785615 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f07e-7ab1-17df-ae95-8335f8935435, command-id=01f0f07e-7acd-14b9-8873-0d9cf9efda32) - Closing
[0m13:50:46.786271 [debug] [ThreadPool]: On list_delivery_analytics: Close
[0m13:50:46.786609 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f07e-7ab1-17df-ae95-8335f8935435) - Closing
[0m13:50:46.997175 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_delivery_analytics_staging) - Creating connection
[0m13:50:46.998104 [debug] [ThreadPool]: Acquiring new databricks connection 'list_delivery_analytics_staging'
[0m13:50:47.005799 [debug] [ThreadPool]: Using databricks connection "list_delivery_analytics_staging"
[0m13:50:47.006461 [debug] [ThreadPool]: On list_delivery_analytics_staging: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics_staging"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'delivery_analytics' 
  AND table_schema = 'staging'

  
[0m13:50:47.006940 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:50:48.000187 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f07e-7bbd-13c0-87b0-c4de61c69eb5) - Created
[0m13:50:48.617736 [debug] [ThreadPool]: SQL status: OK in 1.610 seconds
[0m13:50:48.621299 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f07e-7bbd-13c0-87b0-c4de61c69eb5, command-id=01f0f07e-7bd9-15d8-b2b3-b0c6685cfe04) - Closing
[0m13:50:48.621972 [debug] [ThreadPool]: On list_delivery_analytics_staging: Close
[0m13:50:48.622387 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f07e-7bbd-13c0-87b0-c4de61c69eb5) - Closing
[0m13:50:48.820288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a2c9ee35-1951-4243-9564-1600cfc2d36a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4D0CB5700>]}
[0m13:50:48.825901 [debug] [Thread-3 (]: Began running node model.delivery_analytics.stg_couriers
[0m13:50:48.826830 [info ] [Thread-3 (]: 1 of 3 START sql view model staging.stg_couriers ............................... [RUN]
[0m13:50:48.828209 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.delivery_analytics.stg_couriers) - Creating connection
[0m13:50:48.828800 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.delivery_analytics.stg_couriers'
[0m13:50:48.829334 [debug] [Thread-3 (]: Began compiling node model.delivery_analytics.stg_couriers
[0m13:50:48.854160 [debug] [Thread-3 (]: Writing injected SQL for node "model.delivery_analytics.stg_couriers"
[0m13:50:48.857347 [debug] [Thread-3 (]: Began executing node model.delivery_analytics.stg_couriers
[0m13:50:48.890154 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m13:50:48.894238 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m13:50:48.895474 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'a2c9ee35-1951-4243-9564-1600cfc2d36a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4D0F0A850>]}
[0m13:50:48.916611 [debug] [Thread-3 (]: Creating view `delivery_analytics`.`staging`.`stg_couriers`
[0m13:50:48.932278 [debug] [Thread-3 (]: Writing runtime sql for node "model.delivery_analytics.stg_couriers"
[0m13:50:48.934750 [debug] [Thread-3 (]: Using databricks connection "model.delivery_analytics.stg_couriers"
[0m13:50:48.935459 [debug] [Thread-3 (]: On model.delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "node_id": "model.delivery_analytics.stg_couriers"} */

  
  
  create or replace view `delivery_analytics`.`staging`.`stg_couriers`
  
  as (
    -- stg_couriers.sql
with source as (
    select
        courier_id,
        name,
        vehicle_type,
        city
    from `delivery_analytics2`.`raw`.`couriers`
)

select * from source
  )

[0m13:50:48.936259 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:50:49.973552 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f07e-7ce9-19d9-b089-293a43d8ffc3) - Created
[0m13:50:50.562507 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "node_id": "model.delivery_analytics.stg_couriers"} */

  
  
  create or replace view `delivery_analytics`.`staging`.`stg_couriers`
  
  as (
    -- stg_couriers.sql
with source as (
    select
        courier_id,
        name,
        vehicle_type,
        city
    from `delivery_analytics2`.`raw`.`couriers`
)

select * from source
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`couriers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 15 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`couriers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 15 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`couriers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 15 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0f07e-7d05-1acd-b1d8-037daf1967e7
[0m13:50:50.563910 [debug] [Thread-3 (]: On model.delivery_analytics.stg_couriers: Close
[0m13:50:50.564420 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f07e-7ce9-19d9-b089-293a43d8ffc3) - Closing
[0m13:50:50.772036 [debug] [Thread-3 (]: Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`couriers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 15 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_couriers.sql
[0m13:50:50.775072 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a2c9ee35-1951-4243-9564-1600cfc2d36a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4A903B6D0>]}
[0m13:50:50.776072 [error] [Thread-3 (]: 1 of 3 ERROR creating sql view model staging.stg_couriers ...................... [[31mERROR[0m in 1.95s]
[0m13:50:50.777345 [debug] [Thread-3 (]: Finished running node model.delivery_analytics.stg_couriers
[0m13:50:50.777928 [debug] [Thread-3 (]: Began running node model.delivery_analytics.stg_customers
[0m13:50:50.778654 [debug] [Thread-6 (]: Marking all children of 'model.delivery_analytics.stg_couriers' to be skipped because of status 'error'.  Reason: Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`couriers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 15 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_couriers.sql.
[0m13:50:50.779757 [info ] [Thread-3 (]: 2 of 3 START sql view model staging.stg_customers .............................. [RUN]
[0m13:50:50.781821 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.delivery_analytics.stg_customers) - Creating connection
[0m13:50:50.782314 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.delivery_analytics.stg_customers'
[0m13:50:50.782756 [debug] [Thread-3 (]: Began compiling node model.delivery_analytics.stg_customers
[0m13:50:50.786547 [debug] [Thread-3 (]: Writing injected SQL for node "model.delivery_analytics.stg_customers"
[0m13:50:50.788809 [debug] [Thread-3 (]: Began executing node model.delivery_analytics.stg_customers
[0m13:50:50.793895 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m13:50:50.795827 [debug] [Thread-3 (]: Creating view `delivery_analytics`.`staging`.`stg_customers`
[0m13:50:50.797163 [debug] [Thread-3 (]: Writing runtime sql for node "model.delivery_analytics.stg_customers"
[0m13:50:50.799640 [debug] [Thread-3 (]: Using databricks connection "model.delivery_analytics.stg_customers"
[0m13:50:50.800303 [debug] [Thread-3 (]: On model.delivery_analytics.stg_customers: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "node_id": "model.delivery_analytics.stg_customers"} */

  
  
  create or replace view `delivery_analytics`.`staging`.`stg_customers`
  
  as (
    with source as (
    select
        customer_id,
        name,
        email,
        phone,
        address,
        city
    from `delivery_analytics2`.`raw`.`customers`
)

select * from source
  )

[0m13:50:50.801035 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:50:51.835811 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f07e-7e07-1b97-b73b-8ff7c8c338ec) - Created
[0m13:50:52.416407 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "node_id": "model.delivery_analytics.stg_customers"} */

  
  
  create or replace view `delivery_analytics`.`staging`.`stg_customers`
  
  as (
    with source as (
    select
        customer_id,
        name,
        email,
        phone,
        address,
        city
    from `delivery_analytics2`.`raw`.`customers`
)

select * from source
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0f07e-7e21-1f89-9b34-7c95d9cf2300
[0m13:50:52.417655 [debug] [Thread-3 (]: On model.delivery_analytics.stg_customers: Close
[0m13:50:52.418159 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f07e-7e07-1b97-b73b-8ff7c8c338ec) - Closing
[0m13:50:52.607401 [debug] [Thread-3 (]: Database Error in model stg_customers (models\staging\stg_customers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_customers.sql
[0m13:50:52.608287 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a2c9ee35-1951-4243-9564-1600cfc2d36a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4D0FBFEE0>]}
[0m13:50:52.609246 [error] [Thread-3 (]: 2 of 3 ERROR creating sql view model staging.stg_customers ..................... [[31mERROR[0m in 1.83s]
[0m13:50:52.610567 [debug] [Thread-3 (]: Finished running node model.delivery_analytics.stg_customers
[0m13:50:52.611300 [debug] [Thread-3 (]: Began running node model.delivery_analytics.stg_shipments
[0m13:50:52.612001 [debug] [Thread-6 (]: Marking all children of 'model.delivery_analytics.stg_customers' to be skipped because of status 'error'.  Reason: Database Error in model stg_customers (models\staging\stg_customers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_customers.sql.
[0m13:50:52.612734 [info ] [Thread-3 (]: 3 of 3 START sql view model staging.stg_shipments .............................. [RUN]
[0m13:50:52.614140 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.delivery_analytics.stg_shipments) - Creating connection
[0m13:50:52.615010 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.delivery_analytics.stg_shipments'
[0m13:50:52.615686 [debug] [Thread-3 (]: Began compiling node model.delivery_analytics.stg_shipments
[0m13:50:52.621903 [debug] [Thread-3 (]: Writing injected SQL for node "model.delivery_analytics.stg_shipments"
[0m13:50:52.625326 [debug] [Thread-3 (]: Began executing node model.delivery_analytics.stg_shipments
[0m13:50:52.630974 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m13:50:52.632889 [debug] [Thread-3 (]: Creating view `delivery_analytics`.`staging`.`stg_shipments`
[0m13:50:52.633987 [debug] [Thread-3 (]: Writing runtime sql for node "model.delivery_analytics.stg_shipments"
[0m13:50:52.635961 [debug] [Thread-3 (]: Using databricks connection "model.delivery_analytics.stg_shipments"
[0m13:50:52.636849 [debug] [Thread-3 (]: On model.delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "node_id": "model.delivery_analytics.stg_shipments"} */

  
  
  create or replace view `delivery_analytics`.`staging`.`stg_shipments`
  
  as (
    with source as (
    select
        shipment_id,
        sender_id,
        receiver_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        completed_at,
        price,
        delivery_city,
        delivery_type,
        distance_km
    from `delivery_analytics2`.`raw`.`shipments`
),

status_logic as (
    select
        *,
        case
            when shipment_date is null then 'pending'
            when shipment_date is not null and completed_at is null then 'picked_up'
            when completed_at is not null then 'delivered'
        end as current_status
    from source
)

select * from status_logic
  )

[0m13:50:52.637608 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:50:53.611438 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f07e-7f16-125a-8760-f120af0fb778) - Created
[0m13:50:54.138270 [debug] [Thread-3 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "delivery_analytics", "target_name": "dev", "node_id": "model.delivery_analytics.stg_shipments"} */

  
  
  create or replace view `delivery_analytics`.`staging`.`stg_shipments`
  
  as (
    with source as (
    select
        shipment_id,
        sender_id,
        receiver_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        completed_at,
        price,
        delivery_city,
        delivery_type,
        distance_km
    from `delivery_analytics2`.`raw`.`shipments`
),

status_logic as (
    select
        *,
        case
            when shipment_date is null then 'pending'
            when shipment_date is not null and completed_at is null then 'picked_up'
            when completed_at is not null then 'delivered'
        end as current_status
    from source
)

select * from status_logic
  )

: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`shipments` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 21 pos 9
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`shipments` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 21 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`shipments` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 21 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0f07e-7f31-1bd5-b899-e685d9a7d847
[0m13:50:54.140269 [debug] [Thread-3 (]: On model.delivery_analytics.stg_shipments: Close
[0m13:50:54.141021 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f07e-7f16-125a-8760-f120af0fb778) - Closing
[0m13:50:54.349199 [debug] [Thread-3 (]: Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`shipments` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 21 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_shipments.sql
[0m13:50:54.350190 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a2c9ee35-1951-4243-9564-1600cfc2d36a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4D0FC2210>]}
[0m13:50:54.351119 [error] [Thread-3 (]: 3 of 3 ERROR creating sql view model staging.stg_shipments ..................... [[31mERROR[0m in 1.74s]
[0m13:50:54.352783 [debug] [Thread-3 (]: Finished running node model.delivery_analytics.stg_shipments
[0m13:50:54.353730 [debug] [Thread-6 (]: Marking all children of 'model.delivery_analytics.stg_shipments' to be skipped because of status 'error'.  Reason: Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`shipments` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 21 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_shipments.sql.
[0m13:50:54.356120 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m13:50:54.356611 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:50:54.357267 [info ] [MainThread]: 
[0m13:50:54.357981 [info ] [MainThread]: Finished running 3 view models in 0 hours 0 minutes and 9.18 seconds (9.18s).
[0m13:50:54.359966 [debug] [MainThread]: Command end result
[0m13:50:54.449018 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:50:54.454556 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:50:54.466196 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m13:50:54.467040 [info ] [MainThread]: 
[0m13:50:54.468355 [info ] [MainThread]: [31mCompleted with 3 errors, 0 partial successes, and 0 warnings:[0m
[0m13:50:54.469650 [info ] [MainThread]: 
[0m13:50:54.470762 [error] [MainThread]: [31mFailure in model stg_couriers (models\staging\stg_couriers.sql)[0m
[0m13:50:54.471830 [error] [MainThread]:   Database Error in model stg_couriers (models\staging\stg_couriers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`couriers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 15 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_couriers.sql
[0m13:50:54.472790 [info ] [MainThread]: 
[0m13:50:54.473630 [info ] [MainThread]:   compiled code at target\compiled\delivery_analytics\models\staging\stg_couriers.sql
[0m13:50:54.474377 [info ] [MainThread]: 
[0m13:50:54.475193 [error] [MainThread]: [31mFailure in model stg_customers (models\staging\stg_customers.sql)[0m
[0m13:50:54.475986 [error] [MainThread]:   Database Error in model stg_customers (models\staging\stg_customers.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`customers` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_customers.sql
[0m13:50:54.476815 [info ] [MainThread]: 
[0m13:50:54.477624 [info ] [MainThread]:   compiled code at target\compiled\delivery_analytics\models\staging\stg_customers.sql
[0m13:50:54.478277 [info ] [MainThread]: 
[0m13:50:54.479090 [error] [MainThread]: [31mFailure in model stg_shipments (models\staging\stg_shipments.sql)[0m
[0m13:50:54.480061 [error] [MainThread]:   Database Error in model stg_shipments (models\staging\stg_shipments.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `delivery_analytics2`.`raw`.`shipments` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 21 pos 9
  compiled code at target\run\delivery_analytics\models\staging\stg_shipments.sql
[0m13:50:54.480821 [info ] [MainThread]: 
[0m13:50:54.481717 [info ] [MainThread]:   compiled code at target\compiled\delivery_analytics\models\staging\stg_shipments.sql
[0m13:50:54.482637 [info ] [MainThread]: 
[0m13:50:54.483599 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=0 NO-OP=0 TOTAL=3
[0m13:50:54.484899 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ModelParamUsageDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m13:50:54.486497 [debug] [MainThread]: Command `dbt run` failed at 13:50:54.486320 after 13.11 seconds
[0m13:50:54.487192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4A9C79DF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4A96E6F30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4D0F778F0>]}
[0m13:50:54.487848 [debug] [MainThread]: Flushing usage events
[0m13:50:55.764256 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:52:45.702852 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D4618C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D577BED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D577BB10>]}


============================== 13:52:45.708002 | fd77ca1b-1ffd-42c0-b9b9-b23927a883d0 ==============================
[0m13:52:45.708002 [info ] [MainThread]: Running with dbt=1.11.2
[0m13:52:45.708934 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'no_print': 'None', 'version_check': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\Korisnik\\.dbt', 'use_colors': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_path': 'C:\\Users\\Korisnik\\OneDrive\\Desktop\\dbt_delivery_analytics\\logs'}
[0m13:52:46.026195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D4A85220>]}
[0m13:52:46.121643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D55E2E00>]}
[0m13:52:46.123139 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:52:46.564838 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m13:52:46.699516 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m13:52:46.700714 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m13:52:46.701400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D7191150>]}
[0m13:52:48.023844 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'shipments_by_hour_day' in the 'models' section of file 'models\schema.yml'
[0m13:52:48.026660 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'shipments_by_city_vehicle' in the 'models' section of file 'models\schema.yml'
[0m13:52:48.442036 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_delivery_analytics.not_null_shipments_by_hour_day_day_of_week.e2d126a571' (models\schema.yml) depends on a node named 'shipments_by_hour_day' in package '' which was not found
[0m13:52:48.443102 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_delivery_analytics.not_null_shipments_by_hour_day_shipment_hour.6ee57859e9' (models\schema.yml) depends on a node named 'shipments_by_hour_day' in package '' which was not found
[0m13:52:48.444164 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_delivery_analytics.not_null_shipments_by_city_vehicle_city.1d36860158' (models\schema.yml) depends on a node named 'shipments_by_city_vehicle' in package '' which was not found
[0m13:52:48.445009 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbt_delivery_analytics.not_null_shipments_by_city_vehicle_vehicle_type.51a805e811' (models\schema.yml) depends on a node named 'shipments_by_city_vehicle' in package '' which was not found
[0m13:52:48.568287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D5786300>]}
[0m13:52:48.674677 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:52:48.679025 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:52:48.712521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D7585630>]}
[0m13:52:48.713479 [info ] [MainThread]: Found 20 models, 5 data tests, 3 sources, 459 macros
[0m13:52:48.714371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D72057E0>]}
[0m13:52:48.718794 [info ] [MainThread]: 
[0m13:52:48.720002 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:52:48.720992 [info ] [MainThread]: 
[0m13:52:48.722101 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m13:52:48.729945 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2'
[0m13:52:48.874657 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2"
[0m13:52:48.875220 [debug] [ThreadPool]: On list_delivery_analytics2: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2"} */

    select distinct nspname from pg_namespace
  
[0m13:52:48.875606 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:52:49.029764 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.154 seconds
[0m13:52:49.032791 [debug] [ThreadPool]: On list_delivery_analytics2: Close
[0m13:52:49.039116 [debug] [ThreadPool]: Acquiring new postgres connection 'list_delivery_analytics2_analytics'
[0m13:52:49.046559 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m13:52:49.047012 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: BEGIN
[0m13:52:49.047398 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:52:49.137342 [debug] [ThreadPool]: SQL status: BEGIN in 0.090 seconds
[0m13:52:49.138190 [debug] [ThreadPool]: Using postgres connection "list_delivery_analytics2_analytics"
[0m13:52:49.138938 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "list_delivery_analytics2_analytics"} */
select
      'delivery_analytics2' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'analytics'
    union all
    select
      'delivery_analytics2' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'analytics'
  
[0m13:52:49.193503 [debug] [ThreadPool]: SQL status: SELECT 15 in 0.054 seconds
[0m13:52:49.195874 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: ROLLBACK
[0m13:52:49.197100 [debug] [ThreadPool]: On list_delivery_analytics2_analytics: Close
[0m13:52:49.210816 [debug] [MainThread]: Using postgres connection "master"
[0m13:52:49.211325 [debug] [MainThread]: On master: BEGIN
[0m13:52:49.211675 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:52:49.274243 [debug] [MainThread]: SQL status: BEGIN in 0.062 seconds
[0m13:52:49.274812 [debug] [MainThread]: Using postgres connection "master"
[0m13:52:49.275256 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m13:52:49.308568 [debug] [MainThread]: SQL status: SELECT 10 in 0.033 seconds
[0m13:52:49.312159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D7581E50>]}
[0m13:52:49.312696 [debug] [MainThread]: On master: ROLLBACK
[0m13:52:49.313398 [debug] [MainThread]: Using postgres connection "master"
[0m13:52:49.313796 [debug] [MainThread]: On master: BEGIN
[0m13:52:49.314744 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m13:52:49.315162 [debug] [MainThread]: On master: COMMIT
[0m13:52:49.315530 [debug] [MainThread]: Using postgres connection "master"
[0m13:52:49.315886 [debug] [MainThread]: On master: COMMIT
[0m13:52:49.316629 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:52:49.317047 [debug] [MainThread]: On master: Close
[0m13:52:49.324463 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.stg_shipments
[0m13:52:49.325207 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.stg_couriers
[0m13:52:49.325864 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.stg_customers
[0m13:52:49.326688 [info ] [Thread-3 (]: 3 of 20 START sql view model analytics.stg_shipments ........................... [RUN]
[0m13:52:49.329362 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_shipments'
[0m13:52:49.327536 [info ] [Thread-1 (]: 1 of 20 START sql view model analytics.stg_couriers ............................ [RUN]
[0m13:52:49.330226 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.stg_shipments
[0m13:52:49.330916 [debug] [Thread-1 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_couriers'
[0m13:52:49.328390 [info ] [Thread-2 (]: 2 of 20 START sql view model analytics.stg_customers ........................... [RUN]
[0m13:52:49.339999 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_shipments"
[0m13:52:49.340734 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.stg_couriers
[0m13:52:49.341498 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.stg_customers'
[0m13:52:49.344682 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_couriers"
[0m13:52:49.345379 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.stg_customers
[0m13:52:49.350524 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.stg_customers"
[0m13:52:49.351482 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.stg_shipments
[0m13:52:49.378418 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.stg_couriers
[0m13:52:49.394057 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.stg_customers
[0m13:52:49.402858 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_shipments"
[0m13:52:49.406845 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_couriers"
[0m13:52:49.411885 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.stg_customers"
[0m13:52:49.415002 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m13:52:49.415629 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m13:52:49.416162 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: BEGIN
[0m13:52:49.416724 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: BEGIN
[0m13:52:49.417358 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m13:52:49.417841 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:52:49.418408 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:52:49.418959 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: BEGIN
[0m13:52:49.419971 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:52:49.502986 [debug] [Thread-3 (]: SQL status: BEGIN in 0.085 seconds
[0m13:52:49.503941 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m13:52:49.504702 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipments"} */

  create view "delivery_analytics2"."analytics"."stg_shipments__dbt_tmp"
    
    
  as (
    with source as (
    select
        shipment_id,
        sender_id,
        receiver_id,
        courier_id,
        shipment_date,
        expected_delivery_date,
        completed_at,
        price,
        delivery_city,
        delivery_type,
        distance_km
    from "delivery_analytics2"."raw"."shipments"
),

status_logic as (
    select
        *,
        case
            when shipment_date is null then 'pending'
            when shipment_date is not null and completed_at is null then 'picked_up'
            when completed_at is not null then 'delivered'
        end as current_status
    from source
)

select * from status_logic
  );
[0m13:52:49.506985 [debug] [Thread-1 (]: SQL status: BEGIN in 0.089 seconds
[0m13:52:49.507490 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m13:52:49.507941 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_couriers"} */

  create view "delivery_analytics2"."analytics"."stg_couriers__dbt_tmp"
    
    
  as (
    -- stg_couriers.sql
with source as (
    select
        courier_id,
        name,
        vehicle_type,
        city
    from "delivery_analytics2"."raw"."couriers"
)

select * from source
  );
[0m13:52:49.515067 [debug] [Thread-2 (]: SQL status: BEGIN in 0.095 seconds
[0m13:52:49.515565 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m13:52:49.516040 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_customers"} */

  create view "delivery_analytics2"."analytics"."stg_customers__dbt_tmp"
    
    
  as (
    with source as (
    select
        customer_id,
        name,
        email,
        phone,
        address,
        city
    from "delivery_analytics2"."raw"."customers"
)

select * from source
  );
[0m13:52:49.537854 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.021 seconds
[0m13:52:49.538480 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.030 seconds
[0m13:52:49.546027 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m13:52:49.546558 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.041 seconds
[0m13:52:49.550301 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m13:52:49.550967 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_customers"} */
alter table "delivery_analytics2"."analytics"."stg_customers" rename to "stg_customers__dbt_backup"
[0m13:52:49.556390 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m13:52:49.557096 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_couriers"} */
alter table "delivery_analytics2"."analytics"."stg_couriers" rename to "stg_couriers__dbt_backup"
[0m13:52:49.557786 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipments"} */
alter table "delivery_analytics2"."analytics"."stg_shipments" rename to "stg_shipments__dbt_backup"
[0m13:52:49.559487 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:52:49.560064 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m13:52:49.560609 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m13:52:49.563903 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m13:52:49.567474 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m13:52:49.570906 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m13:52:49.571501 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_couriers"} */
alter table "delivery_analytics2"."analytics"."stg_couriers__dbt_tmp" rename to "stg_couriers"
[0m13:52:49.572135 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_customers"} */
alter table "delivery_analytics2"."analytics"."stg_customers__dbt_tmp" rename to "stg_customers"
[0m13:52:49.572727 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipments"} */
alter table "delivery_analytics2"."analytics"."stg_shipments__dbt_tmp" rename to "stg_shipments"
[0m13:52:49.574146 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:52:49.574826 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:52:49.595149 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: COMMIT
[0m13:52:49.595760 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.022 seconds
[0m13:52:49.597652 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: COMMIT
[0m13:52:49.598305 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m13:52:49.600298 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: COMMIT
[0m13:52:49.600950 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m13:52:49.601740 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: COMMIT
[0m13:52:49.602469 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m13:52:49.603004 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: COMMIT
[0m13:52:49.603710 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: COMMIT
[0m13:52:49.605336 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m13:52:49.613180 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics2"."analytics"."stg_customers__dbt_backup"
[0m13:52:49.619220 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.stg_customers"
[0m13:52:49.619716 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_customers"} */
drop view if exists "delivery_analytics2"."analytics"."stg_customers__dbt_backup" cascade
[0m13:52:49.620280 [debug] [Thread-3 (]: SQL status: COMMIT in 0.016 seconds
[0m13:52:49.620899 [debug] [Thread-1 (]: SQL status: COMMIT in 0.016 seconds
[0m13:52:49.623746 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics2"."analytics"."stg_shipments__dbt_backup"
[0m13:52:49.626360 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."stg_couriers__dbt_backup"
[0m13:52:49.627563 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.stg_shipments"
[0m13:52:49.628488 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.stg_couriers"
[0m13:52:49.629069 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_shipments"} */
drop view if exists "delivery_analytics2"."analytics"."stg_shipments__dbt_backup" cascade
[0m13:52:49.629868 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.stg_couriers"} */
drop view if exists "delivery_analytics2"."analytics"."stg_couriers__dbt_backup" cascade
[0m13:52:49.633787 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.003 seconds
[0m13:52:49.634624 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.014 seconds
[0m13:52:49.635297 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.005 seconds
[0m13:52:49.639218 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.stg_couriers: Close
[0m13:52:49.641769 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.stg_customers: Close
[0m13:52:49.643737 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.stg_shipments: Close
[0m13:52:49.647910 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D75671D0>]}
[0m13:52:49.648532 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D79D1A90>]}
[0m13:52:49.650274 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D782AC30>]}
[0m13:52:49.649722 [info ] [Thread-1 (]: 1 of 20 OK created sql view model analytics.stg_couriers ....................... [[32mCREATE VIEW[0m in 0.31s]
[0m13:52:49.651504 [info ] [Thread-2 (]: 2 of 20 OK created sql view model analytics.stg_customers ...................... [[32mCREATE VIEW[0m in 0.31s]
[0m13:52:49.653865 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.stg_couriers
[0m13:52:49.655227 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.stg_customers
[0m13:52:49.652560 [info ] [Thread-3 (]: 3 of 20 OK created sql view model analytics.stg_shipments ...................... [[32mCREATE VIEW[0m in 0.32s]
[0m13:52:49.657062 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.stg_shipments
[0m13:52:49.657786 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.dim_couriers
[0m13:52:49.658651 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.dim_customers
[0m13:52:49.659299 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.dim_cities
[0m13:52:49.660013 [info ] [Thread-4 (]: 4 of 20 START sql table model analytics.dim_couriers ........................... [RUN]
[0m13:52:49.661390 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.fct_shipments
[0m13:52:49.660884 [info ] [Thread-2 (]: 6 of 20 START sql table model analytics.dim_customers .......................... [RUN]
[0m13:52:49.663118 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.dbt_delivery_analytics.dim_couriers'
[0m13:52:49.662431 [info ] [Thread-1 (]: 5 of 20 START sql table model analytics.dim_cities ............................. [RUN]
[0m13:52:49.664525 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.stg_customers, now model.dbt_delivery_analytics.dim_customers)
[0m13:52:49.663880 [info ] [Thread-3 (]: 7 of 20 START sql table model analytics.fct_shipments .......................... [RUN]
[0m13:52:49.665274 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.dim_couriers
[0m13:52:49.665929 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.stg_couriers, now model.dbt_delivery_analytics.dim_cities)
[0m13:52:49.666698 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.dim_customers
[0m13:52:49.667374 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.stg_shipments, now model.dbt_delivery_analytics.fct_shipments)
[0m13:52:49.671059 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.dim_couriers"
[0m13:52:49.671827 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.dim_cities
[0m13:52:49.676651 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.dim_customers"
[0m13:52:49.677474 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.fct_shipments
[0m13:52:49.680937 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.dim_cities"
[0m13:52:49.688229 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipments"
[0m13:52:49.690943 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.dim_cities
[0m13:52:49.706873 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.dim_couriers
[0m13:52:49.732606 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.dim_couriers"
[0m13:52:49.733399 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.dim_cities"
[0m13:52:49.734078 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.fct_shipments
[0m13:52:49.740321 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipments"
[0m13:52:49.741296 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.dim_customers
[0m13:52:49.745434 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.dim_customers"
[0m13:52:49.746849 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.dim_couriers"
[0m13:52:49.747497 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.dim_cities"
[0m13:52:49.748085 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:52:49.748568 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.dim_couriers: BEGIN
[0m13:52:49.749164 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.dim_cities: BEGIN
[0m13:52:49.749691 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments: BEGIN
[0m13:52:49.750389 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.dim_customers"
[0m13:52:49.750990 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:52:49.751599 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:52:49.752428 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:52:49.753142 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.dim_customers: BEGIN
[0m13:52:49.754303 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:52:49.837597 [debug] [Thread-1 (]: SQL status: BEGIN in 0.086 seconds
[0m13:52:49.838178 [debug] [Thread-4 (]: SQL status: BEGIN in 0.087 seconds
[0m13:52:49.838606 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.dim_cities"
[0m13:52:49.839132 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.dim_couriers"
[0m13:52:49.839674 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.dim_cities: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_cities"} */

  
    

  create  table "delivery_analytics2"."analytics"."dim_cities__dbt_tmp"
  
  
    as
  
  (
    select distinct city
from "delivery_analytics2"."analytics"."stg_customers"
  );
  
[0m13:52:49.840173 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.dim_couriers: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_couriers"} */

  
    

  create  table "delivery_analytics2"."analytics"."dim_couriers__dbt_tmp"
  
  
    as
  
  (
    select *
from "delivery_analytics2"."analytics"."stg_couriers"
  );
  
[0m13:52:49.843533 [debug] [Thread-3 (]: SQL status: BEGIN in 0.091 seconds
[0m13:52:49.844013 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:52:49.844567 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */

  
    

  create  table "delivery_analytics2"."analytics"."fct_shipments__dbt_tmp"
  
  
    as
  
  (
    with shipments as (

    select *
    from "delivery_analytics2"."analytics"."stg_shipments"

),

customers as (

    select *
    from "delivery_analytics2"."analytics"."stg_customers"

),


couriers as (

    select *
    from "delivery_analytics2"."analytics"."stg_couriers"

)

select
    s.shipment_id,

    -- keys
    s.sender_id,
    s.receiver_id,
    s.courier_id,

    -- dates
    s.shipment_date,
    s.expected_delivery_date,
    s.completed_at,

    -- metrics
    s.price,
    s.distance_km,

    -- attributes
    s.delivery_city,
    s.delivery_type,
    s.current_status,

    -- derived metrics
    case
        when s.completed_at is not null
        then (s.completed_at::date - s.shipment_date)
    end as delivery_duration_days,

    case
        when s.completed_at > s.expected_delivery_date then true
        else false
    end as is_late_delivery

from shipments s




/*
    FACT TABLE: fct_shipments

    Svaki red u ovoj tabeli predstavlja jednu pošiljku (shipment) iz naše baze.

    Svrha:
    - Centralizovati sve metrike vezane za pošiljke na jednom mjestu
    - Omogućiti analitiku po vremenu, kuriru, tipu pošiljke i gradu
    - Poslužiti kao ulaz za BI dashboard ili dalju analitiku u marts layer-u

    Šta se ovdje računa:
    1. Ključne kolone:
        - shipment_id
        - sender_id, receiver_id
        - courier_id
    2. Datumi:
        - shipment_date (kada je pošiljka poslana)
        - expected_delivery_date (planirani datum isporuke)
        - completed_at (kada je pošiljka zaista dostavljena)
    3. Metričke kolone:
        - price
        - distance_km
    4. Atributi:
        - delivery_city
        - delivery_type (standard / express)
        - current_status (pending, picked_up, delivered)
    5. Derived metrics (izvedene kolone):
        - delivery_duration_days = koliko je dana trajala pošiljka
        - is_late_delivery = boolean, true ako je pošiljka dostavljena nakon expected_delivery_date

    Napomena:
    - Ova tabela služi kao "fact" jer sadrži sve mjere koje se mogu agregirati
    - Za dimenzije ćemo imati posebne dim_tables: dim_customers, dim_couriers, dim_cities
    - Može se koristiti za: analizu performansi kurira, kašnjenja po gradu, tip pošiljke, revenue po gradu itd.

*/
  );
  
[0m13:52:49.852231 [debug] [Thread-2 (]: SQL status: BEGIN in 0.098 seconds
[0m13:52:49.852704 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.dim_customers"
[0m13:52:49.853100 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.dim_customers: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_customers"} */

  
    

  create  table "delivery_analytics2"."analytics"."dim_customers__dbt_tmp"
  
  
    as
  
  (
    select *
from "delivery_analytics2"."analytics"."stg_customers"
  );
  
[0m13:52:49.856235 [debug] [Thread-4 (]: SQL status: SELECT 10 in 0.015 seconds
[0m13:52:49.856750 [debug] [Thread-1 (]: SQL status: SELECT 5 in 0.016 seconds
[0m13:52:49.864206 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.dim_couriers"
[0m13:52:49.864756 [debug] [Thread-2 (]: SQL status: SELECT 20 in 0.011 seconds
[0m13:52:49.867799 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.dim_cities"
[0m13:52:49.868370 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.dim_couriers: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_couriers"} */
alter table "delivery_analytics2"."analytics"."dim_couriers__dbt_tmp" rename to "dim_couriers"
[0m13:52:49.871508 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.dim_customers"
[0m13:52:49.872072 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.dim_cities: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_cities"} */
alter table "delivery_analytics2"."analytics"."dim_cities__dbt_tmp" rename to "dim_cities"
[0m13:52:49.872790 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.dim_customers: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_customers"} */
alter table "delivery_analytics2"."analytics"."dim_customers__dbt_tmp" rename to "dim_customers"
[0m13:52:49.873336 [debug] [Thread-3 (]: SQL status: SELECT 50 in 0.028 seconds
[0m13:52:49.873877 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:52:49.877391 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:52:49.879158 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.dim_couriers: COMMIT
[0m13:52:49.879677 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m13:52:49.880116 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.006 seconds
[0m13:52:49.880657 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics2"."analytics"."fct_shipments" rename to "fct_shipments__dbt_backup"
[0m13:52:49.881174 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.dim_couriers"
[0m13:52:49.882631 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.dim_customers: COMMIT
[0m13:52:49.884211 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.dim_cities: COMMIT
[0m13:52:49.884878 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.dim_couriers: COMMIT
[0m13:52:49.885452 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.dim_customers"
[0m13:52:49.885930 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:52:49.886369 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.dim_cities"
[0m13:52:49.886994 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.dim_customers: COMMIT
[0m13:52:49.889839 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:52:49.890331 [debug] [Thread-4 (]: SQL status: COMMIT in 0.003 seconds
[0m13:52:49.890762 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.dim_cities: COMMIT
[0m13:52:49.891457 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
alter table "delivery_analytics2"."analytics"."fct_shipments__dbt_tmp" rename to "fct_shipments"
[0m13:52:49.894372 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics2"."analytics"."dim_couriers__dbt_backup"
[0m13:52:49.898058 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.dim_couriers"
[0m13:52:49.898563 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.dim_couriers: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_couriers"} */
drop table if exists "delivery_analytics2"."analytics"."dim_couriers__dbt_backup" cascade
[0m13:52:49.899080 [debug] [Thread-1 (]: SQL status: COMMIT in 0.004 seconds
[0m13:52:49.899641 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m13:52:49.900186 [debug] [Thread-2 (]: SQL status: COMMIT in 0.009 seconds
[0m13:52:49.902550 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."dim_cities__dbt_backup"
[0m13:52:49.903077 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.003 seconds
[0m13:52:49.904972 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m13:52:49.907221 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics2"."analytics"."dim_customers__dbt_backup"
[0m13:52:49.912716 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.dim_cities"
[0m13:52:49.914436 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.dim_couriers: Close
[0m13:52:49.914983 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:52:49.915779 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.dim_customers"
[0m13:52:49.916341 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.dim_cities: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_cities"} */
drop table if exists "delivery_analytics2"."analytics"."dim_cities__dbt_backup" cascade
[0m13:52:49.917109 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments: COMMIT
[0m13:52:49.917803 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D7ACFE00>]}
[0m13:52:49.918346 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.dim_customers: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.dim_customers"} */
drop table if exists "delivery_analytics2"."analytics"."dim_customers__dbt_backup" cascade
[0m13:52:49.920122 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m13:52:49.920829 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m13:52:49.921335 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.001 seconds
[0m13:52:49.923016 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.dim_cities: Close
[0m13:52:49.919556 [info ] [Thread-4 (]: 4 of 20 OK created sql table model analytics.dim_couriers ...................... [[32mSELECT 10[0m in 0.25s]
[0m13:52:49.925962 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics2"."analytics"."fct_shipments__dbt_backup"
[0m13:52:49.927430 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.dim_customers: Close
[0m13:52:49.928259 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.dim_couriers
[0m13:52:49.928881 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D79285F0>]}
[0m13:52:49.929829 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipments"
[0m13:52:49.930677 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D792B290>]}
[0m13:52:49.932189 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipments"} */
drop table if exists "delivery_analytics2"."analytics"."fct_shipments__dbt_backup" cascade
[0m13:52:49.931580 [info ] [Thread-1 (]: 5 of 20 OK created sql table model analytics.dim_cities ........................ [[32mSELECT 5[0m in 0.26s]
[0m13:52:49.933432 [info ] [Thread-2 (]: 6 of 20 OK created sql table model analytics.dim_customers ..................... [[32mSELECT 20[0m in 0.27s]
[0m13:52:49.934623 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.dim_cities
[0m13:52:49.935607 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.dim_customers
[0m13:52:49.954693 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.021 seconds
[0m13:52:49.956527 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_shipments: Close
[0m13:52:49.957268 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D792A810>]}
[0m13:52:49.957956 [info ] [Thread-3 (]: 7 of 20 OK created sql table model analytics.fct_shipments ..................... [[32mSELECT 50[0m in 0.29s]
[0m13:52:49.958988 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.fct_shipments
[0m13:52:49.960173 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.fct_courier_load
[0m13:52:49.960891 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.couriers_performance
[0m13:52:49.961451 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.courier_load
[0m13:52:49.961993 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.active_delays
[0m13:52:49.962635 [info ] [Thread-3 (]: 11 of 20 START sql table model analytics.fct_courier_load ...................... [RUN]
[0m13:52:49.963289 [info ] [Thread-2 (]: 10 of 20 START sql view model analytics.couriers_performance ................... [RUN]
[0m13:52:49.965699 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipments, now model.dbt_delivery_analytics.fct_courier_load)
[0m13:52:49.964177 [info ] [Thread-1 (]: 9 of 20 START sql view model analytics.courier_load ............................ [RUN]
[0m13:52:49.966526 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.dim_customers, now model.dbt_delivery_analytics.couriers_performance)
[0m13:52:49.967071 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.fct_courier_load
[0m13:52:49.964825 [info ] [Thread-4 (]: 8 of 20 START sql view model analytics.active_delays ........................... [RUN]
[0m13:52:49.967710 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.dim_cities, now model.dbt_delivery_analytics.courier_load)
[0m13:52:49.968329 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.couriers_performance
[0m13:52:49.971974 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_courier_load"
[0m13:52:49.972700 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.dim_couriers, now model.dbt_delivery_analytics.active_delays)
[0m13:52:49.973353 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.courier_load
[0m13:52:49.976653 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.couriers_performance"
[0m13:52:49.977354 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.active_delays
[0m13:52:49.980264 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.courier_load"
[0m13:52:49.982933 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.active_delays"
[0m13:52:49.983888 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.fct_courier_load
[0m13:52:49.988936 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_courier_load"
[0m13:52:49.989572 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.couriers_performance
[0m13:52:49.993472 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.couriers_performance"
[0m13:52:49.994478 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.courier_load
[0m13:52:49.994992 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.active_delays
[0m13:52:49.995676 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_load"
[0m13:52:49.999224 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.courier_load"
[0m13:52:50.002835 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.active_delays"
[0m13:52:50.003515 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_courier_load: BEGIN
[0m13:52:50.004067 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m13:52:50.004815 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:52:50.005409 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.couriers_performance: BEGIN
[0m13:52:50.006126 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:52:50.007140 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m13:52:50.007686 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m13:52:50.008108 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_load: BEGIN
[0m13:52:50.008614 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.active_delays: BEGIN
[0m13:52:50.009102 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:52:50.009810 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:52:50.073487 [debug] [Thread-3 (]: SQL status: BEGIN in 0.069 seconds
[0m13:52:50.074156 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_load"
[0m13:52:50.074703 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_courier_load: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_courier_load"} */

  
    

  create  table "delivery_analytics2"."analytics"."fct_courier_load__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
    where shipment_date is not null
)

select
    courier_id,
    shipment_date,
    count(*) as shipments_per_day,
    sum(distance_km) as total_distance_per_day,
    sum(price) as revenue_per_day

from shipments
group by courier_id, shipment_date
order by courier_id, shipment_date



/*

Daily load po kuriru

*/
  );
  
[0m13:52:50.078107 [debug] [Thread-2 (]: SQL status: BEGIN in 0.072 seconds
[0m13:52:50.078572 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m13:52:50.079000 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */

  create view "delivery_analytics2"."analytics"."couriers_performance__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    courier_id,
    count(*) as total_shipments,
    count(case when current_status = 'delivered' then 1 end) as delivered_shipments,
    avg(delivery_duration_days) as avg_delivery_duration_days,
    sum(case when delivery_type = 'express' then 1 else 0 end) as express_shipments_count,
    avg(distance_km) as avg_distance_per_shipment,
    sum(distance_km) as total_distance_per_courier,
    (count(case when is_late_delivery = false then 1 end)::float / nullif(count(*),0))*100 as success_rate_percent,
    count(case when current_status != 'delivered' then 1 end) as pending_shipments
from shipments
group by courier_id
order by courier_id

/*
VIEW: couriers_performance

Svrha:
- Prikazuje performanse kurira
- Metričke kolone: total_shipments, delivered_shipments, avg_delivery_duration_days,
  express_shipments_count, avg_distance_per_shipment, total_distance_per_courier, success_rate_percent, pending_shipments
*/
  );
[0m13:52:50.088116 [debug] [Thread-1 (]: SQL status: BEGIN in 0.079 seconds
[0m13:52:50.088694 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m13:52:50.089209 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */

  create view "delivery_analytics2"."analytics"."courier_load__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    courier_id,
    shipment_date,
    count(*) as total_shipments_per_day,
    count(case when current_status != 'delivered' then 1 end) as active_shipments
from shipments
group by courier_id, shipment_date
order by courier_id, shipment_date

/*
VIEW: courier_load

Svrha:
- Analiza radnog opterećenja kurira po danu
- active_shipments = pošiljke koje nisu još dostavljene
*/
  );
[0m13:52:50.089670 [debug] [Thread-4 (]: SQL status: BEGIN in 0.080 seconds
[0m13:52:50.090126 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m13:52:50.090519 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.active_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.active_delays"} */

  create view "delivery_analytics2"."analytics"."active_delays__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
    where current_status != 'delivered'
)

select
    shipment_id,
    courier_id,
    delivery_city,
    delivery_type,
    shipment_date,
    expected_delivery_date,
    current_status,
    (current_date - expected_delivery_date) as delay_days
from shipments
where current_date > expected_delivery_date
order by delay_days desc

/*
VIEW: active_delays
Svrha:
- Prikazuje sve aktivne pošiljke koje kasne
- Metričke kolone: delay_days
- Koristi se za monitoring kašnjenja i upozorenja
*/
  );
[0m13:52:50.095521 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.016 seconds
[0m13:52:50.099751 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m13:52:50.100217 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.010 seconds
[0m13:52:50.100669 [debug] [Thread-3 (]: SQL status: SELECT 50 in 0.025 seconds
[0m13:52:50.101131 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.010 seconds
[0m13:52:50.101569 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */
alter table "delivery_analytics2"."analytics"."couriers_performance__dbt_tmp" rename to "couriers_performance"
[0m13:52:50.104233 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m13:52:50.107076 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_load"
[0m13:52:50.110100 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m13:52:50.110675 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */
alter table "delivery_analytics2"."analytics"."courier_load__dbt_tmp" rename to "courier_load"
[0m13:52:50.111181 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_courier_load: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_courier_load"} */
alter table "delivery_analytics2"."analytics"."fct_courier_load" rename to "fct_courier_load__dbt_backup"
[0m13:52:50.111594 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:52:50.111990 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.active_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.active_delays"} */
alter table "delivery_analytics2"."analytics"."active_delays__dbt_tmp" rename to "active_delays"
[0m13:52:50.113586 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.couriers_performance: COMMIT
[0m13:52:50.114039 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:52:50.114494 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m13:52:50.115073 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m13:52:50.115544 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:52:50.118093 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_load"
[0m13:52:50.119596 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_load: COMMIT
[0m13:52:50.120060 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.couriers_performance: COMMIT
[0m13:52:50.121565 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.active_delays: COMMIT
[0m13:52:50.122051 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_courier_load: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_courier_load"} */
alter table "delivery_analytics2"."analytics"."fct_courier_load__dbt_tmp" rename to "fct_courier_load"
[0m13:52:50.122584 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m13:52:50.123163 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m13:52:50.123696 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_load: COMMIT
[0m13:52:50.124159 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.active_delays: COMMIT
[0m13:52:50.124690 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m13:52:50.125113 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:52:50.125544 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m13:52:50.128001 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics2"."analytics"."couriers_performance__dbt_backup"
[0m13:52:50.128491 [debug] [Thread-4 (]: SQL status: COMMIT in 0.003 seconds
[0m13:52:50.130112 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_courier_load: COMMIT
[0m13:52:50.132280 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."courier_load__dbt_backup"
[0m13:52:50.133258 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.couriers_performance"
[0m13:52:50.135439 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics2"."analytics"."active_delays__dbt_backup"
[0m13:52:50.136023 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_load"
[0m13:52:50.136824 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.courier_load"
[0m13:52:50.137359 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.couriers_performance: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.couriers_performance"} */
drop view if exists "delivery_analytics2"."analytics"."couriers_performance__dbt_backup" cascade
[0m13:52:50.138282 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.active_delays"
[0m13:52:50.138812 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_courier_load: COMMIT
[0m13:52:50.139354 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_load: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.courier_load"} */
drop view if exists "delivery_analytics2"."analytics"."courier_load__dbt_backup" cascade
[0m13:52:50.139926 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.active_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.active_delays"} */
drop view if exists "delivery_analytics2"."analytics"."active_delays__dbt_backup" cascade
[0m13:52:50.140364 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.001 seconds
[0m13:52:50.141990 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.couriers_performance: Close
[0m13:52:50.142389 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.002 seconds
[0m13:52:50.142805 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.002 seconds
[0m13:52:50.143236 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m13:52:50.145126 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.courier_load: Close
[0m13:52:50.145930 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D79297F0>]}
[0m13:52:50.147535 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.active_delays: Close
[0m13:52:50.150615 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics2"."analytics"."fct_courier_load__dbt_backup"
[0m13:52:50.152000 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D79297F0>]}
[0m13:52:50.151438 [info ] [Thread-2 (]: 10 of 20 OK created sql view model analytics.couriers_performance .............. [[32mCREATE VIEW[0m in 0.18s]
[0m13:52:50.153093 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_load"
[0m13:52:50.153646 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D792B830>]}
[0m13:52:50.155234 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.couriers_performance
[0m13:52:50.154393 [info ] [Thread-1 (]: 9 of 20 OK created sql view model analytics.courier_load ....................... [[32mCREATE VIEW[0m in 0.18s]
[0m13:52:50.155976 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_courier_load: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_courier_load"} */
drop table if exists "delivery_analytics2"."analytics"."fct_courier_load__dbt_backup" cascade
[0m13:52:50.157346 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.fct_courier_performance
[0m13:52:50.156772 [info ] [Thread-4 (]: 8 of 20 OK created sql view model analytics.active_delays ...................... [[32mCREATE VIEW[0m in 0.18s]
[0m13:52:50.158388 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.courier_load
[0m13:52:50.160228 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.active_delays
[0m13:52:50.159354 [info ] [Thread-2 (]: 12 of 20 START sql table model analytics.fct_courier_performance ............... [RUN]
[0m13:52:50.160996 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.fct_shipment_delays
[0m13:52:50.161662 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.int_active_delays
[0m13:52:50.162295 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.couriers_performance, now model.dbt_delivery_analytics.fct_courier_performance)
[0m13:52:50.162962 [info ] [Thread-1 (]: 13 of 20 START sql table model analytics.fct_shipment_delays ................... [RUN]
[0m13:52:50.164283 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.fct_courier_performance
[0m13:52:50.163607 [info ] [Thread-4 (]: 14 of 20 START sql view model analytics.int_active_delays ...................... [RUN]
[0m13:52:50.165022 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.courier_load, now model.dbt_delivery_analytics.fct_shipment_delays)
[0m13:52:50.168624 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_courier_performance"
[0m13:52:50.169386 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.010 seconds
[0m13:52:50.170000 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.active_delays, now model.dbt_delivery_analytics.int_active_delays)
[0m13:52:50.170554 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.fct_shipment_delays
[0m13:52:50.172512 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.fct_courier_load: Close
[0m13:52:50.172994 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.int_active_delays
[0m13:52:50.176267 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.fct_shipment_delays"
[0m13:52:50.179803 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.int_active_delays"
[0m13:52:50.180340 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.fct_courier_performance
[0m13:52:50.181045 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D7801F70>]}
[0m13:52:50.185803 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_courier_performance"
[0m13:52:50.186639 [info ] [Thread-3 (]: 11 of 20 OK created sql table model analytics.fct_courier_load ................. [[32mSELECT 50[0m in 0.22s]
[0m13:52:50.188054 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.fct_courier_load
[0m13:52:50.188535 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.fct_shipment_delays
[0m13:52:50.189060 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.int_active_delays
[0m13:52:50.189643 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.int_shipment_delays
[0m13:52:50.193825 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.fct_shipment_delays"
[0m13:52:50.197027 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.int_active_delays"
[0m13:52:50.197559 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_performance"
[0m13:52:50.198119 [info ] [Thread-3 (]: 15 of 20 START sql view model analytics.int_shipment_delays .................... [RUN]
[0m13:52:50.199103 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_courier_performance: BEGIN
[0m13:52:50.199874 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_courier_load, now model.dbt_delivery_analytics.int_shipment_delays)
[0m13:52:50.200505 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:52:50.201065 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.int_shipment_delays
[0m13:52:50.205502 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.int_shipment_delays"
[0m13:52:50.206403 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipment_delays"
[0m13:52:50.206982 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.int_active_delays"
[0m13:52:50.207522 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipment_delays: BEGIN
[0m13:52:50.208104 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.int_active_delays: BEGIN
[0m13:52:50.208702 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:52:50.209215 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:52:50.210401 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.int_shipment_delays
[0m13:52:50.215620 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.int_shipment_delays"
[0m13:52:50.219659 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_delays"
[0m13:52:50.220458 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.int_shipment_delays: BEGIN
[0m13:52:50.221151 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:52:50.268176 [debug] [Thread-2 (]: SQL status: BEGIN in 0.068 seconds
[0m13:52:50.268692 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_performance"
[0m13:52:50.269073 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_courier_performance: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_courier_performance"} */

  
    

  create  table "delivery_analytics2"."analytics"."fct_courier_performance__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    courier_id,

    count(*) as total_shipments,
    count(case when current_status = 'delivered' then 1 end) as delivered_shipments,
    avg(delivery_duration_days) as avg_delivery_duration_days,
    sum(price) as total_revenue,
    sum(case when is_late_delivery then 1 else 0 end)::float / nullif(count(*),0) as pct_late_deliveries

from shipments
group by courier_id
  );
  
[0m13:52:50.276288 [debug] [Thread-4 (]: SQL status: BEGIN in 0.067 seconds
[0m13:52:50.276797 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.int_active_delays"
[0m13:52:50.277186 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.int_active_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_active_delays"} */

  create view "delivery_analytics2"."analytics"."int_active_delays__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select *
from shipments
where current_status in ('pending','picked_up')
   or (current_status='delivered' and is_late_delivery)



/*

Koje posiljke trenutno kasne

*/
  );
[0m13:52:50.282513 [debug] [Thread-1 (]: SQL status: BEGIN in 0.074 seconds
[0m13:52:50.283223 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipment_delays"
[0m13:52:50.283843 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipment_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipment_delays"} */

  
    

  create  table "delivery_analytics2"."analytics"."fct_shipment_delays__dbt_tmp"
  
  
    as
  
  (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    shipment_id,
    shipment_date,
    expected_delivery_date,
    completed_at,
    delivery_city,
    delivery_type,
    courier_id,

    -- kašnjenje u danima
    case
        when completed_at is not null then (completed_at::date - expected_delivery_date::date)
        else (current_date - expected_delivery_date::date)
    end as delay_days,

    -- da li je pošiljka kasnila
    case
        when completed_at > expected_delivery_date then true
        when completed_at is null and current_date > expected_delivery_date then true
        else false
    end as is_late_delivery

from shipments

/*
FACT TABLE: fct_shipment_delays

Svrha:
- Prati kašnjenja po pošiljci
- Može se koristiti za analizu kašnjenja po gradu, tipu pošiljke i kuriru
- Ulaz za view-eve: delays_by_city_type, courier_load, couriers_performance itd.

Kolone:
- shipment_id: jedinstveni identifikator pošiljke
- shipment_date, expected_delivery_date, completed_at: datumi
- courier_id, delivery_city, delivery_type: atributi pošiljke
- delay_days: broj dana kašnjenja
- is_late_delivery: boolean, true ako je pošiljka kasnila
*/
  );
  
[0m13:52:50.285363 [debug] [Thread-2 (]: SQL status: SELECT 10 in 0.016 seconds
[0m13:52:50.288416 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_performance"
[0m13:52:50.288822 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_courier_performance: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_courier_performance"} */
alter table "delivery_analytics2"."analytics"."fct_courier_performance" rename to "fct_courier_performance__dbt_backup"
[0m13:52:50.289264 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.012 seconds
[0m13:52:50.292368 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.int_active_delays"
[0m13:52:50.292800 [debug] [Thread-3 (]: SQL status: BEGIN in 0.072 seconds
[0m13:52:50.293248 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m13:52:50.293699 [debug] [Thread-1 (]: SQL status: SELECT 50 in 0.009 seconds
[0m13:52:50.294073 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.int_active_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_active_delays"} */
alter table "delivery_analytics2"."analytics"."int_active_delays__dbt_tmp" rename to "int_active_delays"
[0m13:52:50.294459 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_delays"
[0m13:52:50.296848 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_performance"
[0m13:52:50.303606 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipment_delays"
[0m13:52:50.304181 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.int_shipment_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_shipment_delays"} */

  create view "delivery_analytics2"."analytics"."int_shipment_delays__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    shipment_id,
    shipment_date,
    expected_delivery_date,
    completed_at,
    case 
        when completed_at is not null then (completed_at::date - expected_delivery_date)
        else (current_date - expected_delivery_date)
    end as delay_days
from shipments



/*

Koliko je kasnjenje po posiljci

*/
  );
[0m13:52:50.304689 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_courier_performance: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_courier_performance"} */
alter table "delivery_analytics2"."analytics"."fct_courier_performance__dbt_tmp" rename to "fct_courier_performance"
[0m13:52:50.305190 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:52:50.305654 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipment_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipment_delays"} */
alter table "delivery_analytics2"."analytics"."fct_shipment_delays" rename to "fct_shipment_delays__dbt_backup"
[0m13:52:50.307367 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.int_active_delays: COMMIT
[0m13:52:50.307776 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m13:52:50.308304 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.int_active_delays"
[0m13:52:50.308757 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:52:50.310027 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_courier_performance: COMMIT
[0m13:52:50.310462 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.int_active_delays: COMMIT
[0m13:52:50.313177 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipment_delays"
[0m13:52:50.313697 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_performance"
[0m13:52:50.314158 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.008 seconds
[0m13:52:50.314654 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipment_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipment_delays"} */
alter table "delivery_analytics2"."analytics"."fct_shipment_delays__dbt_tmp" rename to "fct_shipment_delays"
[0m13:52:50.315118 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_courier_performance: COMMIT
[0m13:52:50.317808 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_delays"
[0m13:52:50.318276 [debug] [Thread-4 (]: SQL status: COMMIT in 0.004 seconds
[0m13:52:50.318910 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.int_shipment_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_shipment_delays"} */
alter table "delivery_analytics2"."analytics"."int_shipment_delays__dbt_tmp" rename to "int_shipment_delays"
[0m13:52:50.321467 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics2"."analytics"."int_active_delays__dbt_backup"
[0m13:52:50.322370 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m13:52:50.322861 [debug] [Thread-2 (]: SQL status: COMMIT in 0.004 seconds
[0m13:52:50.323611 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.int_active_delays"
[0m13:52:50.324074 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:52:50.325705 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipment_delays: COMMIT
[0m13:52:50.327829 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics2"."analytics"."fct_courier_performance__dbt_backup"
[0m13:52:50.328303 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.int_active_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_active_delays"} */
drop view if exists "delivery_analytics2"."analytics"."int_active_delays__dbt_backup" cascade
[0m13:52:50.329854 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.int_shipment_delays: COMMIT
[0m13:52:50.330321 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipment_delays"
[0m13:52:50.331026 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.fct_courier_performance"
[0m13:52:50.331565 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_delays"
[0m13:52:50.332049 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.001 seconds
[0m13:52:50.332466 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipment_delays: COMMIT
[0m13:52:50.332937 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_courier_performance: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_courier_performance"} */
drop table if exists "delivery_analytics2"."analytics"."fct_courier_performance__dbt_backup" cascade
[0m13:52:50.333532 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.int_shipment_delays: COMMIT
[0m13:52:50.334904 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.int_active_delays: Close
[0m13:52:50.335949 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D7A458B0>]}
[0m13:52:50.337135 [debug] [Thread-3 (]: SQL status: COMMIT in 0.002 seconds
[0m13:52:50.336683 [info ] [Thread-4 (]: 14 of 20 OK created sql view model analytics.int_active_delays ................. [[32mCREATE VIEW[0m in 0.17s]
[0m13:52:50.337772 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m13:52:50.341244 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics2"."analytics"."int_shipment_delays__dbt_backup"
[0m13:52:50.341993 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.int_active_delays
[0m13:52:50.344199 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."fct_shipment_delays__dbt_backup"
[0m13:52:50.344753 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.009 seconds
[0m13:52:50.345600 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_delays"
[0m13:52:50.346167 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.int_shipment_status
[0m13:52:50.347047 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.fct_shipment_delays"
[0m13:52:50.348469 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.fct_courier_performance: Close
[0m13:52:50.348957 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.int_shipment_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_shipment_delays"} */
drop view if exists "delivery_analytics2"."analytics"."int_shipment_delays__dbt_backup" cascade
[0m13:52:50.350012 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipment_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.fct_shipment_delays"} */
drop table if exists "delivery_analytics2"."analytics"."fct_shipment_delays__dbt_backup" cascade
[0m13:52:50.349565 [info ] [Thread-4 (]: 16 of 20 START sql view model analytics.int_shipment_status .................... [RUN]
[0m13:52:50.351146 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.int_active_delays, now model.dbt_delivery_analytics.int_shipment_status)
[0m13:52:50.351662 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.int_shipment_status
[0m13:52:50.352102 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.001 seconds
[0m13:52:50.352647 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D725A6F0>]}
[0m13:52:50.355975 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.int_shipment_status"
[0m13:52:50.357674 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.int_shipment_delays: Close
[0m13:52:50.358101 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.007 seconds
[0m13:52:50.358816 [info ] [Thread-2 (]: 12 of 20 OK created sql table model analytics.fct_courier_performance .......... [[32mSELECT 10[0m in 0.19s]
[0m13:52:50.360626 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.fct_shipment_delays: Close
[0m13:52:50.361475 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D7A45130>]}
[0m13:52:50.362178 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.fct_courier_performance
[0m13:52:50.363721 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D7A45130>]}
[0m13:52:50.364264 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.int_shipment_status
[0m13:52:50.363111 [info ] [Thread-3 (]: 15 of 20 OK created sql view model analytics.int_shipment_delays ............... [[32mCREATE VIEW[0m in 0.16s]
[0m13:52:50.364962 [debug] [Thread-2 (]: Began running node model.dbt_delivery_analytics.shipment_delays
[0m13:52:50.370980 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.int_shipment_status"
[0m13:52:50.365871 [info ] [Thread-1 (]: 13 of 20 OK created sql table model analytics.fct_shipment_delays .............. [[32mSELECT 50[0m in 0.20s]
[0m13:52:50.371913 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.int_shipment_delays
[0m13:52:50.372512 [info ] [Thread-2 (]: 17 of 20 START sql view model analytics.shipment_delays ........................ [RUN]
[0m13:52:50.373691 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.fct_shipment_delays
[0m13:52:50.374284 [debug] [Thread-3 (]: Began running node model.dbt_delivery_analytics.shipment_status
[0m13:52:50.375091 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_courier_performance, now model.dbt_delivery_analytics.shipment_delays)
[0m13:52:50.375750 [debug] [Thread-1 (]: Began running node model.dbt_delivery_analytics.shipments_by_time
[0m13:52:50.376259 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_status"
[0m13:52:50.376962 [info ] [Thread-3 (]: 18 of 20 START sql view model analytics.shipment_status ........................ [RUN]
[0m13:52:50.377693 [debug] [Thread-2 (]: Began compiling node model.dbt_delivery_analytics.shipment_delays
[0m13:52:50.378658 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.int_shipment_status: BEGIN
[0m13:52:50.378200 [info ] [Thread-1 (]: 19 of 20 START sql view model analytics.shipments_by_time ...................... [RUN]
[0m13:52:50.379252 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.int_shipment_delays, now model.dbt_delivery_analytics.shipment_status)
[0m13:52:50.381796 [debug] [Thread-2 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipment_delays"
[0m13:52:50.382323 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:52:50.382843 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.fct_shipment_delays, now model.dbt_delivery_analytics.shipments_by_time)
[0m13:52:50.383455 [debug] [Thread-3 (]: Began compiling node model.dbt_delivery_analytics.shipment_status
[0m13:52:50.384264 [debug] [Thread-1 (]: Began compiling node model.dbt_delivery_analytics.shipments_by_time
[0m13:52:50.387703 [debug] [Thread-3 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipment_status"
[0m13:52:50.391845 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_delivery_analytics.shipments_by_time"
[0m13:52:50.392882 [debug] [Thread-2 (]: Began executing node model.dbt_delivery_analytics.shipment_delays
[0m13:52:50.396834 [debug] [Thread-2 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipment_delays"
[0m13:52:50.397886 [debug] [Thread-3 (]: Began executing node model.dbt_delivery_analytics.shipment_status
[0m13:52:50.398416 [debug] [Thread-1 (]: Began executing node model.dbt_delivery_analytics.shipments_by_time
[0m13:52:50.404114 [debug] [Thread-3 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipment_status"
[0m13:52:50.408133 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_delivery_analytics.shipments_by_time"
[0m13:52:50.409147 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_delays"
[0m13:52:50.409747 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: BEGIN
[0m13:52:50.410293 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m13:52:50.411641 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m13:52:50.412228 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m13:52:50.412804 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: BEGIN
[0m13:52:50.413314 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipment_status: BEGIN
[0m13:52:50.413917 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:52:50.414495 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m13:52:50.450728 [debug] [Thread-4 (]: SQL status: BEGIN in 0.068 seconds
[0m13:52:50.451320 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_status"
[0m13:52:50.451757 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.int_shipment_status: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_shipment_status"} */

  create view "delivery_analytics2"."analytics"."int_shipment_status__dbt_tmp"
    
    
  as (
    select
    shipment_id,
    current_status,
    delivery_type,
    delivery_city,
    courier_id
from "delivery_analytics2"."analytics"."fct_shipments"




/*

Koji je status po posiljci je li 
pending, picked up, delivered


*/
  );
[0m13:52:50.458277 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.006 seconds
[0m13:52:50.462874 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_status"
[0m13:52:50.463556 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.int_shipment_status: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_shipment_status"} */
alter table "delivery_analytics2"."analytics"."int_shipment_status__dbt_tmp" rename to "int_shipment_status"
[0m13:52:50.464726 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:52:50.466821 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.int_shipment_status: COMMIT
[0m13:52:50.467276 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_status"
[0m13:52:50.467746 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.int_shipment_status: COMMIT
[0m13:52:50.469422 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m13:52:50.471716 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics2"."analytics"."int_shipment_status__dbt_backup"
[0m13:52:50.472613 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.int_shipment_status"
[0m13:52:50.473037 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.int_shipment_status: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.int_shipment_status"} */
drop view if exists "delivery_analytics2"."analytics"."int_shipment_status__dbt_backup" cascade
[0m13:52:50.473921 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.000 seconds
[0m13:52:50.474370 [debug] [Thread-2 (]: SQL status: BEGIN in 0.064 seconds
[0m13:52:50.476667 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.int_shipment_status: Close
[0m13:52:50.477317 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_delays"
[0m13:52:50.477971 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_delays"} */

  create view "delivery_analytics2"."analytics"."shipment_delays__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    shipment_id,
    shipment_date,
    expected_delivery_date,
    completed_at,

    -- delay_days = difference između expected i actual delivery
    case
        when completed_at is not null then 
            extract(day from (completed_at::timestamp - expected_delivery_date::timestamp))
        else
            extract(day from (current_date - expected_delivery_date::timestamp))
    end as delay_days,

    -- is_late_delivery
    case
        when completed_at is not null and completed_at > expected_delivery_date then true
        when completed_at is null and current_date > expected_delivery_date then true
        else false
    end as is_late_delivery

from shipments

/*
VIEW: fct_shipment_delays
Svrha:
- Kašnjenje po pošiljci
- Metričke kolone: delay_days, is_late_delivery
- Može se agregirati po gradu, tipu pošiljke, kuriru
*/
  );
[0m13:52:50.478932 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D7009DF0>]}
[0m13:52:50.479983 [info ] [Thread-4 (]: 16 of 20 OK created sql view model analytics.int_shipment_status ............... [[32mCREATE VIEW[0m in 0.13s]
[0m13:52:50.481164 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.int_shipment_status
[0m13:52:50.481996 [debug] [Thread-4 (]: Began running node model.dbt_delivery_analytics.delays_by_city_type
[0m13:52:50.483039 [debug] [Thread-3 (]: SQL status: BEGIN in 0.068 seconds
[0m13:52:50.482615 [info ] [Thread-4 (]: 20 of 20 START sql view model analytics.delays_by_city_type .................... [RUN]
[0m13:52:50.483699 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m13:52:50.484343 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.dbt_delivery_analytics.int_shipment_status, now model.dbt_delivery_analytics.delays_by_city_type)
[0m13:52:50.484943 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipment_status: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_status"} */

  create view "delivery_analytics2"."analytics"."shipment_status__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
)

select
    current_status,
    count(*) as total_shipments,
    sum(case when current_status != 'delivered' then 1 else 0 end) as active_shipments
from shipments
group by current_status

/*
VIEW: shipment_status
Svrha:
- Prikazuje broj pošiljki po statusu: pending, picked_up, delivered
- Koristi se za monitoring opterećenja i trenutnog statusa pošiljki
*/
  );
[0m13:52:50.485485 [debug] [Thread-4 (]: Began compiling node model.dbt_delivery_analytics.delays_by_city_type
[0m13:52:50.489656 [debug] [Thread-4 (]: Writing injected SQL for node "model.dbt_delivery_analytics.delays_by_city_type"
[0m13:52:50.490587 [debug] [Thread-2 (]: SQL status: CREATE VIEW in 0.011 seconds
[0m13:52:50.495813 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_delays"
[0m13:52:50.496389 [debug] [Thread-1 (]: SQL status: BEGIN in 0.083 seconds
[0m13:52:50.496750 [debug] [Thread-3 (]: SQL status: CREATE VIEW in 0.011 seconds
[0m13:52:50.497298 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_delays"} */
alter table "delivery_analytics2"."analytics"."shipment_delays__dbt_tmp" rename to "shipment_delays"
[0m13:52:50.497869 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m13:52:50.498401 [debug] [Thread-4 (]: Began executing node model.dbt_delivery_analytics.delays_by_city_type
[0m13:52:50.502008 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m13:52:50.502698 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */

  create view "delivery_analytics2"."analytics"."shipments_by_time__dbt_tmp"
    
    
  as (
    with shipments as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipments"
    where shipment_date is not null
)

select
    date_trunc('day', shipment_date) as day,
    date_trunc('week', shipment_date) as week,
    date_trunc('month', shipment_date) as month,
    delivery_city,
    courier_id,
    delivery_type,
    count(*) as total_shipments,
    count(case when is_late_delivery = false then 1 end) as on_time_shipments,
    avg(delivery_duration_days) as avg_delivery_days,
    sum(price) as total_revenue,
    sum(distance_km) as total_distance

from shipments
group by 1,2,3,4,5,6
order by day, delivery_city, courier_id, delivery_type

/*
VIEW: shipments_by_time

Svrha:
- Analiza pošiljki po danu, sedmici i mjesecu
- Segmentacija po delivery_city, courier_id, delivery_type (standard/express)
- Metričke kolone: total_shipments, on_time_shipments, avg_delivery_days, total_revenue, total_distance
- Može se koristiti za trend analizu i dashboard vizualizacije
*/
  );
[0m13:52:50.506552 [debug] [Thread-4 (]: Writing runtime sql for node "model.dbt_delivery_analytics.delays_by_city_type"
[0m13:52:50.507042 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m13:52:50.507470 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipment_status: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_status"} */
alter table "delivery_analytics2"."analytics"."shipment_status__dbt_tmp" rename to "shipment_status"
[0m13:52:50.509449 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: COMMIT
[0m13:52:50.510168 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_delays"
[0m13:52:50.510531 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: COMMIT
[0m13:52:50.510935 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m13:52:50.511353 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:52:50.511922 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: BEGIN
[0m13:52:50.513300 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipment_status: COMMIT
[0m13:52:50.513720 [debug] [Thread-2 (]: SQL status: COMMIT in 0.002 seconds
[0m13:52:50.514131 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:52:50.514614 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m13:52:50.516890 [debug] [Thread-2 (]: Applying DROP to: "delivery_analytics2"."analytics"."shipment_delays__dbt_backup"
[0m13:52:50.517531 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipment_status: COMMIT
[0m13:52:50.518053 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.010 seconds
[0m13:52:50.518845 [debug] [Thread-2 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_delays"
[0m13:52:50.521993 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m13:52:50.522492 [debug] [Thread-3 (]: SQL status: COMMIT in 0.003 seconds
[0m13:52:50.522965 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_delays"} */
drop view if exists "delivery_analytics2"."analytics"."shipment_delays__dbt_backup" cascade
[0m13:52:50.523478 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */
alter table "delivery_analytics2"."analytics"."shipments_by_time__dbt_tmp" rename to "shipments_by_time"
[0m13:52:50.526063 [debug] [Thread-3 (]: Applying DROP to: "delivery_analytics2"."analytics"."shipment_status__dbt_backup"
[0m13:52:50.527028 [debug] [Thread-3 (]: Using postgres connection "model.dbt_delivery_analytics.shipment_status"
[0m13:52:50.527468 [debug] [Thread-2 (]: SQL status: DROP VIEW in 0.001 seconds
[0m13:52:50.527872 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipment_status: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipment_status"} */
drop view if exists "delivery_analytics2"."analytics"."shipment_status__dbt_backup" cascade
[0m13:52:50.528252 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m13:52:50.529837 [debug] [Thread-2 (]: On model.dbt_delivery_analytics.shipment_delays: Close
[0m13:52:50.531343 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: COMMIT
[0m13:52:50.531749 [debug] [Thread-3 (]: SQL status: DROP VIEW in 0.001 seconds
[0m13:52:50.532151 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m13:52:50.532687 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D7B59010>]}
[0m13:52:50.534061 [debug] [Thread-3 (]: On model.dbt_delivery_analytics.shipment_status: Close
[0m13:52:50.534575 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: COMMIT
[0m13:52:50.535492 [info ] [Thread-2 (]: 17 of 20 OK created sql view model analytics.shipment_delays ................... [[32mCREATE VIEW[0m in 0.16s]
[0m13:52:50.536629 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D725B2F0>]}
[0m13:52:50.537385 [debug] [Thread-2 (]: Finished running node model.dbt_delivery_analytics.shipment_delays
[0m13:52:50.537936 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m13:52:50.538606 [info ] [Thread-3 (]: 18 of 20 OK created sql view model analytics.shipment_status ................... [[32mCREATE VIEW[0m in 0.16s]
[0m13:52:50.541767 [debug] [Thread-1 (]: Applying DROP to: "delivery_analytics2"."analytics"."shipments_by_time__dbt_backup"
[0m13:52:50.542488 [debug] [Thread-3 (]: Finished running node model.dbt_delivery_analytics.shipment_status
[0m13:52:50.543242 [debug] [Thread-1 (]: Using postgres connection "model.dbt_delivery_analytics.shipments_by_time"
[0m13:52:50.543866 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.shipments_by_time"} */
drop view if exists "delivery_analytics2"."analytics"."shipments_by_time__dbt_backup" cascade
[0m13:52:50.544802 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.000 seconds
[0m13:52:50.546204 [debug] [Thread-1 (]: On model.dbt_delivery_analytics.shipments_by_time: Close
[0m13:52:50.546832 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D792B5F0>]}
[0m13:52:50.547428 [info ] [Thread-1 (]: 19 of 20 OK created sql view model analytics.shipments_by_time ................. [[32mCREATE VIEW[0m in 0.16s]
[0m13:52:50.548311 [debug] [Thread-1 (]: Finished running node model.dbt_delivery_analytics.shipments_by_time
[0m13:52:50.570763 [debug] [Thread-4 (]: SQL status: BEGIN in 0.057 seconds
[0m13:52:50.571363 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m13:52:50.571740 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */

  create view "delivery_analytics2"."analytics"."delays_by_city_type__dbt_tmp"
    
    
  as (
    with delays as (
    select *
    from "delivery_analytics2"."analytics"."fct_shipment_delays"
)

select
    delivery_city,
    delivery_type,
    count(*) as total_shipments,
    count(case when is_late_delivery = false then 1 end) as on_time_shipments,
    avg(delay_days) as avg_delay_days
from delays
group by delivery_city, delivery_type
order by delivery_city, delivery_type

/*
VIEW: delays_by_city_type

Svrha:
- Agregacija kašnjenja po delivery_city i delivery_type
- Metričke kolone: total_shipments, on_time_shipments, avg_delay_days
*/
  );
[0m13:52:50.580085 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.008 seconds
[0m13:52:50.583037 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m13:52:50.583449 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */
alter table "delivery_analytics2"."analytics"."delays_by_city_type__dbt_tmp" rename to "delays_by_city_type"
[0m13:52:50.584297 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m13:52:50.586029 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: COMMIT
[0m13:52:50.586430 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m13:52:50.586756 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: COMMIT
[0m13:52:50.588959 [debug] [Thread-4 (]: SQL status: COMMIT in 0.001 seconds
[0m13:52:50.590934 [debug] [Thread-4 (]: Applying DROP to: "delivery_analytics2"."analytics"."delays_by_city_type__dbt_backup"
[0m13:52:50.591546 [debug] [Thread-4 (]: Using postgres connection "model.dbt_delivery_analytics.delays_by_city_type"
[0m13:52:50.591878 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: /* {"app": "dbt", "dbt_version": "1.11.2", "profile_name": "dbt_delivery_analytics", "target_name": "dev", "node_id": "model.dbt_delivery_analytics.delays_by_city_type"} */
drop view if exists "delivery_analytics2"."analytics"."delays_by_city_type__dbt_backup" cascade
[0m13:52:50.592563 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.000 seconds
[0m13:52:50.594004 [debug] [Thread-4 (]: On model.dbt_delivery_analytics.delays_by_city_type: Close
[0m13:52:50.594675 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd77ca1b-1ffd-42c0-b9b9-b23927a883d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D7929F70>]}
[0m13:52:50.595254 [info ] [Thread-4 (]: 20 of 20 OK created sql view model analytics.delays_by_city_type ............... [[32mCREATE VIEW[0m in 0.11s]
[0m13:52:50.596284 [debug] [Thread-4 (]: Finished running node model.dbt_delivery_analytics.delays_by_city_type
[0m13:52:50.598266 [debug] [MainThread]: Using postgres connection "master"
[0m13:52:50.598674 [debug] [MainThread]: On master: BEGIN
[0m13:52:50.599030 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:52:50.661626 [debug] [MainThread]: SQL status: BEGIN in 0.062 seconds
[0m13:52:50.662149 [debug] [MainThread]: On master: COMMIT
[0m13:52:50.662509 [debug] [MainThread]: Using postgres connection "master"
[0m13:52:50.662847 [debug] [MainThread]: On master: COMMIT
[0m13:52:50.663381 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:52:50.663735 [debug] [MainThread]: On master: Close
[0m13:52:50.664247 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:52:50.664518 [debug] [MainThread]: Connection 'list_delivery_analytics2' was properly closed.
[0m13:52:50.664759 [debug] [MainThread]: Connection 'list_delivery_analytics2_analytics' was properly closed.
[0m13:52:50.665010 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipment_status' was properly closed.
[0m13:52:50.665240 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipments_by_time' was properly closed.
[0m13:52:50.665541 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.shipment_delays' was properly closed.
[0m13:52:50.665773 [debug] [MainThread]: Connection 'model.dbt_delivery_analytics.delays_by_city_type' was properly closed.
[0m13:52:50.666160 [info ] [MainThread]: 
[0m13:52:50.666860 [info ] [MainThread]: Finished running 7 table models, 13 view models in 0 hours 0 minutes and 1.94 seconds (1.94s).
[0m13:52:50.670809 [debug] [MainThread]: Command end result
[0m13:52:50.699841 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\manifest.json
[0m13:52:50.703386 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\semantic_manifest.json
[0m13:52:50.711490 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\Korisnik\OneDrive\Desktop\dbt_delivery_analytics\target\run_results.json
[0m13:52:50.711947 [info ] [MainThread]: 
[0m13:52:50.712587 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:52:50.713238 [info ] [MainThread]: 
[0m13:52:50.713767 [info ] [MainThread]: Done. PASS=20 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=20
[0m13:52:50.714947 [debug] [MainThread]: Command `dbt run` succeeded at 13:52:50.714825 after 5.34 seconds
[0m13:52:50.715304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D48DD9D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D725A6F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184D7B5BFB0>]}
[0m13:52:50.715656 [debug] [MainThread]: Flushing usage events
[0m13:52:51.934099 [debug] [MainThread]: An error was encountered while trying to flush usage events
